"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
"""

import pandas as pd
import numpy as np
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Tuple, Optional
import yaml

logger = logging.getLogger(__name__)


class DataPreprocessor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏, –æ—á–∏—Å—Ç–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            config_path: –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.events_df: Optional[pd.DataFrame] = None
        self.users_df: Optional[pd.DataFrame] = None
        self.merged_df: Optional[pd.DataFrame] = None
        self.stats = {}
        
        logger.info("DataPreprocessor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏

        Returns:
            Tuple –∏–∑ –¥–≤—É—Ö DataFrame: events –∏ users
        """
        logger.info("–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö...")

        # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
        raw_path = self.config['data']['raw_path']
        events_file = Path(raw_path) / self.config['data']['events_file']
        users_file = Path(raw_path) / self.config['data']['users_file']

        # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ (–ë–ï–ó —Ç–∏–ø–æ–≤ –¥–ª—è –¥–∞—Ç—ã!)
        dtype_events = {
            '–≠–∫—Ä–∞–Ω': 'category',
            '–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª': 'category',
            '–î–µ–π—Å—Ç–≤–∏–µ': 'category',
            '–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞': 'int32',
            '–ù–æ–º–µ—Ä —Å–µ—Å—Å–∏–∏ –≤ —Ä–∞–º–∫–∞—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞': 'int64',
            '–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞': 'category',
            '–ú–æ–¥–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞': 'category',
            '–¢–∏–ø —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞': 'category',
            '–û–°': 'category'
        }

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–±—ã—Ç–∏–π - —Å–Ω–∞—á–∞–ª–∞ –ë–ï–ó –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã
        try:
            logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–±—ã—Ç–∏–π...")
            self.events_df = pd.read_csv(
                events_file,
                dtype=dtype_events
            )

            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –≤—Ä—É—á–Ω—É—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º
            logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã...")
            # –§–æ—Ä–º–∞—Ç: 2025-09-29T10:20:27+03:00[Europe/Moscow]
            # –£–±–∏—Ä–∞–µ–º —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å –≤ —Å–∫–æ–±–∫–∞—Ö, –µ—Å–ª–∏ –µ—Å—Ç—å
            date_col = self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].astype(str)
            # –£–±–∏—Ä–∞–µ–º [Europe/Moscow] –µ—Å–ª–∏ –µ—Å—Ç—å
            date_col = date_col.str.replace(r'\[.*?\]', '', regex=True)

            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
            self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'] = pd.to_datetime(
                date_col,
                format='ISO8601',
                utc=True,
                errors='coerce'
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ–ª—å–∫–æ –¥–∞—Ç –Ω–µ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–æ—Å—å
            null_dates = self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].isna().sum()
            if null_dates > 0:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å {null_dates} –¥–∞—Ç")

            logger.info(f"–°–æ–±—ã—Ç–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(self.events_df)} —Å—Ç—Ä–æ–∫")

        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {events_file}")
            raise
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ–±—ã—Ç–∏–π: {e}")
            raise

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (–ë–ï–ó —É–∫–∞–∑–∞–Ω–∏—è —Ç–∏–ø–∞ –¥–ª—è age_back –∏–∑-–∑–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö NA)
        try:
            self.users_df = pd.read_csv(
                users_file,
                dtype={
                    'number': 'int32',
                    'gender': 'category'
                }
            )

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º age_back –≤ int16 —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º NA
            if 'age_back' in self.users_df.columns:
                self.users_df['age_back'] = self.users_df['age_back'].astype('Int16')

            logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(self.users_df)} —Å—Ç—Ä–æ–∫")
        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {users_file}")
            raise

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats['events_loaded'] = len(self.events_df)
        self.stats['users_loaded'] = len(self.users_df)

        return self.events_df, self.users_df

    def clean_data(self) -> None:
        """
        –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        """
        logger.info("–ù–∞—á–∞–ª–æ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö...")

        if self.events_df is None or self.users_df is None:
            raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –í—ã–∑–æ–≤–∏—Ç–µ load_data() —Å–Ω–∞—á–∞–ª–∞.")

        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π –∏–∑ —Å–æ–±—ã—Ç–∏–π
        events_before = len(self.events_df)
        self.events_df = self.events_df.drop_duplicates().copy()
        events_removed = events_before - len(self.events_df)
        logger.info(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–µ–π –≤ events: {events_removed}")
        self.stats['events_duplicates_removed'] = events_removed

        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        users_before = len(self.users_df)
        self.users_df = self.users_df.drop_duplicates().copy()
        users_removed = users_before - len(self.users_df)
        logger.info(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–µ–π –≤ users: {users_removed}")
        self.stats['users_duplicates_removed'] = users_removed

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ categorical –∫–æ–ª–æ–Ω–∫–∞—Ö
        if '–î–µ–π—Å—Ç–≤–∏–µ' in self.events_df.columns:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ object –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
            self.events_df['–î–µ–π—Å—Ç–≤–∏–µ'] = self.events_df['–î–µ–π—Å—Ç–≤–∏–µ'].astype('object')
            self.events_df['–î–µ–π—Å—Ç–≤–∏–µ'] = self.events_df['–î–µ–π—Å—Ç–≤–∏–µ'].fillna('–ù–µ —É–∫–∞–∑–∞–Ω–æ')
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ category
            self.events_df['–î–µ–π—Å—Ç–≤–∏–µ'] = self.events_df['–î–µ–π—Å—Ç–≤–∏–µ'].astype('category')

        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∫—Ä–∏—Ç–∏—á–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
        critical_columns = ['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è', '–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞']
        initial_count = len(self.events_df)
        self.events_df = self.events_df.dropna(subset=critical_columns).copy()
        removed_count = initial_count - len(self.events_df)
        if removed_count > 0:
            logger.info(f"–£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∫—Ä–∏—Ç–∏—á–Ω—ã–º–∏ –ø–æ–ª—è–º–∏: {removed_count}")

        logger.info("–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def create_features(self) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö

        Returns:
            DataFrame —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –Ω–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        if self.events_df is None:
            raise ValueError("Events –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—É—Å—Ç—ã–µ
        if len(self.events_df) == 0:
            logger.error("DataFrame –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏!")
            raise ValueError("DataFrame –ø—É—Å—Ç–æ–π, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏")

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        self.events_df['date'] = self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].dt.date
        self.events_df['hour'] = self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].dt.hour
        self.events_df['day_of_week'] = self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].dt.dayofweek
        self.events_df['is_weekend'] = self.events_df['day_of_week'].isin([5, 6])
        self.events_df['month'] = self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].dt.month
        self.events_df['day'] = self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].dt.day

        # –í—Ä–µ–º—è —Å—É—Ç–æ–∫
        def get_time_of_day(hour):
            if pd.isna(hour):
                return '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            if 6 <= hour < 12:
                return '–£—Ç—Ä–æ'
            elif 12 <= hour < 18:
                return '–î–µ–Ω—å'
            elif 18 <= hour < 24:
                return '–í–µ—á–µ—Ä'
            else:
                return '–ù–æ—á—å'

        self.events_df['time_of_day'] = self.events_df['hour'].apply(get_time_of_day)
        self.events_df['time_of_day'] = self.events_df['time_of_day'].astype('category')

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        logger.info("–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
        self.merged_df = self.events_df.merge(
            self.users_df,
            left_on='–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞',
            right_on='number',
            how='left'
        )

        # –í–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –≥—Ä—É–ø–ø—ã
        def get_age_group(age):
            if pd.isna(age):
                return '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            elif age < 25:
                return '18-24'
            elif age < 35:
                return '25-34'
            elif age < 45:
                return '35-44'
            elif age < 55:
                return '45-54'
            elif age < 65:
                return '55-64'
            else:
                return '65+'

        self.merged_df['age_group'] = self.merged_df['age_back'].apply(get_age_group)
        self.merged_df['age_group'] = self.merged_df['age_group'].astype('category')

        logger.info(f"–°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ò—Ç–æ–≥–æ–≤–∞—è —Ñ–æ—Ä–º–∞: {self.merged_df.shape}")
        self.stats['merged_df_shape'] = self.merged_df.shape

        return self.merged_df

    def generate_profile_report(self) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π –¥–∞–Ω–Ω—ã—Ö —Å —ç–∫—Å–ø–æ—Ä—Ç–æ–º –≤ JSON
        –°–æ–∑–¥–∞—ë—Ç –ø—Ä–æ—Ñ–∏–ª—å —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (merged_df)

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –¥–∞–Ω–Ω—ã—Ö
        """
        logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª—è –¥–∞–Ω–Ω—ã—Ö...")

        if self.merged_df is None or len(self.merged_df) == 0:
            logger.error("–û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ —Å–æ–∑–¥–∞–Ω –∏–ª–∏ –ø—É—Å—Ç–æ–π. –í—ã–∑–æ–≤–∏—Ç–µ create_features() —Å–Ω–∞—á–∞–ª–∞.")
            raise ValueError("merged_df –Ω–µ —Å–æ–∑–¥–∞–Ω. –í—ã–∑–æ–≤–∏—Ç–µ create_features() –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –ø—Ä–æ—Ñ–∏–ª—è.")

        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        html_path = Path(self.config['reports']['html_path'])
        json_path = Path(self.config['reports']['json_path'])
        html_path.mkdir(parents=True, exist_ok=True)
        json_path.mkdir(parents=True, exist_ok=True)

        # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ydata-profiling
        try:
            from ydata_profiling import ProfileReport

            # ==========================================
            # –ü–†–û–§–ò–õ–¨ –î–õ–Ø MERGED_DF
            # ==========================================

            # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É
            sample_size = min(4000000, len(self.merged_df))
            if len(self.merged_df) > sample_size:
                logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ {sample_size} —Å—Ç—Ä–æ–∫ –∏–∑ {len(self.merged_df)} –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è")
                merged_sample = self.merged_df.sample(n=sample_size, random_state=42)
            else:
                merged_sample = self.merged_df

            logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è –¥–ª—è merged_df...")
            logger.info("–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...")

            merged_report = ProfileReport(
                merged_sample,
                title='MCD Application - Complete Data Profile',
                minimal=False,  # –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç —Å–æ –≤—Å–µ–º–∏ –¥–µ—Ç–∞–ª—è–º–∏
                explorative=True,
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                correlations={
                    "auto": {"calculate": True},
                    "pearson": {"calculate": True},
                    "spearman": {"calculate": True},
                    "kendall": {"calculate": False},  # –ú–µ–¥–ª–µ–Ω–Ω–æ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                    "phi_k": {"calculate": True},
                    "cramers": {"calculate": True},
                },
                interactions={
                    "continuous": True,
                    "targets": []
                },
                missing_diagrams={
                    "heatmap": True,
                    "dendrogram": True,
                    "matrix": True,
                    "bar": True
                }
            )

            # ==========================================
            # –°–û–•–†–ê–ù–ï–ù–ò–ï HTML
            # ==========================================
            logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML –ø—Ä–æ—Ñ–∏–ª—è...")
            merged_report.to_file(str(html_path / 'merged_data_profile.html'))
            logger.info(f"‚úì HTML –ø—Ä–æ—Ñ–∏–ª—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {html_path / 'merged_data_profile.html'}")

            # ==========================================
            # –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–û–õ–ù–û–ì–û JSON –ü–†–û–§–ò–õ–Ø
            # ==========================================
            logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ JSON –ø—Ä–æ—Ñ–∏–ª—è...")
            merged_json = merged_report.to_json()
            with open(json_path / 'merged_data_profile_full.json', 'w', encoding='utf-8') as f:
                f.write(merged_json)
            logger.info(f"‚úì –ü–æ–ª–Ω—ã–π JSON –ø—Ä–æ—Ñ–∏–ª—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {json_path / 'merged_data_profile_full.json'}")

        except ImportError:
            logger.warning("ydata-profiling –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º HTML –∏ –ø–æ–ª–Ω—ã–µ JSON –æ—Ç—á–µ—Ç—ã")
            logger.info("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ydata-profiling")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–æ—Ñ–∏–ª—è: {e}")
            logger.info("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Å–æ–∑–¥–∞–Ω–∏–µ–º –∫—Ä–∞—Ç–∫–æ–≥–æ summary...")

        # ==========================================
        # –°–û–ó–î–ê–ù–ò–ï –ö–†–ê–¢–ö–û–ì–û JSON SUMMARY
        # ==========================================
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ JSON summary...")
        json_summary = self._create_json_summary()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ summary
        with open(json_path / 'data_profile_summary.json', 'w', encoding='utf-8') as f:
            json.dump(json_summary, f, ensure_ascii=False, indent=2)

        logger.info(f"‚úì JSON summary —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {json_path / 'data_profile_summary.json'}")

        # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç
        print(f"\n{'=' * 80}")
        print("–ü–†–û–§–ò–õ–ò –î–ê–ù–ù–´–• –°–û–ó–î–ê–ù–´:")
        print(f"{'=' * 80}")
        print(f"üìä HTML –æ—Ç—á—ë—Ç:        {html_path / 'merged_data_profile.html'}")
        print(f"üìÑ JSON (–ø–æ–ª–Ω—ã–π):     {json_path / 'merged_data_profile_full.json'}")
        print(f"üìã JSON (summary):    {json_path / 'data_profile_summary.json'}")
        print(f"{'=' * 80}\n")

        return json_summary
    
    def _create_json_summary(self) -> Dict:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ JSON –æ—Ç—á–µ—Ç–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        summary = {
            'generated_at': datetime.now().isoformat(),
            'events': {
                'total_rows': int(len(self.events_df)),
                'total_columns': int(len(self.events_df.columns)),
                'duplicates_removed': int(self.stats.get('events_duplicates_removed', 0)),
                'date_range': {
                    'min': str(self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].min()) if len(self.events_df) > 0 else 'N/A',
                    'max': str(self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].max()) if len(self.events_df) > 0 else 'N/A',
                    'days': int((self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].max() -
                               self.events_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].min()).days) if len(self.events_df) > 0 else 0
                },
                'unique_devices': int(self.events_df['–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'].nunique()),
                'unique_sessions': int(self.events_df['–ù–æ–º–µ—Ä —Å–µ—Å—Å–∏–∏ –≤ —Ä–∞–º–∫–∞—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'].nunique()),
                'columns_info': {},
                'top_screens': {},
                'top_functions': {},
                'device_info': {}
            },
            'users': {
                'total_rows': int(len(self.users_df)),
                'duplicates_removed': int(self.stats.get('users_duplicates_removed', 0)),
                'age_stats': {},
                'gender_distribution': {}
            }
        }
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º
        for col in self.events_df.columns:
            summary['events']['columns_info'][col] = {
                'dtype': str(self.events_df[col].dtype),
                'null_count': int(self.events_df[col].isna().sum()),
                'null_percentage': float(self.events_df[col].isna().sum() / len(self.events_df) * 100),
                'unique_values': int(self.events_df[col].nunique())
            }
        
        # –¢–æ–ø —ç–∫—Ä–∞–Ω—ã –∏ —Ñ—É–Ω–∫—Ü–∏–∏
        summary['events']['top_screens'] = {
            str(k): int(v) for k, v in 
            self.events_df['–≠–∫—Ä–∞–Ω'].value_counts().head(10).items()
        }
        
        summary['events']['top_functions'] = {
            str(k): int(v) for k, v in 
            self.events_df['–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª'].value_counts().head(10).items()
        }
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö
        summary['events']['device_info'] = {
            'os_distribution': {
                str(k): int(v) for k, v in 
                self.events_df['–û–°'].value_counts().items()
            },
            'device_types': {
                str(k): int(v) for k, v in 
                self.events_df['–¢–∏–ø —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'].value_counts().items()
            },
            'top_manufacturers': {
                str(k): int(v) for k, v in 
                self.events_df['–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'].value_counts().head(10).items()
            }
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        summary['users']['age_stats'] = {
            'mean': float(self.users_df['age_back'].mean()),
            'median': float(self.users_df['age_back'].median()),
            'min': int(self.users_df['age_back'].min()),
            'max': int(self.users_df['age_back'].max()),
            'std': float(self.users_df['age_back'].std())
        }
        
        summary['users']['gender_distribution'] = {
            str(k): int(v) for k, v in 
            self.users_df['gender'].value_counts().items()
        }
        
        return summary

    def save_processed_data(self) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ CSV
        """
        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

        processed_path = Path(self.config['data']['processed_path'])
        processed_path.mkdir(parents=True, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ merged_df –≤ CSV
        if self.merged_df is not None and len(self.merged_df) > 0:
            csv_file = processed_path / 'merged_data_test.csv'
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {csv_file}...")

            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è date –≤ string –¥–ª—è CSV
            df_to_save = self.merged_df.copy()
            if 'date' in df_to_save.columns:
                df_to_save['date'] = df_to_save['date'].astype(str)

            df_to_save.to_csv(csv_file, index=False, encoding='utf-8')

            file_size = csv_file.stat().st_size / 1024 ** 2
            logger.info(f"‚úì Merged_df —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {csv_file} ({file_size:.2f} MB)")

        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ events –≤ CSV (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if self.events_df is not None and len(self.events_df) > 0:
            events_csv = processed_path / 'events_cleaned.csv'
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –≤ {events_csv}...")

            df_to_save = self.events_df.copy()
            if 'date' in df_to_save.columns:
                df_to_save['date'] = df_to_save['date'].astype(str)

            df_to_save.to_csv(events_csv, index=False, encoding='utf-8')
            logger.info(f"‚úì –û—á–∏—â–µ–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {events_csv}")

        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    def load_merged_data_base(self, path: str = None) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ merged_df –∏–∑ CSV —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ç–∏–ø–∏–∑–∞—Ü–∏–µ–π

        Args:
            path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É CSV (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è data/processed/merged_data.csv)

        Returns:
            DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏
        """
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ merged_df –∏–∑ CSV...")

        if path is None:
            processed_path = Path(self.config['data']['processed_path'])
            path = processed_path / 'merged_data.csv'

        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(
                f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}\n"
                f"–ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–ª–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö —Å–Ω–∞—á–∞–ª–∞."
            )

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ CSV
        logger.info(f"–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {path}")
        self.merged_df = pd.read_csv(path, low_memory=False)

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.merged_df):,} —Å—Ç—Ä–æ–∫, {len(self.merged_df.columns)} –∫–æ–ª–æ–Ω–æ–∫")

        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        logger.info("–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")

        # DateTime –∫–æ–ª–æ–Ω–∫–∞
        if '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' in self.merged_df.columns:
            logger.debug("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' –≤ datetime...")
            self.merged_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'] = pd.to_datetime(
                self.merged_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'],
                utc=True,
                errors='coerce'
            )

        # Categorical –∫–æ–ª–æ–Ω–∫–∏
        categorical_cols = [
            '–≠–∫—Ä–∞–Ω', '–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª', '–î–µ–π—Å—Ç–≤–∏–µ',
            '–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞', '–ú–æ–¥–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞',
            '–¢–∏–ø —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞', '–û–°', 'time_of_day',
            'gender', 'age_group'
        ]

        for col in categorical_cols:
            if col in self.merged_df.columns:
                logger.debug(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è {col} –≤ category")
                self.merged_df[col] = self.merged_df[col].astype('category')

        # Boolean –∫–æ–ª–æ–Ω–∫–∞
        if 'is_weekend' in self.merged_df.columns:
            self.merged_df['is_weekend'] = self.merged_df['is_weekend'].astype('bool')

        # Integer –∫–æ–ª–æ–Ω–∫–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
        int_cols = {
            '–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞': 'int32',
            '–ù–æ–º–µ—Ä —Å–µ—Å—Å–∏–∏ –≤ —Ä–∞–º–∫–∞—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞': 'int64',
            'hour': 'int8',
            'day_of_week': 'int8',
            'month': 'int8',
            'day': 'int8',
            'number': 'int32'
        }

        for col, dtype in int_cols.items():
            if col in self.merged_df.columns:
                logger.debug(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è {col} –≤ {dtype}")
                self.merged_df[col] = self.merged_df[col].astype(dtype)

        # Nullable integer –¥–ª—è age_back
        if 'age_back' in self.merged_df.columns:
            self.merged_df['age_back'] = pd.to_numeric(
                self.merged_df['age_back'],
                errors='coerce'
            ).astype('Int16')

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è date –æ–±—Ä–∞—Ç–Ω–æ –≤ date (–∏–∑ string)
        if 'date' in self.merged_df.columns:
            self.merged_df['date'] = pd.to_datetime(
                self.merged_df['date'],
                errors='coerce'
            ).dt.date

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
        memory_usage = self.merged_df.memory_usage(deep=True).sum() / 1024 ** 2
        logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_usage:.2f} MB")

        logger.info("‚úì –ó–∞–≥—Ä—É–∑–∫–∞ merged_df –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

        return self.merged_df

    def load_merged_data(self, path: str = None) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ merged_df –∏–∑ CSV —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ç–∏–ø–∏–∑–∞—Ü–∏–µ–π

        Args:
            path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É CSV (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è data/processed/merged_data.csv)

        Returns:
            DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏
        """
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ merged_df –∏–∑ CSV...")

        if path is None:
            processed_path = Path(self.config['data']['processed_path'])
            path = processed_path / 'merged_data.csv'

        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(
                f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}\n"
                f"–ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–ª–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö —Å–Ω–∞—á–∞–ª–∞."
            )

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ CSV
        logger.info(f"–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {path}")
        self.merged_df = pd.read_csv(path, low_memory=False)

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.merged_df):,} —Å—Ç—Ä–æ–∫, {len(self.merged_df.columns)} –∫–æ–ª–æ–Ω–æ–∫")

        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        logger.info("–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")

        # DateTime –∫–æ–ª–æ–Ω–∫–∞
        if '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' in self.merged_df.columns:
            logger.debug("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' –≤ datetime...")
            self.merged_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'] = pd.to_datetime(
                self.merged_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'],
                utc=True,
                errors='coerce'
            )

        # Categorical –∫–æ–ª–æ–Ω–∫–∏
        categorical_cols = [
            '–≠–∫—Ä–∞–Ω', '–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª', '–î–µ–π—Å—Ç–≤–∏–µ',
            '–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞', '–ú–æ–¥–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞',
            '–¢–∏–ø —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞', '–û–°', 'time_of_day',
            'gender', 'age_group'
        ]

        for col in categorical_cols:
            if col in self.merged_df.columns:
                logger.debug(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è {col} –≤ category")
                self.merged_df[col] = self.merged_df[col].astype('category')

        # Boolean –∫–æ–ª–æ–Ω–∫–∞
        if 'is_weekend' in self.merged_df.columns:
            self.merged_df['is_weekend'] = self.merged_df['is_weekend'].astype('bool')

        # Integer –∫–æ–ª–æ–Ω–∫–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
        int_cols = {
            # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            '–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞': 'int32',
            '–ù–æ–º–µ—Ä —Å–µ—Å—Å–∏–∏ –≤ —Ä–∞–º–∫–∞—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞': 'int64',
            'hour': 'int8',
            'day_of_week': 'int8',
            'month': 'int8',
            'day': 'int8',
            'number': 'int32',
            'global_session_id': 'int32',  # –ì–ª–æ–±–∞–ª—å–Ω—ã–π ID —Å–µ—Å—Å–∏–∏ (–¥–æ ~1M)
            'duration_seconds': 'int32',  # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ–±—ã—Ç–∏—è (—Å–µ–∫—É–Ω–¥—ã)
            'click_count': 'int16',  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–∫–æ–≤ (1-100)
            'dbl_duration_seconds': 'int32',  # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –¥—É–±–ª–µ–π
            'dbl_count': 'int16'  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –¥—É–±–ª–µ–π
        }

        for col, dtype in int_cols.items():
            if col in self.merged_df.columns:
                try:
                    logger.debug(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è {col} –≤ {dtype}")
                    self.merged_df[col] = pd.to_numeric(
                        self.merged_df[col],
                        errors='coerce'
                    ).fillna(0).astype(dtype)
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å {col} –≤ {dtype}: {e}")
                    # Fallback –Ω–∞ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π —Ç–∏–ø
                    if dtype == 'int8':
                        fallback_dtype = 'int16'
                    elif dtype == 'int16':
                        fallback_dtype = 'int32'
                    elif dtype == 'int32':
                        fallback_dtype = 'int64'
                    else:
                        fallback_dtype = 'int64'

                    logger.warning(f"–ò—Å–ø–æ–ª—å–∑—É—é fallback —Ç–∏–ø {fallback_dtype}")
                    self.merged_df[col] = pd.to_numeric(
                        self.merged_df[col],
                        errors='coerce'
                    ).fillna(0).astype(fallback_dtype)

        # Nullable integer –¥–ª—è age_back
        if 'age_back' in self.merged_df.columns:
            self.merged_df['age_back'] = pd.to_numeric(
                self.merged_df['age_back'],
                errors='coerce'
            ).astype('Int16')

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è date –æ–±—Ä–∞—Ç–Ω–æ –≤ date (–∏–∑ string)
        if 'date' in self.merged_df.columns:
            self.merged_df['date'] = pd.to_datetime(
                self.merged_df['date'],
                errors='coerce'
            ).dt.date

        # ============================================================
        # –°–¢–ê–¢–£–° –ù–û–í–´–• –ö–û–õ–û–ù–û–ö –û–ë–†–ê–ë–û–¢–ö–ò
        # ============================================================

        pipeline_cols_status = {}

        if 'global_session_id' in self.merged_df.columns:
            unique_sessions = self.merged_df['global_session_id'].nunique()
            pipeline_cols_status['global_session_id'] = f"‚úì {unique_sessions:,} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–µ—Å—Å–∏–π"
        else:
            pipeline_cols_status['global_session_id'] = "‚úó –ù–µ –Ω–∞–π–¥–µ–Ω (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ add_global_session_id)"

        if 'duration_seconds' in self.merged_df.columns:
            avg_duration = self.merged_df['duration_seconds'].mean()
            max_duration = self.merged_df['duration_seconds'].max()
            pipeline_cols_status['duration_seconds'] = f"‚úì –°—Ä–µ–¥–Ω–µ–µ: {avg_duration:.1f}—Å, –ú–∞–∫—Å: {max_duration:,}—Å"
        else:
            pipeline_cols_status['duration_seconds'] = "‚úó –ù–µ –Ω–∞–π–¥–µ–Ω (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ calculate_event_duration)"

        if 'click_count' in self.merged_df.columns:
            avg_tries = self.merged_df['click_count'].mean()
            max_tries = self.merged_df['click_count'].max()
            pipeline_cols_status['click_count'] = f"‚úì –°—Ä–µ–¥–Ω–µ–µ: {avg_tries:.2f}, –ú–∞–∫—Å: {max_tries}"
        else:
            pipeline_cols_status['click_count'] = "‚úó –ù–µ –Ω–∞–π–¥–µ–Ω (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ remove_consecutive_duplicates_with_tries)"

        if 'dbl_duration_seconds' in self.merged_df.columns:
            avg_dbl_dur = self.merged_df['dbl_duration_seconds'].mean()
            sum_dbl_dur = self.merged_df['dbl_duration_seconds'].sum()
            pipeline_cols_status[
                'dbl_duration_seconds'] = f"‚úì –°—Ä–µ–¥–Ω–µ–µ: {avg_dbl_dur:.1f}—Å, –°—É–º–º–∞: {sum_dbl_dur / 3600:.1f}—á"
        else:
            pipeline_cols_status[
                'dbl_duration_seconds'] = "‚úó –ù–µ –Ω–∞–π–¥–µ–Ω (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ remove_consecutive_duplicates_with_tries)"

        if 'dbl_count' in self.merged_df.columns:
            avg_dbl = self.merged_df['dbl_count'].mean()
            max_dbl = self.merged_df['dbl_count'].max()
            pipeline_cols_status['dbl_count'] = f"‚úì –°—Ä–µ–¥–Ω–µ–µ: {avg_dbl:.2f}, –ú–∞–∫—Å: {max_dbl}"
        else:
            pipeline_cols_status['dbl_count'] = "‚úó –ù–µ –Ω–∞–π–¥–µ–Ω (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ remove_consecutive_duplicates_with_tries)"

        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–∞ pipeline –∫–æ–ª–æ–Ω–æ–∫
        if any('‚úì' in status for status in pipeline_cols_status.values()):
            logger.info(f"\n{'=' * 60}")
            logger.info("–°–¢–ê–¢–£–° –ö–û–õ–û–ù–û–ö PIPELINE –û–ë–†–ê–ë–û–¢–ö–ò:")
            logger.info(f"{'=' * 60}")
            for col_name, status in pipeline_cols_status.items():
                logger.info(f"  {col_name}: {status}")
            logger.info(f"{'=' * 60}\n")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
        memory_usage = self.merged_df.memory_usage(deep=True).sum() / 1024 ** 2
        logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_usage:.2f} MB")

        logger.info("‚úì –ó–∞–≥—Ä—É–∑–∫–∞ merged_df –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

        return self.merged_df

    def load_merged_data_funnel(self, path: str = None) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ merged_df –∏–∑ CSV —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ç–∏–ø–∏–∑–∞—Ü–∏–µ–π
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ funnel features (68 –∫–æ–ª–æ–Ω–æ–∫ –æ—Ç FunnelFeaturesExtractor)

        Args:
            path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É CSV (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è data/processed/merged_data.csv)

        Returns:
            DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏
        """
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ merged_df –∏–∑ CSV...")

        if path is None:
            processed_path = Path(self.config['data']['processed_path'])
            path = processed_path / 'merged_data.csv'

        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(
                f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}\n"
                f"–ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–ª–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö —Å–Ω–∞—á–∞–ª–∞."
            )

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ CSV
        logger.info(f"–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {path}")
        self.merged_df = pd.read_csv(path, low_memory=False)

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.merged_df):,} —Å—Ç—Ä–æ–∫, {len(self.merged_df.columns)} –∫–æ–ª–æ–Ω–æ–∫")

        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        logger.info("–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")

        # DateTime –∫–æ–ª–æ–Ω–∫–∞
        if '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' in self.merged_df.columns:
            logger.debug("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' –≤ datetime...")
            self.merged_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'] = pd.to_datetime(
                self.merged_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'],
                utc=True,
                errors='coerce'
            )

        # Categorical –∫–æ–ª–æ–Ω–∫–∏
        categorical_cols = [
            '–≠–∫—Ä–∞–Ω', '–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª', '–î–µ–π—Å—Ç–≤–∏–µ',
            '–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞', '–ú–æ–¥–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞',
            '–¢–∏–ø —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞', '–û–°', 'time_of_day',
            'gender', 'age_group'
        ]

        for col in categorical_cols:
            if col in self.merged_df.columns:
                logger.debug(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è {col} –≤ category")
                self.merged_df[col] = self.merged_df[col].astype('category')

        # Boolean –∫–æ–ª–æ–Ω–∫–∞
        if 'is_weekend' in self.merged_df.columns:
            self.merged_df['is_weekend'] = self.merged_df['is_weekend'].astype('bool')

        # Integer –∫–æ–ª–æ–Ω–∫–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
        int_cols = {
            # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            '–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞': 'int32',
            '–ù–æ–º–µ—Ä —Å–µ—Å—Å–∏–∏ –≤ —Ä–∞–º–∫–∞—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞': 'int64',
            'hour': 'int8',
            'day_of_week': 'int8',
            'month': 'int8',
            'day': 'int8',
            'number': 'int32',
            'global_session_id': 'int32',  # –ì–ª–æ–±–∞–ª—å–Ω—ã–π ID —Å–µ—Å—Å–∏–∏ (–¥–æ ~1M)
            'duration_seconds': 'int32',  # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ–±—ã—Ç–∏—è (—Å–µ–∫—É–Ω–¥—ã)
            'click_count': 'int16',  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–∫–æ–≤ (1-100)
            'dbl_duration_seconds': 'int32',  # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –¥—É–±–ª–µ–π
            'dbl_count': 'int16'  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –¥—É–±–ª–µ–π
        }

        # ============================================================
        # FUNNEL FEATURES - –ö–û–õ–û–ù–ö–ò –§–£–ù–ö–¶–ò–û–ù–ê–õ–¨–ù–´–• –ë–õ–û–ö–û–í (68 –∫–æ–ª–æ–Ω–æ–∫)
        # ============================================================
        # –ü—Ä–µ—Ñ–∏–∫—Å—ã –≤—Å–µ—Ö 17 —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤
        funnel_prefixes = [
            'request',  # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞—è–≤–∫–∏
            'req_manage',  # –ü—Ä–æ—Å–º–æ—Ç—Ä –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞—è–≤–∫–∞–º–∏
            'profile',  # –ü—Ä–æ—Ñ–∏–ª—å
            'nav',  # –ù–∞–≤–∏–≥–∞—Ü–∏—è
            'notif',  # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
            'poll_oss',  # –û–ø—Ä–æ—Å—ã –∏ —Å–æ–±—Ä–∞–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤
            'rewards',  # –ë–∞–ª–ª—ã –∏ –ø–æ–æ—â—Ä–µ–Ω–∏—è
            'my_home',  # –ú–æ–π –¥–æ–º
            'partners',  # –£—Å–ª—É–≥–∏ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤
            'transport',  # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º
            'ann_view',  # –ü—Ä–æ—Å–º–æ—Ç—Ä –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            'smart',  # –£–º–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
            'support',  # –¢–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞
            'guest',  # –ì–æ—Å—Ç–µ–≤–æ–π –¥–æ—Å—Ç—É–ø
            'city_serv',  # –ì–æ—Ä–æ–¥—Å–∫–∏–µ —Å–µ—Ä–≤–∏—Å—ã
            'address',  # –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥—Ä–µ—Å–∞
            'ann_create'  # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        ]

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø—ã –¥–ª—è funnel features
        for prefix in funnel_prefixes:
            int_cols[f'{prefix}_count'] = 'int16'  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π (0-1000)
            int_cols[f'{prefix}_max_step'] = 'int8'  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —à–∞–≥ (-1 –¥–æ 50)
            int_cols[f'{prefix}_success_count'] = 'int8'  # –£—Å–ø–µ—à–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (0-50)
            int_cols[f'{prefix}_review_count'] = 'int8'  # Review –¥–µ–π—Å—Ç–≤–∏—è (0-50)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–∏–ø–∏–∑–∞—Ü–∏—é
        for col, dtype in int_cols.items():
            if col in self.merged_df.columns:
                try:
                    logger.debug(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è {col} –≤ {dtype}")

                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è max_step (–º–æ–∂–µ—Ç –±—ã—Ç—å -1)
                    if col.endswith('_max_step'):
                        self.merged_df[col] = pd.to_numeric(
                            self.merged_df[col],
                            errors='coerce'
                        ).fillna(-1).astype(dtype)
                    else:
                        self.merged_df[col] = pd.to_numeric(
                            self.merged_df[col],
                            errors='coerce'
                        ).fillna(0).astype(dtype)

                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å {col} –≤ {dtype}: {e}")
                    # Fallback –Ω–∞ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π —Ç–∏–ø
                    if dtype == 'int8':
                        fallback_dtype = 'int16'
                    elif dtype == 'int16':
                        fallback_dtype = 'int32'
                    elif dtype == 'int32':
                        fallback_dtype = 'int64'
                    else:
                        fallback_dtype = 'int64'

                    logger.warning(f"–ò—Å–ø–æ–ª—å–∑—É—é fallback —Ç–∏–ø {fallback_dtype}")

                    if col.endswith('_max_step'):
                        self.merged_df[col] = pd.to_numeric(
                            self.merged_df[col],
                            errors='coerce'
                        ).fillna(-1).astype(fallback_dtype)
                    else:
                        self.merged_df[col] = pd.to_numeric(
                            self.merged_df[col],
                            errors='coerce'
                        ).fillna(0).astype(fallback_dtype)

        # Nullable integer –¥–ª—è age_back
        if 'age_back' in self.merged_df.columns:
            self.merged_df['age_back'] = pd.to_numeric(
                self.merged_df['age_back'],
                errors='coerce'
            ).astype('Int16')

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è date –æ–±—Ä–∞—Ç–Ω–æ –≤ date (–∏–∑ string)
        if 'date' in self.merged_df.columns:
            self.merged_df['date'] = pd.to_datetime(
                self.merged_df['date'],
                errors='coerce'
            ).dt.date

        # ============================================================
        # –°–¢–ê–¢–£–° –ö–û–õ–û–ù–û–ö –û–°–ù–û–í–ù–û–ì–û PIPELINE
        # ============================================================

        pipeline_cols_status = {}

        if 'global_session_id' in self.merged_df.columns:
            unique_sessions = self.merged_df['global_session_id'].nunique()
            pipeline_cols_status['global_session_id'] = f"‚úì {unique_sessions:,} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–µ—Å—Å–∏–π"
        else:
            pipeline_cols_status['global_session_id'] = "‚úó –ù–µ –Ω–∞–π–¥–µ–Ω (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ add_global_session_id)"

        if 'duration_seconds' in self.merged_df.columns:
            avg_duration = self.merged_df['duration_seconds'].mean()
            max_duration = self.merged_df['duration_seconds'].max()
            pipeline_cols_status['duration_seconds'] = f"‚úì –°—Ä–µ–¥–Ω–µ–µ: {avg_duration:.1f}—Å, –ú–∞–∫—Å: {max_duration:,}—Å"
        else:
            pipeline_cols_status['duration_seconds'] = "‚úó –ù–µ –Ω–∞–π–¥–µ–Ω (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ calculate_event_duration)"

        if 'click_count' in self.merged_df.columns:
            avg_tries = self.merged_df['click_count'].mean()
            max_tries = self.merged_df['click_count'].max()
            pipeline_cols_status['click_count'] = f"‚úì –°—Ä–µ–¥–Ω–µ–µ: {avg_tries:.2f}, –ú–∞–∫—Å: {max_tries}"
        else:
            pipeline_cols_status['click_count'] = "‚úó –ù–µ –Ω–∞–π–¥–µ–Ω (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ remove_consecutive_duplicates_with_tries)"

        if 'dbl_duration_seconds' in self.merged_df.columns:
            avg_dbl_dur = self.merged_df['dbl_duration_seconds'].mean()
            sum_dbl_dur = self.merged_df['dbl_duration_seconds'].sum()
            pipeline_cols_status[
                'dbl_duration_seconds'] = f"‚úì –°—Ä–µ–¥–Ω–µ–µ: {avg_dbl_dur:.1f}—Å, –°—É–º–º–∞: {sum_dbl_dur / 3600:.1f}—á"
        else:
            pipeline_cols_status[
                'dbl_duration_seconds'] = "‚úó –ù–µ –Ω–∞–π–¥–µ–Ω (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ remove_consecutive_duplicates_with_tries)"

        if 'dbl_count' in self.merged_df.columns:
            avg_dbl = self.merged_df['dbl_count'].mean()
            max_dbl = self.merged_df['dbl_count'].max()
            pipeline_cols_status['dbl_count'] = f"‚úì –°—Ä–µ–¥–Ω–µ–µ: {avg_dbl:.2f}, –ú–∞–∫—Å: {max_dbl}"
        else:
            pipeline_cols_status['dbl_count'] = "‚úó –ù–µ –Ω–∞–π–¥–µ–Ω (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ remove_consecutive_duplicates_with_tries)"

        # ============================================================
        # –°–¢–ê–¢–£–° FUNNEL FEATURES –ö–û–õ–û–ù–û–ö
        # ============================================================

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ funnel features
        funnel_cols_found = [col for col in self.merged_df.columns
                             if any(col.startswith(f'{prefix}_') for prefix in funnel_prefixes)]

        funnel_features_status = {}

        if funnel_cols_found:
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞
            blocks_with_features = {}
            for prefix in funnel_prefixes:
                prefix_cols = [col for col in funnel_cols_found if col.startswith(f'{prefix}_')]
                if prefix_cols:
                    blocks_with_features[prefix] = len(prefix_cols)

            funnel_features_status['total_blocks'] = f"‚úì {len(blocks_with_features)}/17 –±–ª–æ–∫–æ–≤"
            funnel_features_status['total_columns'] = f"‚úì {len(funnel_cols_found)}/68 –∫–æ–ª–æ–Ω–æ–∫"

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–æ–ø –±–ª–æ–∫–∞–º
            if blocks_with_features:
                # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ—Å—Å–∏–π —Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞
                block_stats = []
                for prefix in funnel_prefixes:
                    count_col = f'{prefix}_count'
                    if count_col in self.merged_df.columns:
                        # –ë–µ—Ä–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —Å–µ—Å—Å–∏—è–º
                        sessions_with_block = (
                                self.merged_df.groupby('global_session_id')[count_col]
                                .first() > 0
                        ).sum()

                        if sessions_with_block > 0:
                            total_actions = self.merged_df.groupby('global_session_id')[count_col].first().sum()
                            block_stats.append({
                                'prefix': prefix,
                                'sessions': sessions_with_block,
                                'actions': int(total_actions)
                            })

                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–µ—Å—Å–∏–π
                block_stats.sort(key=lambda x: x['sessions'], reverse=True)
        else:
            funnel_features_status['status'] = (
                "‚úó –ù–µ –Ω–∞–π–¥–µ–Ω–æ (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ FunnelFeaturesExtractor.transform())"
            )

        # ============================================================
        # –í–´–í–û–î –°–¢–ê–¢–£–°–û–í
        # ============================================================

        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö pipeline –∫–æ–ª–æ–Ω–æ–∫
        if any('‚úì' in status for status in pipeline_cols_status.values()):
            logger.info(f"\n{'=' * 70}")
            logger.info("–°–¢–ê–¢–£–° –ö–û–õ–û–ù–û–ö –û–°–ù–û–í–ù–û–ì–û PIPELINE:")
            logger.info(f"{'=' * 70}")
            for col_name, status in pipeline_cols_status.items():
                logger.info(f"  {col_name}: {status}")
            logger.info(f"{'=' * 70}")

        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–∞ funnel features
        if funnel_features_status:
            logger.info(f"\n{'=' * 70}")
            logger.info("–°–¢–ê–¢–£–° FUNNEL FEATURES (–§–£–ù–ö–¶–ò–û–ù–ê–õ–¨–ù–´–ï –ë–õ–û–ö–ò):")
            logger.info(f"{'=' * 70}")
            for key, status in funnel_features_status.items():
                logger.info(f"  {key}: {status}")

            # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –±–ª–æ–∫–∞–º
            if funnel_cols_found and 'global_session_id' in self.merged_df.columns:
                logger.info(f"\n  –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –±–ª–æ–∫–∞–º:")

                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                block_details = []
                for prefix in funnel_prefixes:
                    count_col = f'{prefix}_count'
                    if count_col in self.merged_df.columns:
                        sessions_data = self.merged_df.groupby('global_session_id')[count_col].first()
                        sessions_with_block = (sessions_data > 0).sum()

                        if sessions_with_block > 0:
                            total_sessions = len(sessions_data)
                            percentage = 100 * sessions_with_block / total_sessions
                            total_actions = int(sessions_data.sum())
                            avg_actions = sessions_data[sessions_data > 0].mean()

                            block_details.append({
                                'prefix': prefix,
                                'sessions': sessions_with_block,
                                'percent': percentage,
                                'actions': total_actions,
                                'avg': avg_actions
                            })

                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5
                block_details.sort(key=lambda x: x['sessions'], reverse=True)
                for i, detail in enumerate(block_details[:5], 1):
                    logger.info(
                        f"    {i}. {detail['prefix']:12s}: "
                        f"{detail['sessions']:6,} —Å–µ—Å—Å–∏–π ({detail['percent']:4.1f}%), "
                        f"{detail['actions']:7,} –¥–µ–π—Å—Ç–≤–∏–π, "
                        f"—Å—Ä–µ–¥–Ω–µ–µ: {detail['avg']:.2f}"
                    )

                if len(block_details) > 5:
                    logger.info(f"    ... –∏ –µ—â—ë {len(block_details) - 5} –±–ª–æ–∫–æ–≤")

            logger.info(f"{'=' * 70}\n")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
        memory_usage = self.merged_df.memory_usage(deep=True).sum() / 1024 ** 2
        logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_usage:.2f} MB")

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞
        logger.info(f"\n{'=' * 70}")
        logger.info("–ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê –ó–ê–ì–†–£–ñ–ï–ù–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê:")
        logger.info(f"{'=' * 70}")
        logger.info(f"  –°—Ç—Ä–æ–∫: {len(self.merged_df):,}")
        logger.info(f"  –ö–æ–ª–æ–Ω–æ–∫: {len(self.merged_df.columns)}")
        logger.info(f"  –ü–µ—Ä–∏–æ–¥: {self.merged_df['date'].min()} - {self.merged_df['date'].max()}")
        logger.info(f"  –ü–∞–º—è—Ç—å: {memory_usage:.2f} MB")

        # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∏ funnel –∫–æ–ª–æ–Ω–æ–∫
        base_cols = len([c for c in self.merged_df.columns if not any(c.startswith(f'{p}_') for p in funnel_prefixes)])
        funnel_cols_count = len(funnel_cols_found)

        logger.info(f"  –ë–∞–∑–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {base_cols}")
        if funnel_cols_count > 0:
            logger.info(f"  Funnel features: {funnel_cols_count}")
        logger.info(f"{'=' * 70}\n")

        logger.info("‚úì –ó–∞–≥—Ä—É–∑–∫–∞ merged_df –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

        return self.merged_df

    def add_global_session_id(self) -> pd.DataFrame:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID —Å–µ—Å—Å–∏–∏ –∏ –ø–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ

        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º global_session_id –∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
        """
        logger.info("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ global_session_id...")

        if self.merged_df is None or len(self.merged_df) == 0:
            raise ValueError("merged_df –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

        # –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä –∏ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ ID
        unique_sessions = self.merged_df[[
            '–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞',
            '–ù–æ–º–µ—Ä —Å–µ—Å—Å–∏–∏ –≤ —Ä–∞–º–∫–∞—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'
        ]].drop_duplicates()

        unique_sessions = unique_sessions.sort_values([
            '–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞',
            '–ù–æ–º–µ—Ä —Å–µ—Å—Å–∏–∏ –≤ —Ä–∞–º–∫–∞—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'
        ]).reset_index(drop=True)

        unique_sessions['global_session_id'] = range(1, len(unique_sessions) + 1)

        # –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ
        self.merged_df = self.merged_df.merge(
            unique_sessions,
            on=['–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞', '–ù–æ–º–µ—Ä —Å–µ—Å—Å–∏–∏ –≤ —Ä–∞–º–∫–∞—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'],
            how='left'
        )

        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {self.merged_df['global_session_id'].nunique():,} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö global_session_id")

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        logger.info("–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        self.merged_df = self.merged_df.sort_values(
            by=['global_session_id', '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'],
            ascending=[True, True]
        ).reset_index(drop=True)

        logger.info("‚úì global_session_id –¥–æ–±–∞–≤–ª–µ–Ω –∏ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã")

        return self.merged_df

    def calculate_event_duration(self) -> pd.DataFrame:
        """
        –†–∞—Å—á—ë—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Å–æ–±—ã—Ç–∏—è–º–∏ –≤ —Ä–∞–º–∫–∞—Ö —Å–µ—Å—Å–∏–∏

        –õ–æ–≥–∏–∫–∞:
        - –î–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫–æ–ª–æ–Ω–∫–∞ duration_seconds = 0 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        - –í —Ä–∞–º–∫–∞—Ö –∫–∞–∂–¥–æ–π global_session_id:
          * –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏ (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π): duration_seconds = —Ä–∞–∑–Ω–∏—Ü–∞ —Å –ü–û–°–õ–ï–î–£–Æ–©–ï–ô –∑–∞–ø–∏—Å—å—é –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
          * –ü–æ—Å–ª–µ–¥–Ω—è—è –∑–∞–ø–∏—Å—å –≤ —Å–µ—Å—Å–∏–∏: duration_seconds = 0

        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: duration_seconds –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ–≤—ë–ª –Ω–∞ —Ç–µ–∫—É—â–µ–º —ç–∫—Ä–∞–Ω–µ
        –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –¥–µ–π—Å—Ç–≤–∏—é.

        –ü—Ä–∏–º–µ—Ä:
        | time      | duration_seconds | –û–ø–∏—Å–∞–Ω–∏–µ                              |
        |-----------|------------------|---------------------------------------|
        | 10:00:00  | 5                | –ü—Ä–æ–≤—ë–ª 5 —Å–µ–∫—É–Ω–¥ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ–±—ã—Ç–∏—è |
        | 10:00:05  | 7                | –ü—Ä–æ–≤—ë–ª 7 —Å–µ–∫—É–Ω–¥                       |
        | 10:00:12  | 48               | –ü—Ä–æ–≤—ë–ª 48 —Å–µ–∫—É–Ω–¥                      |
        | 10:01:00  | 0                | –ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–±—ã—Ç–∏–µ –≤ —Å–µ—Å—Å–∏–∏            |

        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π duration_seconds
        """
        logger.info("–†–∞—Å—á—ë—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Å–æ–±—ã—Ç–∏—è–º–∏...")

        if self.merged_df is None or len(self.merged_df) == 0:
            raise ValueError("merged_df –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

        if 'global_session_id' not in self.merged_df.columns:
            raise ValueError("–¢—Ä–µ–±—É–µ—Ç—Å—è global_session_id (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ add_global_session_id)")

        time_col = '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'

        if time_col not in self.merged_df.columns:
            raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{time_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –≤—Ä–µ–º—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ datetime
        if not pd.api.types.is_datetime64_any_dtype(self.merged_df[time_col]):
            logger.info("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤ datetime...")
            self.merged_df[time_col] = pd.to_datetime(
                self.merged_df[time_col],
                utc=True,
                errors='coerce'
            )

        initial_rows = len(self.merged_df)
        logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {initial_rows:,}")

        # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã
        self.merged_df = self.merged_df.sort_values(
            by=['global_session_id', time_col],
            ascending=[True, True]
        ).reset_index(drop=True)

        logger.info("–î–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ global_session_id –∏ –≤—Ä–µ–º–µ–Ω–∏")

        # ============================================================
        # –†–ê–°–ß–Å–¢ –î–õ–ò–¢–ï–õ–¨–ù–û–°–¢–ò (–° –ü–û–°–õ–ï–î–£–Æ–©–ï–ô –ó–ê–ü–ò–°–¨–Æ)
        # ============================================================

        logger.info("–†–∞—Å—á—ë—Ç —Ä–∞–∑–Ω–∏—Ü—ã –≤–æ –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º —Å–æ–±—ã—Ç–∏–µ–º...")

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É duration_seconds (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0)
        self.merged_df['duration_seconds'] = 0

        # –ü–æ–ª—É—á–∞–µ–º –≤—Ä–µ–º—è –ü–û–°–õ–ï–î–£–Æ–©–ï–ì–û —Å–æ–±—ã—Ç–∏—è –í –¢–û–ô –ñ–ï –°–ï–°–°–ò–ò
        self.merged_df['next_time'] = self.merged_df.groupby(
            'global_session_id',
            sort=False
        )[time_col].shift(-1)  # ‚Üê –ò–ó–ú–ï–ù–ï–ù–û: shift(-1) –≤–º–µ—Å—Ç–æ shift(1)

        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É –≤–æ –≤—Ä–µ–º–µ–Ω–∏ (timedelta)
        time_diff = self.merged_df['next_time'] - self.merged_df[time_col]  # ‚Üê –ò–ó–ú–ï–ù–ï–ù–û: next - current

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º timedelta –≤ —Å–µ–∫—É–Ω–¥—ã
        # –î–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ —Å–µ—Å—Å–∏–∏ (–≥–¥–µ next_time = NaT) –±—É–¥–µ—Ç NaN, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ 0
        self.merged_df['duration_seconds'] = time_diff.dt.total_seconds().fillna(0)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ (int) –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        self.merged_df['duration_seconds'] = self.merged_df['duration_seconds'].astype('int32')

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
        self.merged_df = self.merged_df.drop(columns=['next_time'])

        logger.info("‚úì –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã")

        # ============================================================
        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        # ============================================================

        logger.info(f"\n{'=' * 60}")
        logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–õ–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
        logger.info(f"{'=' * 60}")

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_events = len(self.merged_df)
        last_events = (self.merged_df['duration_seconds'] == 0).sum()
        non_last_events = total_events - last_events

        logger.info(f"–í—Å–µ–≥–æ —Å–æ–±—ã—Ç–∏–π: {total_events:,}")
        logger.info(f"  - –ü–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–±—ã—Ç–∏–π –≤ —Å–µ—Å—Å–∏—è—Ö: {last_events:,} (duration=0)")
        logger.info(f"  - –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π: {non_last_events:,}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ duration_seconds (–∏—Å–∫–ª—é—á–∞—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è)
        non_zero_durations = self.merged_df[self.merged_df['duration_seconds'] > 0]['duration_seconds']

        if len(non_zero_durations) > 0:
            logger.info(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (—Å–µ–∫) –¥–ª—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π:")
            logger.info(f"  –°—Ä–µ–¥–Ω–µ–µ: {non_zero_durations.mean():.2f} —Å–µ–∫ ({non_zero_durations.mean() / 60:.2f} –º–∏–Ω)")
            logger.info(
                f"  –ú–µ–¥–∏–∞–Ω–∞: {non_zero_durations.median():.0f} —Å–µ–∫ ({non_zero_durations.median() / 60:.2f} –º–∏–Ω)")
            logger.info(f"  –ú–∏–Ω: {non_zero_durations.min()} —Å–µ–∫")
            logger.info(f"  –ú–∞–∫—Å: {non_zero_durations.max():,} —Å–µ–∫ ({non_zero_durations.max() / 3600:.2f} —á)")
            logger.info(f"  –°—Ç–¥. –æ—Ç–∫–ª.: {non_zero_durations.std():.2f} —Å–µ–∫")

            # –ü–µ—Ä—Ü–µ–Ω—Ç–∏–ª–∏
            percentiles = non_zero_durations.quantile([0.25, 0.5, 0.75, 0.90, 0.95, 0.99])
            logger.info(f"\n–ü–µ—Ä—Ü–µ–Ω—Ç–∏–ª–∏:")
            logger.info(f"  25%: {percentiles[0.25]:.0f} —Å–µ–∫")
            logger.info(f"  50%: {percentiles[0.50]:.0f} —Å–µ–∫")
            logger.info(f"  75%: {percentiles[0.75]:.0f} —Å–µ–∫")
            logger.info(f"  90%: {percentiles[0.90]:.0f} —Å–µ–∫")
            logger.info(f"  95%: {percentiles[0.95]:.0f} —Å–µ–∫")
            logger.info(f"  99%: {percentiles[0.99]:.0f} —Å–µ–∫")

            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º
            bins = [0, 1, 5, 10, 30, 60, 300, 600, 1800, 3600, float('inf')]
            labels = ['0-1—Å', '1-5—Å', '5-10—Å', '10-30—Å', '30—Å-1–º', '1-5–º', '5-10–º', '10-30–º', '30–º-1—á', '>1—á']

            self.merged_df['duration_bin'] = pd.cut(
                self.merged_df['duration_seconds'],
                bins=bins,
                labels=labels,
                include_lowest=True
            )

            duration_dist = self.merged_df[self.merged_df['duration_seconds'] > 0][
                'duration_bin'].value_counts().sort_index()

            logger.info(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
            for interval, count in duration_dist.items():
                percentage = count / non_last_events * 100
                logger.info(f"  {interval}: {count:,} ({percentage:.1f}%)")

            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
            self.merged_df = self.merged_df.drop(columns=['duration_bin'])

            # –ê–Ω–æ–º–∞–ª—å–Ω–æ –¥–æ–ª–≥–∏–µ –ø–∞—É–∑—ã (>30 –º–∏–Ω—É—Ç)
            long_pauses = (self.merged_df['duration_seconds'] > 1800).sum()
            if long_pauses > 0:
                long_pauses_pct = long_pauses / total_events * 100
                logger.info(f"\n‚ö†Ô∏è  –°–æ–±—ã—Ç–∏–π —Å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é >30 –º–∏–Ω—É—Ç: {long_pauses:,} ({long_pauses_pct:.2f}%)")
                logger.info(f"    (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–æ–ª–≥–æ –æ—Å—Ç–∞–≤–∞–ª—Å—è –Ω–∞ —ç–∫—Ä–∞–Ω–µ –∏–ª–∏ –≤—ã—à–µ–ª –∏–∑ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è)")

                # –ü—Ä–∏–º–µ—Ä—ã –¥–æ–ª–≥–∏—Ö –ø–∞—É–∑
                long_pause_examples = self.merged_df[self.merged_df['duration_seconds'] > 1800].nlargest(5,
                                                                                                         'duration_seconds')
                logger.info(f"\n    –¢–æ–ø-5 —Å–∞–º—ã—Ö –¥–æ–ª–≥–∏—Ö –∑–∞–¥–µ—Ä–∂–µ–∫ –Ω–∞ —ç–∫—Ä–∞–Ω–∞—Ö:")
                for idx, row in long_pause_examples.iterrows():
                    duration_hours = row['duration_seconds'] / 3600
                    logger.info(
                        f"      {duration_hours:.2f} —á - –≠–∫—Ä–∞–Ω: {row.get('–≠–∫—Ä–∞–Ω', 'N/A')}, –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª: {row.get('–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª', 'N/A')}")
        else:
            logger.warning("–ù–µ—Ç —Å–æ–±—ã—Ç–∏–π —Å duration_seconds > 0")

        logger.info(f"{'=' * 60}\n")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats['event_duration'] = {
            'total_events': int(total_events),
            'last_events': int(last_events),
            'non_last_events': int(non_last_events),
            'avg_duration_sec': float(non_zero_durations.mean()) if len(non_zero_durations) > 0 else 0,
            'median_duration_sec': float(non_zero_durations.median()) if len(non_zero_durations) > 0 else 0,
            'max_duration_sec': int(non_zero_durations.max()) if len(non_zero_durations) > 0 else 0,
            'long_pauses_count': int(long_pauses) if 'long_pauses' in locals() else 0
        }

        logger.info("‚úì –†–∞—Å—á—ë—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ")

        return self.merged_df

    def remove_consecutive_duplicates_with_clicks(self) -> pd.DataFrame:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ —Ä–∞–º–∫–∞—Ö —Å–µ—Å—Å–∏–∏ —Å –ø–æ–¥—Å—á—ë—Ç–æ–º –∫–ª–∏–∫–æ–≤

        –õ–æ–≥–∏–∫–∞:
        - –í —Ä–∞–º–∫–∞—Ö –∫–∞–∂–¥–æ–π global_session_id —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ: –≠–∫—Ä–∞–Ω, –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª, –î–µ–π—Å—Ç–≤–∏–µ
        - –ï—Å–ª–∏ –ø–æ–¥—Ä—è–¥ –∏–¥—É—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ –ø–æ —ç—Ç–∏–º 3 –ø–æ–ª—è–º, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω—é—é
        - –í –∫–æ–ª–æ–Ω–∫—É click_count –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–∫–æ–≤, –≥–¥–µ –î–µ–π—Å—Ç–≤–∏–µ != "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
        - –í –∫–æ–ª–æ–Ω–∫—É duration_seconds –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –°–£–ú–ú–£ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π –≤ –≥—Ä—É–ø–ø–µ
        - –í –∫–æ–ª–æ–Ω–∫—É dbl_duration_seconds –∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Å—É–º–º—É duration_seconds –∏–∑ –£–î–ê–õ–Å–ù–ù–´–• –∑–∞–ø–∏—Å–µ–π
        - –í –∫–æ–ª–æ–Ω–∫—É dbl_count –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –£–î–ê–õ–Å–ù–ù–´–• –¥—É–±–ª–µ–π

        –ü—Ä–∏–º–µ—Ä:
        –ë—ã–ª–æ:
        | time  | –≠–∫—Ä–∞–Ω | –î–µ–π—Å—Ç–≤–∏–µ     | click_count | duration_seconds | dbl_duration | dbl_count |
        |-------|-------|--------------|-----------|------------------|--------------|-----------|
        | 10:00 | –ï—â–µ   | –ù–µ —É–∫–∞–∑–∞–Ω–æ   | 0         | 3                | 0            | 0         |
        | 10:03 | –ï—â–µ   | –ù–µ —É–∫–∞–∑–∞–Ω–æ   | 0         | 2                | 0            | 0         |
        | 10:05 | –ï—â–µ   | –¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É| 0         | 5                | 0            | 0         |
        | 10:10 | –ï—â–µ   | –¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É| 0         | 5                | 0            | 0         |

        –°—Ç–∞–ª–æ:
        | time  | –≠–∫—Ä–∞–Ω | –î–µ–π—Å—Ç–≤–∏–µ     | click_count | duration_seconds | dbl_duration | dbl_count |
        |-------|-------|--------------|-----------|------------------|--------------|-----------|
        | 10:10 | –ï—â–µ   | –¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É| 2         | 15               | 10           | 3         |

        –†–∞—Å—á—ë—Ç:
        - click_count = 2 (—Ç–æ–ª—å–∫–æ —Ç–µ, –≥–¥–µ –î–µ–π—Å—Ç–≤–∏–µ != "–ù–µ —É–∫–∞–∑–∞–Ω–æ")
        - duration_seconds = 3 + 2 + 5 + 5 = 15 (—Å—É–º–º–∞ –≤—Å–µ—Ö)
        - dbl_duration_seconds = 3 + 2 + 5 = 10 (—Å—É–º–º–∞ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö)
        - dbl_count = 3 (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö)

        Returns:
            DataFrame —Å —É–¥–∞–ª—ë–Ω–Ω—ã–º–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏ –∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
        """
        logger.info("–£–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –ø–æ–¥—Å—á—ë—Ç–æ–º –∫–ª–∏–∫–æ–≤...")

        if self.merged_df is None or len(self.merged_df) == 0:
            raise ValueError("merged_df –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

        if 'global_session_id' not in self.merged_df.columns:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ add_global_session_id()")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
        initial_rows = len(self.merged_df)
        logger.info(f"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {initial_rows:,}")

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        self.merged_df['click_count'] = 0
        self.merged_df['dbl_duration_seconds'] = 0  # ‚Üê –î–û–ë–ê–í–õ–ï–ù–û
        self.merged_df['dbl_count'] = 0  # ‚Üê –î–û–ë–ê–í–õ–ï–ù–û

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è duration_seconds
        if 'duration_seconds' not in self.merged_df.columns:
            logger.warning("–ö–æ–ª–æ–Ω–∫–∞ 'duration_seconds' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º 0")
            self.merged_df['duration_seconds'] = 0

        # –í–ê–ñ–ù–û: –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ 3 –∫–æ–ª–æ–Ω–∫–∞–º
        comparison_cols = ['–≠–∫—Ä–∞–Ω', '–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª', '–î–µ–π—Å—Ç–≤–∏–µ']
        time_col = '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'
        action_col = '–î–µ–π—Å—Ç–≤–∏–µ'

        logger.info(f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º: {comparison_cols}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        for col in comparison_cols:
            if col not in self.merged_df.columns:
                raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")

        # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã
        self.merged_df = self.merged_df.sort_values(
            by=['global_session_id', time_col],
            ascending=[True, True]
        ).reset_index(drop=True)

        logger.info("–î–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ global_session_id –∏ –≤—Ä–µ–º–µ–Ω–∏")

        # ============================================================
        # –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–´–ô –ü–û–î–•–û–î
        # ============================================================

        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")

        # –°–æ–∑–¥–∞—ë–º —Å–æ—Å—Ç–∞–≤–Ω–æ–π –∫–ª—é—á –∏–∑ 3 –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        key_parts = []
        for col in comparison_cols:
            col_str = self.merged_df[col].astype(str).fillna('__NA__')
            key_parts.append(col_str)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–∏–Ω –∫–ª—é—á
        self.merged_df['comparison_key'] = (
                key_parts[0] + '|' + key_parts[1] + '|' + key_parts[2]
        )

        logger.info("–ü–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")

        # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–ø–∏—Å–∏ –í –¢–û–ô –ñ–ï –°–ï–°–°–ò–ò
        self.merged_df['prev_key'] = self.merged_df.groupby(
            'global_session_id',
            sort=False
        )['comparison_key'].shift(1)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—É—â–∞—è –∑–∞–ø–∏—Å—å –¥—É–±–ª–∏–∫–∞—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–π
        self.merged_df['is_duplicate'] = (
                self.merged_df['comparison_key'] == self.merged_df['prev_key']
        ).fillna(False)

        duplicates_count = self.merged_df['is_duplicate'].sum()
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_count:,}")

        # ============================================================
        # –°–û–ó–î–ê–ù–ò–ï –ì–†–£–ü–ü –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–´–• –î–£–ë–õ–ò–ö–ê–¢–û–í
        # ============================================================

        logger.info("–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")

        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –≥—Ä—É–ø–ø—É –∫–æ–≥–¥–∞ –º–µ–Ω—è–µ—Ç—Å—è —Å–µ—Å—Å–∏—è –ò–õ–ò —Ç–µ–∫—É—â–∞—è –∑–∞–ø–∏—Å—å –ù–ï –¥—É–±–ª–∏–∫–∞—Ç
        self.merged_df['new_group'] = (
                (~self.merged_df['is_duplicate']) |
                (self.merged_df['global_session_id'] != self.merged_df['global_session_id'].shift(1))
        )

        # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º ID –≥—Ä—É–ø–ø–∞–º
        self.merged_df['group_id'] = self.merged_df['new_group'].cumsum()

        # ============================================================
        # –ü–û–î–°–ß–Å–¢ –ú–ï–¢–†–ò–ö –î–õ–Ø –ö–ê–ñ–î–û–ô –ì–†–£–ü–ü–´
        # ============================================================

        logger.info("–ü–æ–¥—Å—á—ë—Ç –∫–ª–∏–∫–æ–≤ –∏ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")

        # –°–æ–∑–¥–∞—ë–º —Ñ–ª–∞–≥: —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –î–µ–π—Å—Ç–≤–∏–µ –∑–Ω–∞—á–∏–º—ã–º (–Ω–µ "–ù–µ —É–∫–∞–∑–∞–Ω–æ")
        self.merged_df['is_meaningful_action'] = (
                self.merged_df[action_col] != '–ù–µ —É–∫–∞–∑–∞–Ω–æ'
        )

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –≥—Ä—É–ø–ø–∞–º
        group_stats = self.merged_df.groupby('group_id').agg({
            'is_duplicate': 'first',
            'global_session_id': 'count',  # –†–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã
            'is_meaningful_action': 'sum',  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞—á–∏–º—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
            'duration_seconds': 'sum'  # –°—É–º–º–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        }).rename(columns={
            'global_session_id': 'group_size',
            'is_meaningful_action': 'meaningful_actions_count',
            'duration_seconds': 'total_duration'
        })

        # –ú–∞–ø–ø–∏–Ω–≥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π DataFrame
        self.merged_df['group_size'] = self.merged_df['group_id'].map(group_stats['group_size'])
        self.merged_df['meaningful_actions_count'] = self.merged_df['group_id'].map(
            group_stats['meaningful_actions_count'])
        self.merged_df['total_duration'] = self.merged_df['group_id'].map(group_stats['total_duration'])

        # –ü–æ–º–µ—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ
        self.merged_df['rank_in_group'] = self.merged_df.groupby('group_id').cumcount(ascending=False)
        self.merged_df['is_last_in_group'] = (self.merged_df['rank_in_group'] == 0)

        # ============================================================
        # –ó–ê–ü–û–õ–ù–ï–ù–ò–ï –ü–û–õ–ï–ô –î–õ–Ø –ü–û–°–õ–ï–î–ù–ï–ô –ó–ê–ü–ò–°–ò –í –ì–†–£–ü–ü–ï
        # ============================================================

        logger.info("–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ click_count, duration_seconds, dbl_duration_seconds, dbl_count...")

        last_in_group_mask = self.merged_df['is_last_in_group']

        # click_count = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞—á–∏–º—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –≤ –≥—Ä—É–ø–ø–µ (–º–∏–Ω–∏–º—É–º 1)
        self.merged_df.loc[last_in_group_mask, 'click_count'] = self.merged_df.loc[
            last_in_group_mask,
            'meaningful_actions_count'
        ].clip(lower=1)

        # duration_seconds = —Å—É–º–º–∞ –≤—Å–µ—Ö –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ –≥—Ä—É–ø–ø–µ
        self.merged_df.loc[last_in_group_mask, 'duration_seconds'] = self.merged_df.loc[
            last_in_group_mask,
            'total_duration'
        ]

        # ‚Üê –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: dbl_duration_seconds –∏ dbl_count
        # –î–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –≤—ã—á–∏—Å–ª—è–µ–º —Å—É–º–º—É duration_seconds —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        # –£–¥–∞–ª—ë–Ω–Ω—ã–µ = –≤—Å–µ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π = group_size - 1

        # –°–æ–∑–¥–∞—ë–º –º–∞—Å–∫—É –¥–ª—è —É–¥–∞–ª—è–µ–º—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ
        self.merged_df['is_removed'] = ~self.merged_df['is_last_in_group']

        # –î–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã —Å—É–º–º–∏—Ä—É–µ–º duration_seconds —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        removed_duration_by_group = self.merged_df[self.merged_df['is_removed']].groupby('group_id')[
            'duration_seconds'].sum()

        # –î–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–ø–∏—Å–∏ –≤ –≥—Ä—É–ø–ø–µ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º —ç—Ç—É —Å—É–º–º—É
        self.merged_df.loc[last_in_group_mask, 'dbl_duration_seconds'] = self.merged_df.loc[
            last_in_group_mask,
            'group_id'
        ].map(removed_duration_by_group).fillna(0).astype('int32')

        # dbl_count = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö = group_size - 1
        self.merged_df.loc[last_in_group_mask, 'dbl_count'] = (
                self.merged_df.loc[last_in_group_mask, 'group_size'] - 1
        )

        # ============================================================
        # –£–î–ê–õ–ï–ù–ò–ï –î–£–ë–õ–ò–ö–ê–¢–û–í
        # ============================================================

        rows_to_keep = self.merged_df['is_last_in_group']
        rows_to_remove_count = (~rows_to_keep).sum()

        logger.info(f"–ó–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {rows_to_remove_count:,}")

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        self.merged_df = self.merged_df[rows_to_keep].copy()

        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        temp_cols = [
            'comparison_key', 'prev_key', 'is_duplicate',
            'new_group', 'group_id', 'group_size',
            'rank_in_group', 'is_last_in_group',
            'is_meaningful_action', 'meaningful_actions_count', 'total_duration',
            'is_removed'
        ]
        self.merged_df = self.merged_df.drop(columns=temp_cols)

        # –°–±—Ä–æ—Å –∏–Ω–¥–µ–∫—Å–∞
        self.merged_df = self.merged_df.reset_index(drop=True)

        # ============================================================
        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        # ============================================================

        final_rows = len(self.merged_df)
        removed_rows = initial_rows - final_rows
        removal_percentage = (removed_rows / initial_rows * 100) if initial_rows > 0 else 0

        logger.info(f"\n{'=' * 60}")
        logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–ò:")
        logger.info(f"{'=' * 60}")
        logger.info(f"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {initial_rows:,}")
        logger.info(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed_rows:,} ({removal_percentage:.2f}%)")
        logger.info(f"–û—Å—Ç–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–µ–π: {final_rows:,}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –Ω–æ–≤—ã–º –ø–æ–ª—è–º
        logger.info(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ dbl_count (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –¥—É–±–ª–µ–π):")
        logger.info(f"  –°—Ä–µ–¥–Ω–µ–µ: {self.merged_df['dbl_count'].mean():.2f}")
        logger.info(f"  –ú–µ–¥–∏–∞–Ω–∞: {self.merged_df['dbl_count'].median():.0f}")
        logger.info(f"  –ú–∞–∫—Å: {self.merged_df['dbl_count'].max()}")

        logger.info(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ dbl_duration_seconds (–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–¥–∞–ª—ë–Ω–Ω—ã—Ö):")
        logger.info(f"  –°—Ä–µ–¥–Ω–µ–µ: {self.merged_df['dbl_duration_seconds'].mean():.2f} —Å–µ–∫")
        logger.info(f"  –ú–µ–¥–∏–∞–Ω–∞: {self.merged_df['dbl_duration_seconds'].median():.0f} —Å–µ–∫")
        logger.info(f"  –ú–∞–∫—Å: {self.merged_df['dbl_duration_seconds'].max():,} —Å–µ–∫")
        logger.info(
            f"  –°—É–º–º–∞: {self.merged_df['dbl_duration_seconds'].sum():,} —Å–µ–∫ ({self.merged_df['dbl_duration_seconds'].sum() / 3600:.2f} —á)")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ dbl_count
        dbl_count_dist = self.merged_df['dbl_count'].value_counts().sort_index()
        logger.info(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ dbl_count:")
        for count, freq in dbl_count_dist.head(10).items():
            percentage = freq / final_rows * 100
            logger.info(f"  {count} —É–¥–∞–ª—ë–Ω–Ω—ã—Ö: {freq:,} ({percentage:.2f}%)")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ click_count
        click_count_dist = self.merged_df['click_count'].value_counts().sort_index()
        logger.info(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ click_count:")
        for tries, count in click_count_dist.head(15).items():
            percentage = count / final_rows * 100
            logger.info(f"  {tries} {'–∫–ª–∏–∫' if tries == 1 else '–∫–ª–∏–∫–æ–≤'}: {count:,} ({percentage:.2f}%)")

        if len(click_count_dist) > 15:
            logger.info(f"  ... (–≤—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {len(click_count_dist)})")

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–∫–æ–≤
        max_tries = self.merged_df['click_count'].max()
        logger.info(f"\n–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–∫–æ–≤: {max_tries}")

        # –°—Ä–µ–¥–Ω–∏–π click_count
        avg_tries = self.merged_df['click_count'].mean()
        logger.info(f"–°—Ä–µ–¥–Ω–∏–π click_count: {avg_tries:.2f}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ duration_seconds
        logger.info(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ duration_seconds:")
        logger.info(f"  –°—Ä–µ–¥–Ω–µ–µ: {self.merged_df['duration_seconds'].mean():.2f} —Å–µ–∫")
        logger.info(f"  –ú–µ–¥–∏–∞–Ω–∞: {self.merged_df['duration_seconds'].median():.0f} —Å–µ–∫")
        logger.info(f"  –ú–∞–∫—Å: {self.merged_df['duration_seconds'].max():,} —Å–µ–∫")

        logger.info(f"{'=' * 60}\n")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats['deduplication'] = {
            'initial_rows': int(initial_rows),
            'removed_rows': int(removed_rows),
            'final_rows': int(final_rows),
            'removal_percentage': float(removal_percentage),
            'max_tries': int(max_tries),
            'avg_tries': float(avg_tries),
            'avg_dbl_count': float(self.merged_df['dbl_count'].mean()),
            'avg_dbl_duration': float(self.merged_df['dbl_duration_seconds'].mean()),
            'click_count_distribution': {int(k): int(v) for k, v in click_count_dist.head(20).items()}
        }

        logger.info("‚úì –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

        return self.merged_df

    def collapse_intermediate_screens(self) -> pd.DataFrame:
        """
        –°—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π —Å –î–µ–π—Å—Ç–≤–∏–µ="–ù–µ —É–∫–∞–∑–∞–Ω–æ" –Ω–∞ –æ–¥–Ω–æ–º —ç–∫—Ä–∞–Ω–µ

        –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê:
        - –í —Ä–∞–º–∫–∞—Ö –∫–∞–∂–¥–æ–π global_session_id –∏—â–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–µ–π —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –≠–∫—Ä–∞–Ω
        - –í–Ω—É—Ç—Ä–∏ —Ç–∞–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ –≥–¥–µ –î–µ–π—Å—Ç–≤–∏–µ != "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
        - –í—Å–µ –∑–∞–ø–∏—Å–∏ —Å –î–µ–π—Å—Ç–≤–∏–µ="–ù–µ —É–∫–∞–∑–∞–Ω–æ" —É–¥–∞–ª—è–µ–º
        - –°—É–º–º—É duration_seconds –∏–∑ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–æ–±–∞–≤–ª—è–µ–º –∫ –ø–µ—Ä–≤–æ–π –æ—Å—Ç–∞–≤—à–µ–π—Å—è –∑–∞–ø–∏—Å–∏

        –ü—Ä–∏–º–µ—Ä:
        –ë—ã–ª–æ:
        | time  | –≠–∫—Ä–∞–Ω        | –î–µ–π—Å—Ç–≤–∏–µ         | duration_seconds |
        |-------|--------------|------------------|------------------|
        | 10:00 | –ù–æ–≤–∞—è –∑–∞—è–≤–∫–∞ | –ù–µ —É–∫–∞–∑–∞–Ω–æ       | 5                |
        | 10:05 | –ù–æ–≤–∞—è –∑–∞—è–≤–∫–∞ | –ù–µ —É–∫–∞–∑–∞–Ω–æ       | 3                |
        | 10:08 | –ù–æ–≤–∞—è –∑–∞—è–≤–∫–∞ | –¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É    | 10               |
        | 10:18 | –ù–æ–≤–∞—è –∑–∞—è–≤–∫–∞ | –í—ã–±–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–∏  | 5                |
        | 10:23 | –ó–∞—è–≤–∫–∏       | –ù–µ —É–∫–∞–∑–∞–Ω–æ       | 0                |

        –°—Ç–∞–ª–æ:
        | time  | –≠–∫—Ä–∞–Ω        | –î–µ–π—Å—Ç–≤–∏–µ         | duration_seconds |
        |-------|--------------|------------------|------------------|
        | 10:08 | –ù–æ–≤–∞—è –∑–∞—è–≤–∫–∞ | –¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É    | 18 (10+5+3)      |
        | 10:18 | –ù–æ–≤–∞—è –∑–∞—è–≤–∫–∞ | –í—ã–±–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–∏  | 5                |
        | 10:23 | –ó–∞—è–≤–∫–∏       | –ù–µ —É–∫–∞–∑–∞–Ω–æ       | 0                |

        Returns:
            DataFrame —Å —É–¥–∞–ª—ë–Ω–Ω—ã–º–∏ "–ù–µ —É–∫–∞–∑–∞–Ω–æ" –∏ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–º duration_seconds
        """
        logger.info("–°—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π —Å –î–µ–π—Å—Ç–≤–∏–µ='–ù–µ —É–∫–∞–∑–∞–Ω–æ' –Ω–∞ –æ–¥–Ω–æ–º —ç–∫—Ä–∞–Ω–µ...")

        if self.merged_df is None or len(self.merged_df) == 0:
            raise ValueError("merged_df –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

        if 'global_session_id' not in self.merged_df.columns:
            raise ValueError("–¢—Ä–µ–±—É–µ—Ç—Å—è global_session_id")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è duration_seconds
        if 'duration_seconds' not in self.merged_df.columns:
            logger.warning("–ö–æ–ª–æ–Ω–∫–∞ 'duration_seconds' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º 0")
            self.merged_df['duration_seconds'] = 0

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
        initial_rows = len(self.merged_df)
        logger.info(f"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {initial_rows:,}")

        time_col = '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'
        screen_col = '–≠–∫—Ä–∞–Ω'
        action_col = '–î–µ–π—Å—Ç–≤–∏–µ'
        not_specified = '–ù–µ —É–∫–∞–∑–∞–Ω–æ'

        # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã
        self.merged_df = self.merged_df.sort_values(
            by=['global_session_id', time_col],
            ascending=[True, True]
        ).reset_index(drop=True)

        logger.info(f"–£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π —Å –î–µ–π—Å—Ç–≤–∏–µ='{not_specified}' –Ω–∞ –æ–¥–Ω–æ–º —ç–∫—Ä–∞–Ω–µ...")

        # ============================================================
        # –ì–†–£–ü–ü–ò–†–û–í–ö–ê –ü–û –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–Ø–ú –û–î–ù–û–ì–û –≠–ö–†–ê–ù–ê
        # ============================================================

        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –≥—Ä—É–ø–ø—É –∫–æ–≥–¥–∞ –º–µ–Ω—è–µ—Ç—Å—è —ç–∫—Ä–∞–Ω –∏–ª–∏ —Å–µ—Å—Å–∏—è
        self.merged_df['prev_screen'] = self.merged_df.groupby(
            'global_session_id',
            sort=False
        )[screen_col].shift(1)

        self.merged_df['screen_changed'] = (
                (self.merged_df[screen_col] != self.merged_df['prev_screen']) |
                (self.merged_df['global_session_id'] != self.merged_df['global_session_id'].shift(1))
        )

        self.merged_df['screen_group_id'] = self.merged_df['screen_changed'].cumsum()

        # ============================================================
        # –û–ë–†–ê–ë–û–¢–ö–ê –ö–ê–ñ–î–û–ô –ì–†–£–ü–ü–´ –≠–ö–†–ê–ù–ê
        # ============================================================

        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –æ–¥–Ω–æ–≥–æ —ç–∫—Ä–∞–Ω–∞...")

        # –ú–∞—Ä–∫–∏—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ
        self.merged_df['to_remove'] = False

        # –î–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã —ç–∫—Ä–∞–Ω–∞
        for group_id, group_df in self.merged_df.groupby('screen_group_id', sort=False):
            group_indices = group_df.index.tolist()

            if len(group_indices) < 2:
                # –¢–æ–ª—å–∫–æ –æ–¥–Ω–∞ –∑–∞–ø–∏—Å—å - –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
                continue

            # –ù–∞—Ö–æ–¥–∏–º –∑–∞–ø–∏—Å–∏ —Å "–ù–µ —É–∫–∞–∑–∞–Ω–æ" –∏ –±–µ–∑
            not_specified_mask = group_df[action_col] == not_specified
            not_specified_indices = group_df[not_specified_mask].index.tolist()
            meaningful_indices = group_df[~not_specified_mask].index.tolist()

            if len(not_specified_indices) == 0:
                # –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π —Å "–ù–µ —É–∫–∞–∑–∞–Ω–æ" - –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
                continue

            if len(meaningful_indices) == 0:
                # –í—Å–µ –∑–∞–ø–∏—Å–∏ —Å "–ù–µ —É–∫–∞–∑–∞–Ω–æ" - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                continue

            # ============================================================
            # –õ–û–ì–ò–ö–ê –°–•–õ–û–ü–´–í–ê–ù–ò–Ø
            # ============================================================

            # –°—É–º–º–∏—Ä—É–µ–º duration_seconds –∏–∑ –∑–∞–ø–∏—Å–µ–π —Å "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
            sum_not_specified_duration = self.merged_df.loc[not_specified_indices, 'duration_seconds'].sum()

            # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—É—é –∑–Ω–∞—á–∏–º—É—é –∑–∞–ø–∏—Å—å (–≥–¥–µ –î–µ–π—Å—Ç–≤–∏–µ != "–ù–µ —É–∫–∞–∑–∞–Ω–æ")
            first_meaningful_idx = meaningful_indices[0]

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –ø–µ—Ä–≤–æ–π –∑–Ω–∞—á–∏–º–æ–π –∑–∞–ø–∏—Å–∏
            self.merged_df.loc[first_meaningful_idx, 'duration_seconds'] += sum_not_specified_duration

            # –ü–æ–º–µ—á–∞–µ–º –∑–∞–ø–∏—Å–∏ —Å "–ù–µ —É–∫–∞–∑–∞–Ω–æ" –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ
            self.merged_df.loc[not_specified_indices, 'to_remove'] = True

        # ============================================================
        # –£–î–ê–õ–ï–ù–ò–ï –ó–ê–ü–ò–°–ï–ô
        # ============================================================

        rows_to_remove_count = self.merged_df['to_remove'].sum()
        logger.info(f"–ó–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (–î–µ–π—Å—Ç–≤–∏–µ='{not_specified}'): {rows_to_remove_count:,}")

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        self.merged_df = self.merged_df[~self.merged_df['to_remove']].copy()

        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        self.merged_df = self.merged_df.drop(columns=['prev_screen', 'screen_changed', 'screen_group_id', 'to_remove'])

        # –°–±—Ä–æ—Å –∏–Ω–¥–µ–∫—Å–∞
        self.merged_df = self.merged_df.reset_index(drop=True)

        # ============================================================
        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        # ============================================================

        final_rows = len(self.merged_df)
        removed_rows = initial_rows - final_rows
        removal_percentage = (removed_rows / initial_rows * 100) if initial_rows > 0 else 0

        logger.info(f"\n{'=' * 60}")
        logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–•–õ–û–ü–´–í–ê–ù–ò–Ø:")
        logger.info(f"{'=' * 60}")
        logger.info(f"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {initial_rows:,}")
        logger.info(f"–£–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π —Å –î–µ–π—Å—Ç–≤–∏–µ='{not_specified}': {removed_rows:,} ({removal_percentage:.2f}%)")
        logger.info(f"–û—Å—Ç–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–µ–π: {final_rows:,}")

        # –°–∫–æ–ª—å–∫–æ "–ù–µ —É–∫–∞–∑–∞–Ω–æ" –æ—Å—Ç–∞–ª–æ—Å—å
        remaining_not_specified = (self.merged_df[action_col] == not_specified).sum()
        remaining_not_specified_pct = (remaining_not_specified / final_rows * 100) if final_rows > 0 else 0
        logger.info(
            f"\n–û—Å—Ç–∞–≤—à–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π —Å –î–µ–π—Å—Ç–≤–∏–µ='{not_specified}': {remaining_not_specified:,} ({remaining_not_specified_pct:.2f}%)")
        logger.info(f"(—ç—Ç–æ –∑–∞–ø–∏—Å–∏, –≥–¥–µ –Ω–∞ —ç–∫—Ä–∞–Ω–µ –Ω–µ –±—ã–ª–æ –¥—Ä—É–≥–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π)")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ duration_seconds
        logger.info(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ duration_seconds –ø–æ—Å–ª–µ —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏—è:")
        logger.info(f"  –°—Ä–µ–¥–Ω–µ–µ: {self.merged_df['duration_seconds'].mean():.2f} —Å–µ–∫")
        logger.info(f"  –ú–µ–¥–∏–∞–Ω–∞: {self.merged_df['duration_seconds'].median():.0f} —Å–µ–∫")
        logger.info(
            f"  –ú–∞–∫—Å: {self.merged_df['duration_seconds'].max():,} —Å–µ–∫ ({self.merged_df['duration_seconds'].max() / 3600:.2f} —á)")
        logger.info(
            f"  –°—É–º–º–∞: {self.merged_df['duration_seconds'].sum():,} —Å–µ–∫ ({self.merged_df['duration_seconds'].sum() / 3600:.2f} —á)")

        # –¢–æ–ø-10 —ç–∫—Ä–∞–Ω–æ–≤ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É duration_seconds
        avg_duration_by_screen = self.merged_df.groupby(screen_col, observed=True)['duration_seconds'].mean().sort_values(
            ascending=False)
        logger.info(f"\n–¢–æ–ø-10 —ç–∫—Ä–∞–Ω–æ–≤ –ø–æ —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
        for screen, avg_dur in avg_duration_by_screen.head(10).items():
            count = (self.merged_df[screen_col] == screen).sum()
            logger.info(f"  {screen}: {avg_dur:.1f} —Å–µ–∫ (–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è {count:,} —Ä–∞–∑)")

        logger.info(f"{'=' * 60}\n")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats['collapse_screens'] = {
            'initial_rows': int(initial_rows),
            'removed_rows': int(removed_rows),
            'final_rows': int(final_rows),
            'removal_percentage': float(removal_percentage),
            'remaining_not_specified': int(remaining_not_specified),
            'avg_duration_sec': float(self.merged_df['duration_seconds'].mean())
        }

        logger.info("‚úì –°—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")

        return self.merged_df

    def save_unique_values_json(self, output_path: str = None) -> Dict:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ –≠–∫—Ä–∞–Ω, –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª, –î–µ–π—Å—Ç–≤–∏–µ –≤ JSON

        –°–æ–∑–¥–∞—ë—Ç JSON —Ñ–∞–π–ª —Å 3 –º–∞—Å—Å–∏–≤–∞–º–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π,
        –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–æ—Ç —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫ —Ä–µ–¥–∫–∏–º).

        Args:
            output_path: –ø—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É JSON —Ñ–∞–π–ª—É
                        (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è reports/json/unique_values.json)

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏ –∏—Ö —á–∞—Å—Ç–æ—Ç–∞–º–∏
        """
        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ JSON...")

        if self.merged_df is None or len(self.merged_df) == 0:
            raise ValueError("merged_df –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        if output_path is None:
            json_path = Path(self.config['reports']['json_path'])
            json_path.mkdir(parents=True, exist_ok=True)
            output_path = json_path / 'unique_values.json'
        else:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

        # –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        target_columns = ['–≠–∫—Ä–∞–Ω', '–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª', '–î–µ–π—Å—Ç–≤–∏–µ']

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–æ–Ω–æ–∫
        for col in target_columns:
            if col not in self.merged_df.columns:
                raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")

        logger.info(f"–ê–Ω–∞–ª–∏–∑ –∫–æ–ª–æ–Ω–æ–∫: {', '.join(target_columns)}")

        # ============================================================
        # –°–ë–û–† –£–ù–ò–ö–ê–õ–¨–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô –° –ß–ê–°–¢–û–¢–ê–ú–ò
        # ============================================================

        result = {
            'metadata': {
                'generated_at': pd.Timestamp.now().isoformat(),
                'total_records': int(len(self.merged_df)),
                'date_range': {
                    'min': str(self.merged_df[
                                   '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].min()) if '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' in self.merged_df.columns else None,
                    'max': str(self.merged_df[
                                   '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].max()) if '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' in self.merged_df.columns else None
                }
            },
            'unique_values': {}
        }

        for col in target_columns:
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–ª–æ–Ω–∫–∏ '{col}'...")

            # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç
            value_counts = self.merged_df[col].value_counts()

            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å —á–∞—Å—Ç–æ—Ç–∞–º–∏
            values_with_frequency = []
            for value, count in value_counts.items():
                values_with_frequency.append({
                    'value': str(value),
                    'count': int(count),
                    'percentage': round(count / len(self.merged_df) * 100, 2)
                })

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_unique = len(values_with_frequency)
            logger.info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {total_unique}")
            logger.info(f"  –¢–æ–ø-3: {', '.join([v['value'] for v in values_with_frequency[:3]])}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result['unique_values'][col] = {
                'total_unique': total_unique,
                'values': values_with_frequency
            }

        # ============================================================
        # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        # ============================================================

        # –¢–æ–ø-10 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π (–≠–∫—Ä–∞–Ω + –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª + –î–µ–π—Å—Ç–≤–∏–µ)
        logger.info("–ü–æ–¥—Å—á—ë—Ç —Ç–æ–ø-10 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π...")

        combinations = self.merged_df.groupby(target_columns).size().reset_index(name='count')
        combinations = combinations.sort_values('count', ascending=False).head(10)

        top_combinations = []
        for _, row in combinations.iterrows():
            top_combinations.append({
                '–≠–∫—Ä–∞–Ω': str(row['–≠–∫—Ä–∞–Ω']),
                '–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª': str(row['–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª']),
                '–î–µ–π—Å—Ç–≤–∏–µ': str(row['–î–µ–π—Å—Ç–≤–∏–µ']),
                'count': int(row['count']),
                'percentage': round(row['count'] / len(self.merged_df) * 100, 2)
            })

        result['top_combinations'] = top_combinations

        # ============================================================
        # –°–û–•–†–ê–ù–ï–ù–ò–ï –í JSON
        # ============================================================

        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª: {output_path}")

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        file_size = output_path.stat().st_size / 1024
        logger.info(f"‚úì –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path} ({file_size:.2f} KB)")

        # ============================================================
        # –í–´–í–û–î –°–¢–ê–¢–ò–°–¢–ò–ö–ò –í –õ–û–ì
        # ============================================================

        logger.info(f"\n{'=' * 60}")
        logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –£–ù–ò–ö–ê–õ–¨–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô:")
        logger.info(f"{'=' * 60}")

        for col in target_columns:
            col_data = result['unique_values'][col]
            logger.info(f"\n{col}:")
            logger.info(f"  –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {col_data['total_unique']}")
            logger.info(f"  –¢–æ–ø-5 –ø–æ —á–∞—Å—Ç–æ—Ç–µ:")
            for i, item in enumerate(col_data['values'][:5], 1):
                logger.info(f"    {i}. {item['value']}: {item['count']:,} ({item['percentage']:.1f}%)")

        logger.info(f"\n–¢–æ–ø-3 –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ (–≠–∫—Ä–∞–Ω + –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª + –î–µ–π—Å—Ç–≤–∏–µ):")
        for i, combo in enumerate(top_combinations[:3], 1):
            logger.info(f"  {i}. {combo['–≠–∫—Ä–∞–Ω']} | {combo['–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª']} | {combo['–î–µ–π—Å—Ç–≤–∏–µ']}")
            logger.info(f"     {combo['count']:,} —Ä–∞–∑ ({combo['percentage']:.1f}%)")

        logger.info(f"{'=' * 60}\n")

        logger.info("‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")

        return result

    def save_unique_combinations_json(self, output_path: str = None) -> Dict:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –≠–∫—Ä–∞–Ω => –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª => –î–µ–π—Å—Ç–≤–∏–µ –≤ JSON

        –°–æ–∑–¥–∞—ë—Ç JSON —Ñ–∞–π–ª —Å –º–∞—Å—Å–∏–≤–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –∑–Ω–∞—á–µ–Ω–∏–π,
        –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–æ—Ç —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫ —Ä–µ–¥–∫–∏–º).
        –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–∞–∫ –µ–¥–∏–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º " => ".

        Args:
            output_path: –ø—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É JSON —Ñ–∞–π–ª—É
                        (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è reports/json/unique_combinations.json)

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º–∏ –∏ –∏—Ö —á–∞—Å—Ç–æ—Ç–∞–º–∏
        """
        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –≤ JSON...")

        if self.merged_df is None or len(self.merged_df) == 0:
            raise ValueError("merged_df –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        if output_path is None:
            json_path = Path(self.config['reports']['json_path'])
            json_path.mkdir(parents=True, exist_ok=True)
            output_path = json_path / 'unique_combinations.json'
        else:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

        # –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        target_columns = ['–≠–∫—Ä–∞–Ω', '–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª', '–î–µ–π—Å—Ç–≤–∏–µ']
        separator = ' => '

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–æ–Ω–æ–∫
        for col in target_columns:
            if col not in self.merged_df.columns:
                raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")

        logger.info(f"–ê–Ω–∞–ª–∏–∑ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: {separator.join(target_columns)}")

        # ============================================================
        # –°–û–ó–î–ê–ù–ò–ï –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–´–• –°–¢–†–û–ö
        # ============================================================

        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫...")

        # –°–æ–∑–¥–∞—ë–º –µ–¥–∏–Ω—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        self.merged_df['_combination'] = (
                self.merged_df['–≠–∫—Ä–∞–Ω'].astype(str) + separator +
                self.merged_df['–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª'].astype(str) + separator +
                self.merged_df['–î–µ–π—Å—Ç–≤–∏–µ'].astype(str)
        )

        # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç
        combination_counts = self.merged_df['_combination'].value_counts()

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: {len(combination_counts):,}")

        # ============================================================
        # –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–ê
        # ============================================================

        total_records = len(self.merged_df)

        combinations_list = []
        for combination, count in combination_counts.items():
            combinations_list.append({
                'path': str(combination),
                'count': int(count),
                'percentage': round(count / total_records * 100, 4)
            })

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        result = {
            'metadata': {
                'generated_at': pd.Timestamp.now().isoformat(),
                'total_records': int(total_records),
                'total_unique_combinations': len(combinations_list),
                'separator': separator,
                'columns': target_columns,
                'date_range': {
                    'min': str(self.merged_df[
                                   '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].min()) if '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' in self.merged_df.columns else None,
                    'max': str(self.merged_df[
                                   '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].max()) if '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' in self.merged_df.columns else None
                }
            },
            'combinations': combinations_list
        }

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
        self.merged_df = self.merged_df.drop(columns=['_combination'])

        # ============================================================
        # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        # ============================================================

        # –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è: —Å–∫–æ–ª—å–∫–æ % —Å–æ–±—ã—Ç–∏–π –ø–æ–∫—Ä—ã–≤–∞—é—Ç —Ç–æ–ø-N –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
        cumulative_percentage = 0
        coverage_stats = {}

        for threshold in [10, 20, 50, 100]:
            if threshold <= len(combinations_list):
                cumulative_percentage = sum(c['percentage'] for c in combinations_list[:threshold])
                coverage_stats[f'top_{threshold}'] = {
                    'combinations': threshold,
                    'coverage_percentage': round(cumulative_percentage, 2)
                }

        result['coverage_stats'] = coverage_stats

        # ============================================================
        # –°–û–•–†–ê–ù–ï–ù–ò–ï –í JSON
        # ============================================================

        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª: {output_path}")

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        file_size = output_path.stat().st_size / 1024
        logger.info(f"‚úì –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path} ({file_size:.2f} KB)")

        # ============================================================
        # –í–´–í–û–î –°–¢–ê–¢–ò–°–¢–ò–ö–ò –í –õ–û–ì
        # ============================================================

        logger.info(f"\n{'=' * 60}")
        logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –£–ù–ò–ö–ê–õ–¨–ù–´–• –ö–û–ú–ë–ò–ù–ê–¶–ò–ô:")
        logger.info(f"{'=' * 60}")
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_records:,}")
        logger.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: {len(combinations_list):,}")
        logger.info(f"–°—Ä–µ–¥–Ω—è—è —á–∞—Å—Ç–æ—Ç–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏: {total_records / len(combinations_list):.2f}")

        logger.info(f"\n–¢–æ–ø-10 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π:")
        for i, combo in enumerate(combinations_list[:10], 1):
            logger.info(f"  {i:2d}. {combo['path']}")
            logger.info(f"      {combo['count']:,} —Ä–∞–∑ ({combo['percentage']:.2f}%)")

        logger.info(f"\n–ü–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
        for key, stats in coverage_stats.items():
            n = stats['combinations']
            pct = stats['coverage_percentage']
            logger.info(f"  –¢–æ–ø-{n:3d} –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–æ–∫—Ä—ã–≤–∞—é—Ç {pct:5.2f}% –≤—Å–µ—Ö —Å–æ–±—ã—Ç–∏–π")

        # –°–∞–º—ã–µ —Ä–µ–¥–∫–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        rare_combinations = [c for c in combinations_list if c['count'] == 1]
        if len(rare_combinations) > 0:
            rare_pct = len(rare_combinations) / len(combinations_list) * 100
            logger.info(f"\n–†–µ–¥–∫–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ (–≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è 1 —Ä–∞–∑): {len(rare_combinations):,} ({rare_pct:.1f}%)")
            logger.info(f"  –ü—Ä–∏–º–µ—Ä—ã:")
            for combo in rare_combinations[:3]:
                logger.info(f"    - {combo['path']}")

        logger.info(f"{'=' * 60}\n")

        logger.info("‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")

        return result

    def remove_trailing_empty_screens(self) -> pd.DataFrame:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π "–ï—â–µ ‚Üí –û—Ç–∫—Ä—ã—Ç–∏–µ —ç–∫—Ä–∞–Ω–∞ ‚Üí –ù–µ —É–∫–∞–∑–∞–Ω–æ" –≤ —Å–µ—Å—Å–∏—è—Ö

        –õ–æ–≥–∏–∫–∞:
        - –í —Ä–∞–º–∫–∞—Ö –∫–∞–∂–¥–æ–π global_session_id –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å
        - –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—è—è –∑–∞–ø–∏—Å—å –∏–º–µ–µ—Ç:
          * –≠–∫—Ä–∞–Ω = "–ï—â–µ"
          * –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª = "–û—Ç–∫—Ä—ã—Ç–∏–µ —ç–∫—Ä–∞–Ω–∞"
          * –î–µ–π—Å—Ç–≤–∏–µ = "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
        - –¢–æ —É–¥–∞–ª—è–µ–º —ç—Ç—É –∑–∞–ø–∏—Å—å

        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ç–∫—Ä—ã–ª –º–µ–Ω—é "–ï—â–µ" –∏ –≤—ã—à–µ–ª, –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã–±—Ä–∞–≤.
        –≠—Ç–æ "–º—ë—Ä—Ç–≤–∞—è —Ç–æ—á–∫–∞" —Å–µ—Å—Å–∏–∏ –±–µ–∑ –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

        –ü—Ä–∏–º–µ—Ä:
        –ë—ã–ª–æ:
        | session | time  | –≠–∫—Ä–∞–Ω        | –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª        | –î–µ–π—Å—Ç–≤–∏–µ    |
        |---------|-------|--------------|-------------------|-------------|
        | 1       | 10:00 | –ó–∞—è–≤–∫–∏       | –ü—Ä–æ—Å–º–æ—Ç—Ä —Å–ø–∏—Å–∫–∞   | –¢–∞–ø –Ω–∞ –∑–∞—è–≤–∫—É |
        | 1       | 10:05 | –ï—â–µ          | –û—Ç–∫—Ä—ã—Ç–∏–µ —ç–∫—Ä–∞–Ω–∞   | –ù–µ —É–∫–∞–∑–∞–Ω–æ  | ‚Üê –£–î–ê–õ–ò–¢–¨

        –°—Ç–∞–ª–æ:
        | session | time  | –≠–∫—Ä–∞–Ω        | –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª        | –î–µ–π—Å—Ç–≤–∏–µ    |
        |---------|-------|--------------|-------------------|-------------|
        | 1       | 10:00 | –ó–∞—è–≤–∫–∏       | –ü—Ä–æ—Å–º–æ—Ç—Ä —Å–ø–∏—Å–∫–∞   | –¢–∞–ø –Ω–∞ –∑–∞—è–≤–∫—É |

        Returns:
            DataFrame —Å —É–¥–∞–ª—ë–Ω–Ω—ã–º–∏ –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ –∑–∞–ø–∏—Å—è–º–∏
        """
        logger.info("–£–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π '–ï—â–µ ‚Üí –û—Ç–∫—Ä—ã—Ç–∏–µ —ç–∫—Ä–∞–Ω–∞ ‚Üí –ù–µ —É–∫–∞–∑–∞–Ω–æ'...")

        if self.merged_df is None or len(self.merged_df) == 0:
            raise ValueError("merged_df –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

        if 'global_session_id' not in self.merged_df.columns:
            raise ValueError("–¢—Ä–µ–±—É–µ—Ç—Å—è global_session_id (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ add_global_session_id)")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
        initial_rows = len(self.merged_df)
        logger.info(f"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {initial_rows:,}")

        time_col = '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'
        screen_col = '–≠–∫—Ä–∞–Ω'
        function_col = '–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª'
        action_col = '–î–µ–π—Å—Ç–≤–∏–µ'

        target_screen = '–ï—â–µ'
        target_function = '–û—Ç–∫—Ä—ã—Ç–∏–µ —ç–∫—Ä–∞–Ω–∞'
        target_action = '–ù–µ —É–∫–∞–∑–∞–Ω–æ'

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_cols = [screen_col, function_col, action_col]
        for col in required_cols:
            if col not in self.merged_df.columns:
                raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")

        # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã
        self.merged_df = self.merged_df.sort_values(
            by=['global_session_id', time_col],
            ascending=[True, True]
        ).reset_index(drop=True)

        logger.info("–î–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ global_session_id –∏ –≤—Ä–µ–º–µ–Ω–∏")

        # ============================================================
        # –ü–û–ò–°–ö –ü–û–°–õ–ï–î–ù–ò–• –ó–ê–ü–ò–°–ï–ô –° –£–°–õ–û–í–ò–ï–ú
        # ============================================================

        logger.info(f"–ü–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π —Å —É—Å–ª–æ–≤–∏–µ–º:")
        logger.info(f"  –≠–∫—Ä–∞–Ω = '{target_screen}'")
        logger.info(f"  –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª = '{target_function}'")
        logger.info(f"  –î–µ–π—Å—Ç–≤–∏–µ = '{target_action}'")

        # –ü–æ–º–µ—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å –≤ –∫–∞–∂–¥–æ–π —Å–µ—Å—Å–∏–∏
        self.merged_df['is_last_in_session'] = False
        last_indices = self.merged_df.groupby('global_session_id', sort=False).tail(1).index
        self.merged_df.loc[last_indices, 'is_last_in_session'] = True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π
        self.merged_df['to_remove'] = (
                self.merged_df['is_last_in_session'] &
                (self.merged_df[screen_col] == target_screen) &
                (self.merged_df[function_col] == target_function) &
                (self.merged_df[action_col] == target_action)
        )

        rows_to_remove_count = self.merged_df['to_remove'].sum()

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {rows_to_remove_count:,}")

        if rows_to_remove_count == 0:
            logger.info("‚ö†Ô∏è  –ó–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            self.merged_df = self.merged_df.drop(columns=['is_last_in_session', 'to_remove'])
            return self.merged_df

        # ============================================================
        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–û –£–î–ê–õ–ï–ù–ò–Ø
        # ============================================================

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–¥–∞–ª—è–µ–º—ã—Ö –∑–∞–ø–∏—Å—è—Ö
        records_to_remove = self.merged_df[self.merged_df['to_remove']].copy()

        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–µ—Å—Å–∏–∏ —Å —Ç–∞–∫–∏–º –æ–∫–æ–Ω—á–∞–Ω–∏–µ–º
        sessions_with_removal = records_to_remove['global_session_id'].unique()
        session_lengths = self.merged_df[
            self.merged_df['global_session_id'].isin(sessions_with_removal)
        ].groupby('global_session_id').size()

        avg_session_length = session_lengths.mean()

        # –ö–∞–∫–∏–µ –µ—â—ë —ç–∫—Ä–∞–Ω—ã –±—ã–ª–∏ –≤ —ç—Ç–∏—Ö —Å–µ—Å—Å–∏—è—Ö
        other_screens_in_affected_sessions = self.merged_df[
            (self.merged_df['global_session_id'].isin(sessions_with_removal)) &
            (~self.merged_df['to_remove'])
            ][screen_col].value_counts().head(5)

        # ============================================================
        # –£–î–ê–õ–ï–ù–ò–ï –ó–ê–ü–ò–°–ï–ô
        # ============================================================

        logger.info("–£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π...")

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        self.merged_df = self.merged_df[~self.merged_df['to_remove']].copy()

        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        self.merged_df = self.merged_df.drop(columns=['is_last_in_session', 'to_remove'])

        # –°–±—Ä–æ—Å –∏–Ω–¥–µ–∫—Å–∞
        self.merged_df = self.merged_df.reset_index(drop=True)

        # ============================================================
        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–°–õ–ï –£–î–ê–õ–ï–ù–ò–Ø
        # ============================================================

        final_rows = len(self.merged_df)
        removed_rows = initial_rows - final_rows
        removal_percentage = (removed_rows / initial_rows * 100) if initial_rows > 0 else 0

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã—Ö —Å–µ—Å—Å–∏–π
        affected_sessions_count = len(sessions_with_removal)
        total_sessions = self.merged_df['global_session_id'].nunique()
        affected_sessions_pct = (affected_sessions_count / (
                    total_sessions + affected_sessions_count) * 100) if total_sessions > 0 else 0

        logger.info(f"\n{'=' * 60}")
        logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –£–î–ê–õ–ï–ù–ò–Ø –ü–û–°–õ–ï–î–ù–ò–• '–ü–£–°–¢–´–•' –ó–ê–ü–ò–°–ï–ô:")
        logger.info(f"{'=' * 60}")
        logger.info(f"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {initial_rows:,}")
        logger.info(f"–£–¥–∞–ª–µ–Ω–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π: {removed_rows:,} ({removal_percentage:.2f}%)")
        logger.info(f"–û—Å—Ç–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–µ–π: {final_rows:,}")

        logger.info(f"\n–ó–∞—Ç—Ä–æ–Ω—É—Ç–æ —Å–µ—Å—Å–∏–π: {affected_sessions_count:,} ({affected_sessions_pct:.2f}%)")
        logger.info(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã—Ö —Å–µ—Å—Å–∏–π: {avg_session_length:.1f} —Å–æ–±—ã—Ç–∏–π")

        logger.info(f"\n–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:")
        logger.info(f"  {affected_sessions_count:,} —Å–µ—Å—Å–∏–π –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å –Ω–∞ '–ï—â–µ ‚Üí –û—Ç–∫—Ä—ã—Ç–∏–µ ‚Üí –ù–µ —É–∫–∞–∑–∞–Ω–æ'")
        logger.info(f"  –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ç–∫—Ä—ã–ª –º–µ–Ω—é '–ï—â–µ' –∏ –≤—ã—à–µ–ª –∏–∑ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")
        logger.info(f"  –¢–∞–∫–∏–µ –∑–∞–ø–∏—Å–∏ –Ω–µ –Ω–µ—Å—É—Ç –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã")

        if len(other_screens_in_affected_sessions) > 0:
            logger.info(f"\n–¢–æ–ø-5 —ç–∫—Ä–∞–Ω–æ–≤ –≤ –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã—Ö —Å–µ—Å—Å–∏—è—Ö (–¥–æ '–ï—â–µ'):")
            for screen, count in other_screens_in_affected_sessions.items():
                percentage = count / other_screens_in_affected_sessions.sum() * 100
                logger.info(f"  {screen}: {count:,} ({percentage:.1f}%)")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ "–ø—É—Å—Ç—ã–µ" —Å–µ—Å—Å–∏–∏ (–µ—Å–ª–∏ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –æ—Å—Ç–∞–ª–∞—Å—å —Ç–æ–ª—å–∫–æ 1 –∑–∞–ø–∏—Å—å)
        session_lengths_after = self.merged_df.groupby('global_session_id').size()
        single_event_sessions = (session_lengths_after == 1).sum()

        if single_event_sessions > 0:
            single_event_pct = single_event_sessions / total_sessions * 100
            logger.info(
                f"\n‚ö†Ô∏è  –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø–æ—è–≤–∏–ª–∏—Å—å —Å–µ—Å—Å–∏–∏ —Å 1 —Å–æ–±—ã—Ç–∏–µ–º: {single_event_sessions:,} ({single_event_pct:.2f}%)")
            logger.info(f"    –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–¥–∞–ª–∏—Ç—å —Ç–∞–∫–∏–µ —Å–µ—Å—Å–∏–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–æ–¥–æ–º")

        logger.info(f"{'=' * 60}\n")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats['remove_trailing_empty'] = {
            'initial_rows': int(initial_rows),
            'removed_rows': int(removed_rows),
            'final_rows': int(final_rows),
            'removal_percentage': float(removal_percentage),
            'affected_sessions': int(affected_sessions_count),
            'avg_session_length': float(avg_session_length),
            'single_event_sessions_after': int(single_event_sessions)
        }

        logger.info("‚úì –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö '–ø—É—Å—Ç—ã—Ö' –∑–∞–ø–∏—Å–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")

        return self.merged_df

    def fix_action_functional_typos(self) -> pd.DataFrame:
        """
        –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö –î–µ–π—Å—Ç–≤–∏–µ –∏ –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª

        –õ–æ–≥–∏–∫–∞:
        - –í –∫–æ–ª–æ–Ω–∫–µ –î–µ–π—Å—Ç–≤–∏–µ:
          * –ó–∞–º–µ–Ω–∏—Ç—å "–≤—ã–±–æ—Ä —Ç–µ–≥–∞ 1" –Ω–∞ "–í—ã–±–æ—Ä —Ç–µ–≥–∞ 1"
          * –ó–∞–º–µ–Ω–∏—Ç—å "–¢–∞–ø –Ω–∞ —É—Å–ª—É–≥—É –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤" –Ω–∞ "–¢–∞–ø –Ω–∞ —É—Å–ª—É–≥—É –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤"
          * –ó–∞–º–µ–Ω–∏—Ç—å "'–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É '–ú–æ–∏'" –Ω–∞ "–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É '–ú–æ–∏'"
        - –í –∫–æ–ª–æ–Ω–∫–µ –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
          * –ó–∞–º–µ–Ω–∏—Ç—å "–í—ã–±–æ—Ä —É—Å–ª—É–≥–∏ –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤" –Ω–∞ "–í—ã–±–æ—Ä —É—Å–ª—É–≥–∏ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤"
          * –í "–ü–µ—Ä–µ—Ö–æ–¥ –∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—é –¥–æ—Å—Ç—É–ø–∞ —á–µ—Ä–µ–∑ +" –∑–∞–º–µ–Ω–∏—Ç—å –î–µ–π—Å—Ç–≤–∏–µ "–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É" –Ω–∞ "–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É '+'"
          * –í "–û—Ç–º–µ–Ω–∞ –æ—Ç–∑—ã–≤–∞ –¥–æ—Å—Ç—É–ø–∞" –∑–∞–º–µ–Ω–∏—Ç—å –î–µ–π—Å—Ç–≤–∏–µ "–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É '–û—Ç–º–µ–Ω–∞'" –Ω–∞ "–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É '–û—Ç–º–µ–Ω–∏—Ç—å'"

        Returns:
            DataFrame —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –æ–ø–µ—á–∞—Ç–∫–∞–º–∏
        """
        logger.info("–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö –î–µ–π—Å—Ç–≤–∏–µ –∏ –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª...")

        if self.merged_df is None or len(self.merged_df) == 0:
            raise ValueError("merged_df –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        action_col = '–î–µ–π—Å—Ç–≤–∏–µ'
        function_col = '–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª'

        for col in [action_col, function_col]:
            if col not in self.merged_df.columns:
                raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
        initial_rows = len(self.merged_df)
        logger.info(f"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {initial_rows:,}")

        # ============================================================
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–ü–ï–ß–ê–¢–û–ö
        # ============================================================

        corrections = [
            # –í –î–µ–π—Å—Ç–≤–∏–µ
            (action_col, "–≤—ã–±–æ—Ä —Ç–µ–≥–∞ 1", "–í—ã–±–æ—Ä —Ç–µ–≥–∞ 1"),
            (action_col, "–¢–∞–ø –Ω–∞ —É—Å–ª—É–≥—É –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤", "–¢–∞–ø –Ω–∞ —É—Å–ª—É–≥—É –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤"),
            (action_col, "'–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É '–ú–æ–∏'", "–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É '–ú–æ–∏'"),
            # –í –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª
            (function_col, "–í—ã–±–æ—Ä —É—Å–ª—É–≥–∏ –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤", "–í—ã–±–æ—Ä —É—Å–ª—É–≥–∏ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤"),
            # –í –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª —Å —É—Å–ª–æ–≤–∏–µ–º –Ω–∞ –î–µ–π—Å—Ç–≤–∏–µ
            (function_col, "–ü–µ—Ä–µ—Ö–æ–¥ –∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—é –¥–æ—Å—Ç—É–ø–∞ —á–µ—Ä–µ–∑ +", None,
             action_col, "–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É", "–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É '+'"),
            (function_col, "–û—Ç–º–µ–Ω–∞ –æ—Ç–∑—ã–≤–∞ –¥–æ—Å—Ç—É–ø–∞", None,
             action_col, "–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É '–û—Ç–º–µ–Ω–∞'", "–¢–∞–ø –Ω–∞ –∫–Ω–æ–ø–∫—É '–û—Ç–º–µ–Ω–∏—Ç—å'"),
        ]
        total_corrections = 0
        for correction in corrections:
            if len(correction) == 3:
                col, old_value, new_value = correction
                mask = self.merged_df[col] == old_value
            elif len(correction) == 6:
                col, target_function, _, action_col_cond, old_action, new_action = correction
                mask = (self.merged_df[col] == target_function) & (self.merged_df[action_col_cond] == old_action)
            else:
                continue

            count_corrections = mask.sum()
            if count_corrections > 0:
                self.merged_df.loc[mask, col] = new_value
                total_corrections += count_corrections
                logger.info(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {count_corrections:,} –∑–∞–ø–∏—Å–µ–π –≤ –∫–æ–ª–æ–Ω–∫–µ '{col}': '{old_value}' ‚Üí '{new_value}'")
            else:
                logger.info(f"–ó–∞–ø–∏—Å–µ–π –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –∫–æ–ª–æ–Ω–∫–µ '{col}' —Å –∑–Ω–∞—á–µ–Ω–∏–µ–º '{old_value}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        if total_corrections == 0:
            logger.info("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫")
        else:
            logger.info(f"–í—Å–µ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {total_corrections:,}")
        # ============================================================
        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–°–õ–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø
        # ============================================================
        final_rows = len(self.merged_df)
        if final_rows != initial_rows:
            logger.warning("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –∏–∑–º–µ–Ω–∏–ª–æ—Å—å –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫, —á—Ç–æ –Ω–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å")
        logger.info(f"\n{'=' * 60}")
        logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –û–ü–ß–ê–¢–û–ö:")
        logger.info(f"{'=' * 60}")
        logger.info(f"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {initial_rows:,}")
        logger.info(f"–û—Å—Ç–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–µ–π: {final_rows:,}")
        logger.info(f"{'=' * 60}\n")
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats['fix_action_functional_typos'] = {
            'initial_rows': int(initial_rows),
            'final_rows': int(final_rows),
            'total_corrections': int(total_corrections)
        }
        logger.info("‚úì –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        return self.merged_df

    def add_user_cohort_status(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ —Å—Ç–∞—Ç—É—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ –º–µ—Å—è—Ü–∞–º

        –°–æ–∑–¥–∞–µ—Ç 3 boolean –∫–æ–ª–æ–Ω–∫–∏ –≤ —Ä–∞–∑—Ä–µ–∑–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞):
        - is_lost: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –±—ã–ª –∞–∫—Ç–∏–≤–µ–Ω –≤ —Å–µ–Ω—Ç—è–±—Ä–µ, –Ω–æ –ù–ï –±—ã–ª –≤ –æ–∫—Ç—è–±—Ä–µ
        - is_stay: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –±—ã–ª –∞–∫—Ç–∏–≤–µ–Ω –ò –≤ —Å–µ–Ω—Ç—è–±—Ä–µ, –ò –≤ –æ–∫—Ç—è–±—Ä–µ (—É–¥–µ—Ä–∂–∞–Ω–Ω—ã–µ)
        - is_new: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –±—ã–ª –∞–∫—Ç–∏–≤–µ–Ω –¢–û–õ–¨–ö–û –≤ –æ–∫—Ç—è–±—Ä–µ (–Ω–æ–≤—ã–µ)

        –õ–æ–≥–∏–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   –°—Ç–∞—Ç—É—Å    ‚îÇ –°–µ–Ω—Ç—è–±—Ä—å‚îÇ –û–∫—Ç—è–±—Ä—å‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ is_lost     ‚îÇ    ‚úì    ‚îÇ   ‚úó    ‚îÇ
        ‚îÇ is_stay     ‚îÇ    ‚úì    ‚îÇ   ‚úì    ‚îÇ
        ‚îÇ is_new      ‚îÇ    ‚úó    ‚îÇ   ‚úì    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

        –í—Å–µ —Å—Ç—Ä–æ–∫–∏ –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ–ª—É—á–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ñ–ª–∞–≥–æ–≤.

        Args:
            df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' –∏ '–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'

        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ is_lost, is_stay, is_new

        Raises:
            ValueError: –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏

        Example:
            >>> df = add_user_cohort_status(df)
            >>> # –ê–Ω–∞–ª–∏–∑ –æ—Ç—Ç–æ–∫–∞
            >>> churn_rate = df['is_lost'].mean()
            >>> # –ê–Ω–∞–ª–∏–∑ —É–¥–µ—Ä–∂–∞–Ω–∏—è
            >>> retention_rate = df['is_stay'].mean()
            >>> # –ê–Ω–∞–ª–∏–∑ —Ä–æ—Å—Ç–∞
            >>> new_users_rate = df['is_new'].mean()
        """
        logger.info("=" * 70)
        logger.info("–î–û–ë–ê–í–õ–ï–ù–ò–ï –ö–û–ì–û–†–¢–ù–û–ì–û –°–¢–ê–¢–£–°–ê –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô")
        logger.info("=" * 70)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if df is None or len(df) == 0:
            raise ValueError("DataFrame –ø—É—Å—Ç–æ–π –∏–ª–∏ None")

        required_cols = ['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è', '–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞']
        missing_cols = [col for col in required_cols if col not in df.columns]

        if missing_cols:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")

        logger.info(f"–ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(df):,} —Å—Ç—Ä–æ–∫")
        logger.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {df['–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'].nunique():,}")

        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è —Ä–∞–±–æ—Ç—ã
        result_df = df.copy()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ datetime –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not pd.api.types.is_datetime64_any_dtype(result_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è']):
            logger.info("–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è' –≤ datetime...")
            result_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'] = pd.to_datetime(
                result_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'],
                utc=True,
                errors='coerce'
            )

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Å—è—Ü
        logger.info("–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Å—è—Ü–µ–≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
        result_df['_temp_month'] = result_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].dt.month

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –≤ –∫–∞–∫–∏—Ö –º–µ—Å—è—Ü–∞—Ö –æ–Ω –±—ã–ª –∞–∫—Ç–∏–≤–µ–Ω
        user_months = result_df.groupby('–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞')['_temp_month'].apply(
            lambda x: set(x.dropna().unique())
        ).to_dict()

        logger.info("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –∫–æ–≥–æ—Ä—Ç–∞–º...")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_status = {}

        stats = {
            'lost': 0,  # –ë—ã–ª –≤ —Å–µ–Ω—Ç—è–±—Ä–µ, –Ω–µ—Ç –≤ –æ–∫—Ç—è–±—Ä–µ
            'stay': 0,  # –ë—ã–ª –≤ —Å–µ–Ω—Ç—è–±—Ä–µ –∏ –æ–∫—Ç—è–±—Ä–µ
            'new': 0,  # –¢–æ–ª—å–∫–æ –≤ –æ–∫—Ç—è–±—Ä–µ
            'other': 0  # –î—Ä—É–≥–∏–µ —Å–ª—É—á–∞–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–ª—å–∫–æ –≤ –∞–≤–≥—É—Å—Ç–µ)
        }

        for user_id, months in user_months.items():
            has_september = 9 in months
            has_october = 10 in months

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
            if has_september and not has_october:
                # –ü–æ—Ç–µ—Ä—è–Ω–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
                user_status[user_id] = {
                    'is_lost': True,
                    'is_stay': False,
                    'is_new': False
                }
                stats['lost'] += 1

            elif has_september and has_october:
                # –£–¥–µ—Ä–∂–∞–Ω–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
                user_status[user_id] = {
                    'is_lost': False,
                    'is_stay': True,
                    'is_new': False
                }
                stats['stay'] += 1

            elif not has_september and has_october:
                # –ù–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
                user_status[user_id] = {
                    'is_lost': False,
                    'is_stay': False,
                    'is_new': True
                }
                stats['new'] += 1

            else:
                # –î—Ä—É–≥–∏–µ —Å–ª—É—á–∞–∏ (–Ω–µ –±—ã–ª –Ω–∏ –≤ —Å–µ–Ω—Ç—è–±—Ä–µ, –Ω–∏ –≤ –æ–∫—Ç—è–±—Ä–µ)
                user_status[user_id] = {
                    'is_lost': False,
                    'is_stay': False,
                    'is_new': False
                }
                stats['other'] += 1

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–∞—Ç—É—Å—ã –∫–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        logger.info("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–æ–≤ –∫ –¥–∞—Ç–∞—Å–µ—Ç—É...")

        result_df['is_lost'] = result_df['–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'].map(
            lambda x: user_status.get(x, {}).get('is_lost', False)
        )
        result_df['is_stay'] = result_df['–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'].map(
            lambda x: user_status.get(x, {}).get('is_stay', False)
        )
        result_df['is_new'] = result_df['–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'].map(
            lambda x: user_status.get(x, {}).get('is_new', False)
        )

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
        result_df.drop('_temp_month', axis=1, inplace=True)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_users = len(user_status)

        logger.info("\n" + "=" * 70)
        logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ö–û–ì–û–†–¢–ê–ú:")
        logger.info("=" * 70)
        logger.info(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {total_users:,}")
        logger.info("")
        logger.info(f"  üî¥ –ü–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ (is_lost):   {stats['lost']:6,} ({100 * stats['lost'] / total_users:5.2f}%)")
        logger.info(f"  üü¢ –£–¥–µ—Ä–∂–∞–Ω–Ω—ã–µ (is_stay):   {stats['stay']:6,} ({100 * stats['stay'] / total_users:5.2f}%)")
        logger.info(f"  üîµ –ù–æ–≤—ã–µ (is_new):         {stats['new']:6,} ({100 * stats['new'] / total_users:5.2f}%)")

        if stats['other'] > 0:
            logger.info(f"  ‚ö™ –ü—Ä–æ—á–∏–µ:                 {stats['other']:6,} ({100 * stats['other'] / total_users:5.2f}%)")

        logger.info("")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–æ–∫–∞–º
        logger.info("–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –°–¢–†–û–ö –ü–û –ö–û–ì–û–†–¢–ê–ú:")
        logger.info("=" * 70)
        logger.info(f"  –°—Ç—Ä–æ–∫ –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã—Ö:  {result_df['is_lost'].sum():8,} ({100 * result_df['is_lost'].mean():5.2f}%)")
        logger.info(f"  –°—Ç—Ä–æ–∫ —É–¥–µ—Ä–∂–∞–Ω–Ω—ã—Ö:  {result_df['is_stay'].sum():8,} ({100 * result_df['is_stay'].mean():5.2f}%)")
        logger.info(f"  –°—Ç—Ä–æ–∫ –Ω–æ–≤—ã—Ö:       {result_df['is_new'].sum():8,} ({100 * result_df['is_new'].mean():5.2f}%)")
        logger.info("=" * 70)

        # –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if stats['lost'] + stats['stay'] > 0:
            churn_rate = 100 * stats['lost'] / (stats['lost'] + stats['stay'])
            retention_rate = 100 * stats['stay'] / (stats['lost'] + stats['stay'])

            logger.info("\n–ö–õ–Æ–ß–ï–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
            logger.info("=" * 70)
            logger.info(f"  Churn Rate (–æ—Ç—Ç–æ–∫):       {churn_rate:5.2f}%")
            logger.info(f"  Retention Rate (—É–¥–µ—Ä–∂–∞–Ω–∏–µ): {retention_rate:5.2f}%")
            logger.info(f"  Growth (–Ω–æ–≤—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏): {stats['new']:,}")
            logger.info("=" * 70)

        logger.info("\n‚úì –ö–æ–≥–æ—Ä—Ç–Ω—ã–µ —Å—Ç–∞—Ç—É—Å—ã —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω—ã")
        logger.info(f"‚úì –î–æ–±–∞–≤–ª–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: 3 (is_lost, is_stay, is_new)")
        logger.info(f"‚úì –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {len(result_df):,} —Å—Ç—Ä–æ–∫ √ó {len(result_df.columns)} –∫–æ–ª–æ–Ω–æ–∫\n")

        return result_df
