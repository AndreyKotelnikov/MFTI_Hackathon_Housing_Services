"""
–ì–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω–æ–≥–æ pipeline –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
"""
import json
import logging
import sys
from pathlib import Path

from src.fb2.full_analyze.analyze_user_behavior import UserBehaviorAnalyzer
from src.fb2.funnels.funnel_features_extractor import FunnelFeaturesExtractor

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent))

from src.data_preprocessing import DataPreprocessor
from src.eda_analysis import EDAAnalyzer
from src.sequence_mining import SequenceAnalyzer
from src.visualization import VisualizationEngine
from src.utils import (
    setup_logging,
    load_config,
    ensure_directories,
    Timer,
    validate_data_files
)
from src.analyze_user_journeys import analyze_user_journeys

logger = logging.getLogger(__name__)


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    """
    print("=" * 80)
    print("–ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨–°–ö–û–ì–û –ü–û–í–ï–î–ï–ù–ò–Ø - –ú–ö–î –ü–†–ò–õ–û–ñ–ï–ù–ò–ï")
    print("=" * 80)
    print()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = load_config("config.yaml")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    setup_logging(config.get('logging', {}))
    logger.info("–ù–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞")

    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    ensure_directories(config)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    if not validate_data_files(config):
        logger.error("–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü–æ–º–µ—Å—Ç–∏—Ç–µ events.csv –∏ users.csv –≤ data/raw/")
        print("\n‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        print("–ü–æ–º–µ—Å—Ç–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é data/raw/:")
        print("  - events.csv")
        print("  - users.csv")
        return 1

    try:
        # ============================================================
        # –≠–¢–ê–ü 1: –ó–∞–≥—Ä—É–∑–∫–∞/–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        # ============================================================
        print("\n" + "=" * 80)
        print("–≠–¢–ê–ü 1: –ó–ê–ì–†–£–ó–ö–ê –ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•")
        print("=" * 80)

        preprocessor = DataPreprocessor(config_path="config.yaml")
        processed_file = Path('data/processed/merged_data.csv')

        if processed_file.exists():
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            print(f"\nüìÇ –ù–∞–π–¥–µ–Ω –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {processed_file}")
            print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

            with Timer("–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"):
                merged_df = preprocessor.load_merged_data()
                print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(merged_df):,}")
                print(f"‚úì –ö–æ–ª–æ–Ω–æ–∫: {len(merged_df.columns)}")
                print(
                    f"‚úì –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {merged_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].min().date()} - {merged_df['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–±—ã—Ç–∏—è'].max().date()}")

                # preprocessor.add_global_session_id()
                # preprocessor.calculate_event_duration()
                # preprocessor.remove_consecutive_duplicates_with_clicks()
                # preprocessor.collapse_intermediate_screens()
                # preprocessor.remove_trailing_empty_screens()
                # preprocessor.fix_action_functional_typos()
                # preprocessor.save_processed_data()

                # –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É
                json_path = 'src/fb2/funnels/categorized_combinations_with_funnels.json'
                # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
                extractor = FunnelFeaturesExtractor(json_path)

                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ funnel features
                df_transformed = extractor.transform(merged_df)

                df = preprocessor.add_user_cohort_status(df_transformed)

                # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ is_new = True
                df_transformed2 = df[df['is_new'] == False].copy()

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–æ–≥–∞—â–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
                preprocessor.merged_df = df_transformed2
                preprocessor.save_processed_data()

                # –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞
                analyzer = UserBehaviorAnalyzer("", 'fb2_output')
                analyzer.run_full_analysis(df_transformed2)

        logger.info("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        return 0

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞: {e}", exc_info=True)
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥-—Ñ–∞–π–ª analysis.log –¥–ª—è –¥–µ—Ç–∞–ª–µ–π")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
