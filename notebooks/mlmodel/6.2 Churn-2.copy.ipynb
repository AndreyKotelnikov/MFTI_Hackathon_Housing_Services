{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dAy7QP620ChQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
        "import warnings\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T1gkMzVX0EFt"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"../../data/clean_data.100k.csv\"\n",
        "SAVE_DIR = \"../../data/graph-save\"\n",
        "OBSERVATION_DAYS = 30\n",
        "PREDICTION_DAYS = 31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq8hCqQS0g85"
      },
      "source": [
        "# Data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ig-RtowH0fKU"
      },
      "outputs": [],
      "source": [
        "def create_temporal_dataset(df, observation_days=30, prediction_days=31):\n",
        "    df = df.copy()\n",
        "    df['event_dt'] = pd.to_datetime(df['event_dt'])\n",
        "\n",
        "    max_date = df['event_dt'].max()\n",
        "    min_date = df['event_dt'].min()\n",
        "\n",
        "    observation_end = max_date - pd.Timedelta(days=prediction_days)\n",
        "    observation_start = observation_end - pd.Timedelta(days=observation_days)\n",
        "\n",
        "    if observation_start < min_date:\n",
        "        observation_start = min_date\n",
        "\n",
        "    print(f\"\\nObservation: {observation_start.date()} to {observation_end.date()}\")\n",
        "    print(f\"Prediction: {observation_end.date()} to {max_date.date()}\")\n",
        "\n",
        "    observation_data = df[\n",
        "        (df['event_dt'] >= observation_start) &\n",
        "        (df['event_dt'] < observation_end)\n",
        "    ].copy()\n",
        "\n",
        "    prediction_data = df[df['event_dt'] >= observation_end]\n",
        "\n",
        "    active_in_observation = set(observation_data['device_id'].unique())\n",
        "    active_in_prediction = set(prediction_data['device_id'].unique())\n",
        "    churned_users = active_in_observation - active_in_prediction\n",
        "\n",
        "    print(f\"Users: {len(active_in_observation):,} | Churned: {len(churned_users):,} ({len(churned_users)/len(active_in_observation):.1%})\")\n",
        "\n",
        "    churn_labels = {uid: 1 if uid in churned_users else 0 for uid in active_in_observation}\n",
        "\n",
        "    return observation_data, churn_labels\n",
        "\n",
        "def create_unified_split(observation_data, churn_labels, save_dir):\n",
        "    user_ids = list(churn_labels.keys())\n",
        "    labels = [churn_labels[uid] for uid in user_ids]\n",
        "\n",
        "    train_val_ids, test_ids, train_val_labels, test_labels = train_test_split(\n",
        "        user_ids, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    train_ids, val_ids, train_labels, val_labels = train_test_split(\n",
        "        train_val_ids, train_val_labels, test_size=0.25, random_state=42, stratify=train_val_labels\n",
        "    )\n",
        "\n",
        "    split_data = {\n",
        "        'train_ids': train_ids,\n",
        "        'val_ids': val_ids,\n",
        "        'test_ids': test_ids,\n",
        "        'train_labels': train_labels,\n",
        "        'val_labels': val_labels,\n",
        "        'test_labels': test_labels\n",
        "    }\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    with open(os.path.join(save_dir, 'data_split.pkl'), 'wb') as f:\n",
        "        pickle.dump(split_data, f)\n",
        "\n",
        "    print(f\"\\nSplit: Train {len(train_ids):,} | Val {len(val_ids):,} | Test {len(test_ids):,}\")\n",
        "\n",
        "    return split_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5r8dDZH1Joz"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IoEyltLf0gIQ"
      },
      "outputs": [],
      "source": [
        "def extract_baseline_features(df, user_ids):\n",
        "    event_features = df.groupby('device_id').agg({\n",
        "        'event_dt': ['min', 'max', 'count'],\n",
        "        'session_id': 'nunique',\n",
        "        'screen': 'nunique',\n",
        "        'feature': 'nunique',\n",
        "        'action': 'count'\n",
        "    })\n",
        "    event_features.columns = ['_'.join(col).strip() for col in event_features.columns]\n",
        "\n",
        "    event_features['days_in_window'] = (\n",
        "        event_features['event_dt_max'] - event_features['event_dt_min']\n",
        "    ).dt.total_seconds() / 86400\n",
        "\n",
        "    event_features['events_per_day'] = event_features['event_dt_count'] / event_features['days_in_window'].clip(lower=1)\n",
        "    event_features['sessions_per_day'] = event_features['session_id_nunique'] / event_features['days_in_window'].clip(lower=1)\n",
        "    event_features['events_per_session'] = event_features['event_dt_count'] / event_features['session_id_nunique'].clip(lower=1)\n",
        "    event_features['screen_diversity'] = event_features['screen_nunique'] / event_features['event_dt_count']\n",
        "    event_features['feature_diversity'] = event_features['feature_nunique'] / event_features['event_dt_count']\n",
        "\n",
        "    first_day = event_features['event_dt_min'].min()\n",
        "    last_day = event_features['event_dt_max'].max()\n",
        "    event_features['days_since_first_seen'] = (event_features['event_dt_min'] - first_day).dt.total_seconds() / 86400\n",
        "    event_features['days_until_window_end'] = (last_day - event_features['event_dt_max']).dt.total_seconds() / 86400\n",
        "    event_features['recency_in_window'] = event_features['days_until_window_end'] / event_features['days_in_window'].clip(lower=1)\n",
        "\n",
        "    demographic = df.groupby('device_id')[['age', 'gender']].first()\n",
        "    features = event_features.join(demographic, how='left')\n",
        "    features = features.drop(columns=['event_dt_min', 'event_dt_max'])\n",
        "\n",
        "    if 'gender' in features.columns:\n",
        "        features = pd.get_dummies(features, columns=['gender'], prefix='gender', drop_first=False)\n",
        "\n",
        "    features = features.fillna(features.median())\n",
        "    features = features.loc[features.index.intersection(user_ids)]\n",
        "\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUDg1Cca1Ssw"
      },
      "source": [
        "# GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UXZ_EwrN1XYL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "class GraphSAGEChurn(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels=128, num_layers=2, dropout=0.3):\n",
        "        super(GraphSAGEChurn, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            x = conv(x, edge_index)\n",
        "            if i < len(self.convs) - 1:\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        out = self.classifier(x)\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "def build_random_graph(n_users, edges_per_node=5):\n",
        "    edge_list = []\n",
        "    for i in range(n_users):\n",
        "        neighbors = np.random.choice(n_users, size=min(edges_per_node, n_users-1), replace=False)\n",
        "        neighbors = neighbors[neighbors != i]\n",
        "        for j in neighbors:\n",
        "            edge_list.append([i, j])\n",
        "\n",
        "    return torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "\n",
        "def train_gnn(data, train_mask, val_mask, test_mask, save_dir, device='cuda', config=None):\n",
        "\n",
        "    hidden_channels = getattr(config, 'hidden_channels', 128)\n",
        "    num_layers = getattr(config, 'num_layers', 2)\n",
        "    dropout = getattr(config, 'dropout', 0.3)\n",
        "    learning_rate = getattr(config, 'learning_rate', 0.01)\n",
        "    weight_decay = getattr(config, 'weight_decay', 5e-4)\n",
        "    max_epochs = getattr(config, 'epochs', 100)\n",
        "\n",
        "    model = GraphSAGEChurn(in_channels=data.x.size(1), hidden_channels=hidden_channels, num_layers=num_layers, dropout=dropout)\n",
        "    model = model.to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    best_val_auc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index).squeeze()\n",
        "        loss = criterion(out[train_mask], data.y[train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data.x, data.edge_index).squeeze()\n",
        "\n",
        "            train_pred = out[train_mask].cpu().numpy()\n",
        "            train_true = data.y[train_mask].cpu().numpy()\n",
        "            val_pred = out[val_mask].cpu().numpy()\n",
        "            val_true = data.y[val_mask].cpu().numpy()\n",
        "\n",
        "            train_auc = roc_auc_score(train_true, train_pred)\n",
        "            val_auc = roc_auc_score(val_true, val_pred)\n",
        "\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1:03d} | Train {train_auc:.3f} | Val {val_auc:.3f}\")\n",
        "\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'config': {'in_channels': data.x.size(1), 'hidden_channels': hidden_channels, 'num_layers': num_layers, 'dropout': dropout}\n",
        "            }, os.path.join(save_dir, 'gnn_model.pth'))\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= 15:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(os.path.join(save_dir, 'gnn_model.pth'))['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index).squeeze()\n",
        "        test_pred = out[test_mask].cpu().numpy()\n",
        "        test_true = data.y[test_mask].cpu().numpy()\n",
        "\n",
        "    test_pred_binary = (test_pred > 0.5).astype(int)\n",
        "    test_auc = roc_auc_score(test_true, test_pred)\n",
        "    test_precision = precision_score(test_true, test_pred_binary)\n",
        "    test_recall = recall_score(test_true, test_pred_binary)\n",
        "    test_f1 = f1_score(test_true, test_pred_binary)\n",
        "    print(f\"\\nGNN: Best Val {best_val_auc:.3f} | Test {test_auc:.3f}\")\n",
        "\n",
        "\n",
        "    return {\n",
        "        'model': 'GNN',\n",
        "        'val_auc': best_val_auc,\n",
        "        'test_auc': test_auc,\n",
        "        'test_precision': test_precision,\n",
        "        'test_recall': test_recall,\n",
        "        'test_f1': test_f1\n",
        "    }\n",
        "\n",
        "GNN_SWEEP_CONFIG = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {'name': 'val_auc', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'hidden_channels': {'values': [64, 128, 256]},\n",
        "        'num_layers': {'values': [1, 2, 3]},\n",
        "        'dropout': {'distribution': 'uniform', 'min': 0.1, 'max': 0.5},\n",
        "        'learning_rate': {'distribution': 'log_uniform_values', 'min': 0.001, 'max': 0.1},\n",
        "        'weight_decay': {'distribution': 'log_uniform_values', 'min': 1e-5, 'max': 1e-3},\n",
        "        'epochs': {'value': 100}\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# all_ids = train_ids + val_ids + test_ids\n",
        "\n",
        "def build_graph_from_edges(edges_df, all_ids, undirected=False):\n",
        "    user_id_to_idx = {uid: i for i, uid in enumerate(all_ids)}\n",
        "\n",
        "    edges_df = edges_df[\n",
        "        edges_df['source_id'].isin(user_id_to_idx) &\n",
        "        edges_df['target_id'].isin(user_id_to_idx)\n",
        "    ]\n",
        "\n",
        "    edge_index = torch.tensor([\n",
        "        edges_df['source_id'].map(user_id_to_idx).values,\n",
        "        edges_df['target_id'].map(user_id_to_idx).values\n",
        "    ], dtype=torch.long)\n",
        "\n",
        "    if undirected:\n",
        "        edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
        "\n",
        "    edge_index = torch.unique(edge_index, dim=1)\n",
        "    return edge_index\n",
        "\n",
        "# edge_index = build_graph_from_edges(\n",
        "#     edges_df,\n",
        "#     all_ids\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OibNzh_J10gU"
      },
      "outputs": [],
      "source": [
        "def run_gnn_single(train_ids, val_ids, test_ids, observation_data, churn_labels, edges_df):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING GNN (SINGLE RUN)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    all_ids = train_ids + val_ids + test_ids\n",
        "    features = extract_baseline_features(observation_data, all_ids)\n",
        "    X_scaled = StandardScaler().fit_transform(features.loc[all_ids])\n",
        "\n",
        "    # edge_index = build_random_graph(len(all_ids), edges_per_node=5)\n",
        "    # print('edge_index', edge_index)\n",
        "    edge_index = build_graph_from_edges(\n",
        "        edges_df,\n",
        "        all_ids\n",
        "    )\n",
        "\n",
        "    user_id_to_idx = {uid: idx for idx, uid in enumerate(all_ids)}\n",
        "    train_mask = torch.zeros(len(all_ids), dtype=torch.bool)\n",
        "    val_mask = torch.zeros(len(all_ids), dtype=torch.bool)\n",
        "    test_mask = torch.zeros(len(all_ids), dtype=torch.bool)\n",
        "\n",
        "    for uid in train_ids:\n",
        "        train_mask[user_id_to_idx[uid]] = True\n",
        "    for uid in val_ids:\n",
        "        val_mask[user_id_to_idx[uid]] = True\n",
        "    for uid in test_ids:\n",
        "        test_mask[user_id_to_idx[uid]] = True\n",
        "\n",
        "    x = torch.FloatTensor(X_scaled)\n",
        "    y = torch.FloatTensor([churn_labels[uid] for uid in all_ids])\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    result = train_gnn(data, train_mask, val_mask, test_mask, SAVE_DIR, device)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm3-RzNY1Z-Z"
      },
      "source": [
        "# Split and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y7jdoJq1bb4",
        "outputId": "6bee49f1-7080-4786-98fa-dbbbc7bd1410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 99,999 events, 16,129 users\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(DATA_PATH)\n",
        "print(f\"Loaded {len(df):,} events, {df['device_id'].nunique():,} users\")\n",
        "edges_df = pd.read_csv(\"../../data/links_graph.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4-wDquQ1gTE",
        "outputId": "15b118b3-6329-417b-8389-3a5486c32e9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Observation: 2025-09-01 to 2025-09-24\n",
            "Prediction: 2025-09-24 to 2025-10-25\n",
            "Users: 8,501 | Churned: 6,230 (73.3%)\n",
            "\n",
            "Split: Train 5,100 | Val 1,700 | Test 1,701\n",
            "Train: 5100 users, Val: 1700 users, Test: 1701 users\n"
          ]
        }
      ],
      "source": [
        "observation_data, churn_labels = create_temporal_dataset(df, OBSERVATION_DAYS, PREDICTION_DAYS)\n",
        "split_data = create_unified_split(observation_data, churn_labels, SAVE_DIR)\n",
        "\n",
        "train_ids = split_data['train_ids']\n",
        "val_ids = split_data['val_ids']\n",
        "test_ids = split_data['test_ids']\n",
        "train_labels = split_data['train_labels']\n",
        "val_labels = split_data['val_labels']\n",
        "test_labels = split_data['test_labels']\n",
        "\n",
        "print(f\"Train: {len(train_ids)} users, Val: {len(val_ids)} users, Test: {len(test_ids)} users\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnHHhkt62UKD"
      },
      "source": [
        "# Test 3 models and compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BPgAK4ZL0Nbp"
      },
      "outputs": [],
      "source": [
        "def run_all_single():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING ALL MODELS (SINGLE RUNS)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    results.append(run_gnn_single(train_ids, val_ids, test_ids, observation_data, churn_labels, edges_df))\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    results_df.to_csv(os.path.join(SAVE_DIR, 'final_results.csv'), index=False)\n",
        "\n",
        "    best_model = results_df.loc[results_df['test_auc'].idxmax()]\n",
        "    print(f\"\\nüèÜ Best: {best_model['model']} (Test AUC: {best_model['test_auc']:.4f})\")\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-dk2mkfa2lcS",
        "outputId": "338db6ce-f2ff-4bfe-d397-8e5e30f39dd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAINING ALL MODELS (SINGLE RUNS)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "TRAINING GNN (SINGLE RUN)\n",
            "================================================================================\n",
            "Using device: cpu\n",
            "Epoch 010 | Train 0.682 | Val 0.694\n",
            "Epoch 020 | Train 0.726 | Val 0.721\n",
            "Epoch 030 | Train 0.752 | Val 0.727\n",
            "Epoch 040 | Train 0.797 | Val 0.746\n",
            "Epoch 050 | Train 0.828 | Val 0.761\n",
            "Epoch 060 | Train 0.841 | Val 0.762\n",
            "Epoch 070 | Train 0.859 | Val 0.759\n",
            "Epoch 080 | Train 0.870 | Val 0.767\n",
            "Epoch 090 | Train 0.885 | Val 0.761\n",
            "Epoch 100 | Train 0.891 | Val 0.763\n",
            "\n",
            "GNN: Best Val 0.772 | Test 0.754\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULTS\n",
            "================================================================================\n",
            "\n",
            "model  val_auc  test_auc  test_precision  test_recall  test_f1\n",
            "  GNN 0.772189  0.754134          0.8121     0.914996 0.860483\n",
            "\n",
            "üèÜ Best: GNN (Test AUC: 0.7541)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>val_auc</th>\n",
              "      <th>test_auc</th>\n",
              "      <th>test_precision</th>\n",
              "      <th>test_recall</th>\n",
              "      <th>test_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GNN</td>\n",
              "      <td>0.772189</td>\n",
              "      <td>0.754134</td>\n",
              "      <td>0.8121</td>\n",
              "      <td>0.914996</td>\n",
              "      <td>0.860483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  model   val_auc  test_auc  test_precision  test_recall   test_f1\n",
              "0   GNN  0.772189  0.754134          0.8121     0.914996  0.860483"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_all_single()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
