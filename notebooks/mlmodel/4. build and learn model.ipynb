{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c9675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "result_df = pd.read_csv(\"../../data/clean_data.100k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e2631",
   "metadata": {},
   "source": [
    "## 4. Модель GNN (GraphSAGE).\n",
    "\n",
    "Модель GraphSAGE (PyG) с обучением и инференсом.\n",
    "Включён и node-level head (предсказывает churn_rate) и edge-level head (предсказывает transition_count / flow).\n",
    "Код предполагает, что у нас в наличии:\n",
    "\n",
    "- `node_df` — DataFrame агрегированных нод (индекс = node_id), в котором есть колонка text_embedding (np.array) и колонка-таргет churn_rate.\n",
    "\n",
    "- `edge_df` — DataFrame агрегированных ребер, индекс = (src_node, dst_node), содержит числовые edge-фичи и колонку transition_count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b850519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn_training.py\n",
    "# GraphSAGE for node-level (churn_rate) and edge-level (transition_count) regression\n",
    "# Requires: torch, torch_geometric, scikit-learn, numpy, pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: build PyG Data (if not already built)\n",
    "# -----------------------------\n",
    "def build_pyg_data_from_dfs(node_df: pd.DataFrame, edge_df: pd.DataFrame,\n",
    "                            node_target_col='churn_rate', edge_target_col='transition_count'):\n",
    "    \"\"\"\n",
    "    node_df.index -> node_id\n",
    "    node_df contains text_embedding column (np.array) and numeric/categorical columns\n",
    "    edge_df.index -> (src_node, dst_node)\n",
    "    \"\"\"\n",
    "    # Map node_id -> idx\n",
    "    node_ids = list(node_df.index)\n",
    "    node_id_map = {nid: i for i, nid in enumerate(node_ids)}\n",
    "\n",
    "    # Prepare node numeric features (exclude target and keep embeddings)\n",
    "    # Extract embedding\n",
    "    if 'text_embedding' in node_df.columns:\n",
    "        emb = np.vstack(node_df['text_embedding'].values).astype(float)\n",
    "        node_df_wo_emb = node_df.drop(columns=['text_embedding'])\n",
    "    else:\n",
    "        emb = np.zeros((len(node_df), 0))\n",
    "        node_df_wo_emb = node_df.copy()\n",
    "\n",
    "    # Identify categorical/object columns and encode to codes\n",
    "    node_df_enc = node_df_wo_emb.copy()\n",
    "    cat_cols = []\n",
    "    for c in node_df_enc.columns:\n",
    "        if node_df_enc[c].dtype == object or str(node_df_enc[c].dtype).startswith('category'):\n",
    "            cat_cols.append(c)\n",
    "            node_df_enc[c] = node_df_enc[c].astype('category').cat.codes\n",
    "\n",
    "    # Numeric columns (exclude target)\n",
    "    num_cols = [c for c in node_df_enc.columns if c != node_target_col]\n",
    "    X_num = node_df_enc[num_cols].astype(float).fillna(0).values\n",
    "\n",
    "    # Scale numeric\n",
    "    scaler = StandardScaler()\n",
    "    if X_num.size > 0:\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "    X = np.concatenate([X_num, emb], axis=1) if emb.size else X_num\n",
    "\n",
    "    x = torch.tensor(X, dtype=torch.float)\n",
    "\n",
    "    # node target y\n",
    "    y_node = torch.tensor(node_df[node_target_col].astype(float).values, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    # Edge index and attributes\n",
    "    # edge_df index should be MultiIndex (src, dst) OR columns src_node/dst_node\n",
    "    if isinstance(edge_df.index, pd.MultiIndex) and edge_df.index.nlevels == 2:\n",
    "        srcs = [node_id_map[s] for s, d in edge_df.index]\n",
    "        dsts = [node_id_map[d] for s, d in edge_df.index]\n",
    "    else:\n",
    "        # try columns\n",
    "        if {'src_node','dst_node'}.issubset(edge_df.columns):\n",
    "            srcs = [node_id_map[s] for s in edge_df['src_node'].values]\n",
    "            dsts = [node_id_map[d] for d in edge_df['dst_node'].values]\n",
    "        else:\n",
    "            raise ValueError(\"edge_df index must be MultiIndex (src,dst) or contain src_node/dst_node columns\")\n",
    "\n",
    "    edge_index = torch.tensor([srcs, dsts], dtype=torch.long)\n",
    "\n",
    "    # Edge attributes (drop src/dst columns if present and target if present)\n",
    "    edge_df_local = edge_df.copy()\n",
    "    for c in ['src_node','dst_node', edge_target_col]:\n",
    "        if c in edge_df_local.columns:\n",
    "            edge_df_local = edge_df_local.drop(columns=[c])\n",
    "    # encode categorical edge cols\n",
    "    for c in edge_df_local.columns:\n",
    "        if edge_df_local[c].dtype == object or str(edge_df_local[c].dtype).startswith('category'):\n",
    "            edge_df_local[c] = edge_df_local[c].astype('category').cat.codes\n",
    "    edge_attr = torch.tensor(edge_df_local.fillna(0).astype(float).values, dtype=torch.float)\n",
    "\n",
    "    # Edge target (for training edge head)\n",
    "    if edge_target_col in edge_df.columns:\n",
    "        edge_y = torch.tensor(edge_df[edge_target_col].astype(float).values, dtype=torch.float).unsqueeze(1)\n",
    "    else:\n",
    "        edge_y = None\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y_node)\n",
    "    data.edge_y = edge_y  # custom attribute for edge targets\n",
    "    data.node_id_map = node_id_map\n",
    "\n",
    "    return data, scaler, num_cols\n",
    "\n",
    "# -----------------------------\n",
    "# Model: GraphSAGE encoder + node regression head + edge regression head\n",
    "# -----------------------------\n",
    "class GraphSAGENet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, num_layers=2, edge_dim=0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Сохраняем параметры как атрибуты класса\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers-1):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "\n",
    "        # Node regression head\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels//2, 1)  # churn_rate scalar\n",
    "        )\n",
    "\n",
    "        # Edge regression head: we'll use concatenation of src_emb || dst_emb || edge_attr\n",
    "        self.edge_dim = edge_dim\n",
    "        edge_input_dim = hidden_channels * 2 + edge_dim\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_input_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, 1)  # transition_count (or normalized)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        # x: [N, in_channels], edge_index: [2, E], edge_attr: [E, edge_dim]\n",
    "        h = x\n",
    "        for conv in self.convs:\n",
    "            h = conv(h, edge_index)\n",
    "            h = F.relu(h)\n",
    "\n",
    "        # node preds\n",
    "        node_pred = self.node_mlp(h)  # [N, 1]\n",
    "\n",
    "        # edge preds\n",
    "        if edge_attr is not None:\n",
    "            src_idx = edge_index[0]\n",
    "            dst_idx = edge_index[1]\n",
    "            src_h = h[src_idx]\n",
    "            dst_h = h[dst_idx]\n",
    "            edge_input = torch.cat([src_h, dst_h, edge_attr], dim=1)\n",
    "            edge_pred = self.edge_mlp(edge_input)  # [E, 1]\n",
    "        else:\n",
    "            edge_pred = None\n",
    "\n",
    "        return node_pred, edge_pred, h  # also return node embeddings\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "def train_model(data: Data,\n",
    "                lr=1e-3,\n",
    "                epochs=200,\n",
    "                val_ratio=0.1,\n",
    "                test_ratio=0.1,\n",
    "                hidden=64,\n",
    "                device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Train GraphSAGENet on Data.\n",
    "    data.edge_y can be None (skip edge head training).\n",
    "    Returns trained model and scalers/maps for later inference.\n",
    "    \"\"\"\n",
    "    # Move data to device\n",
    "    data = data.clone()\n",
    "    data = data.to(device)\n",
    "    N = data.num_nodes\n",
    "    E = data.edge_index.shape[1]\n",
    "\n",
    "    # Train/val/test split on nodes (node-level supervised)\n",
    "    idx = np.arange(N)\n",
    "    idx_train, idx_tmp = train_test_split(idx, test_size=(val_ratio+test_ratio), random_state=42)\n",
    "    relative_val = val_ratio / (val_ratio + test_ratio)\n",
    "    idx_val, idx_test = train_test_split(idx_tmp, test_size=(1-relative_val), random_state=42)\n",
    "\n",
    "    train_mask = torch.zeros(N, dtype=torch.bool, device=device)\n",
    "    val_mask = torch.zeros(N, dtype=torch.bool, device=device)\n",
    "    test_mask = torch.zeros(N, dtype=torch.bool, device=device)\n",
    "    train_mask[idx_train] = True\n",
    "    val_mask[idx_val] = True\n",
    "    test_mask[idx_test] = True\n",
    "\n",
    "    # If edge target exists, train on all edges (no edge split here for simplicity)\n",
    "    edge_has_target = getattr(data, 'edge_y', None) is not None\n",
    "    if edge_has_target:\n",
    "        edge_y = data.edge_y.to(device)\n",
    "    else:\n",
    "        edge_y = None\n",
    "\n",
    "    model = GraphSAGENet(in_channels=data.x.size(1),\n",
    "                         hidden_channels=hidden,\n",
    "                         num_layers=2,\n",
    "                         edge_dim=(data.edge_attr.size(1) if data.edge_attr is not None else 0)).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    best = {'val_loss': float('inf'), 'model_state': None}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        node_pred, edge_pred, _ = model(data.x, data.edge_index, data.edge_attr)\n",
    "        # compute node loss only on train_mask\n",
    "        loss_node = F.mse_loss(node_pred[train_mask], data.y[train_mask])\n",
    "\n",
    "        if edge_has_target and edge_pred is not None:\n",
    "            loss_edge = F.mse_loss(edge_pred, edge_y)  # all edges\n",
    "            loss = loss_node + loss_edge\n",
    "        else:\n",
    "            loss = loss_node\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            node_pred_val, edge_pred_val, _ = model(data.x, data.edge_index, data.edge_attr)\n",
    "            val_loss_node = F.mse_loss(node_pred_val[val_mask], data.y[val_mask]).item()\n",
    "            if edge_has_target and edge_pred_val is not None:\n",
    "                val_loss_edge = F.mse_loss(edge_pred_val, edge_y).item()\n",
    "                val_loss = val_loss_node + val_loss_edge\n",
    "            else:\n",
    "                val_loss = val_loss_node\n",
    "\n",
    "        if val_loss < best['val_loss']:\n",
    "            best['val_loss'] = val_loss\n",
    "            best['model_state'] = model.state_dict()\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:03d} train_loss={loss.item():.6f} val_loss={val_loss:.6f}\")\n",
    "\n",
    "    # load best\n",
    "    model.load_state_dict(best['model_state'])\n",
    "    print(\"Training finished. Best val loss:\", best['val_loss'])\n",
    "    return model, (train_mask, val_mask, test_mask)\n",
    "\n",
    "# -----------------------------\n",
    "# Inference: add new node & new edges, get predictions\n",
    "# -----------------------------\n",
    "def append_node_and_edges_and_predict(model: GraphSAGENet, data: Data,\n",
    "                                      new_node_features: np.ndarray,\n",
    "                                      new_edges: list,\n",
    "                                      device_map: dict = None,\n",
    "                                      scaler=None):\n",
    "    \"\"\"\n",
    "    new_node_features: 1D numpy array matching data.x columns\n",
    "    new_edges: list of tuples (src_node_id, dst_node_id, edge_attr_array)\n",
    "      - src/dst are node indices or node_ids recognized by device_map: if device_map provided,\n",
    "        keys are node_id strings and values are indices in data.x\n",
    "      - for edges involving the new node, you can use 'NEW' as src or dst to indicate the new node.\n",
    "    Returns:\n",
    "      node_pred_for_new_node (float), edge_preds_for_new_edges (list)\n",
    "    Note: This function constructs a new Data object with appended node and edges for inference.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # current counts\n",
    "    N = data.x.size(0)\n",
    "    E = data.edge_index.size(1)\n",
    "\n",
    "    # map node identifiers\n",
    "    def resolve(idx_or_id):\n",
    "        if isinstance(idx_or_id, int):\n",
    "            return idx_or_id\n",
    "        elif device_map is not None and idx_or_id in device_map:\n",
    "            return device_map[idx_or_id]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown node identifier and no device_map provided\")\n",
    "\n",
    "    # Build new x\n",
    "    x_new = torch.cat([data.x.cpu(), torch.tensor(new_node_features, dtype=torch.float).unsqueeze(0)], dim=0)\n",
    "    # Build new edge_index and edge_attr\n",
    "    edge_idx_list = [data.edge_index.cpu().numpy()[0].tolist(), data.edge_index.cpu().numpy()[1].tolist()]\n",
    "    edge_attr_list = data.edge_attr.cpu().numpy().tolist() if data.edge_attr is not None else []\n",
    "\n",
    "    new_edge_attr_tensors = []\n",
    "    new_edges_pairs = []\n",
    "    for src, dst, edge_attr in new_edges:\n",
    "        # allow 'NEW' to denote new node\n",
    "        if src == 'NEW':\n",
    "            src_idx = N\n",
    "        else:\n",
    "            src_idx = resolve(src)\n",
    "        if dst == 'NEW':\n",
    "            dst_idx = N\n",
    "        else:\n",
    "            dst_idx = resolve(dst)\n",
    "        edge_idx_list[0].append(src_idx)\n",
    "        edge_idx_list[1].append(dst_idx)\n",
    "        edge_attr_list.append(np.array(edge_attr).astype(float))\n",
    "        new_edges_pairs.append((src_idx, dst_idx))\n",
    "\n",
    "    edge_index_new = torch.tensor(edge_idx_list, dtype=torch.long)\n",
    "    edge_attr_new = torch.tensor(np.vstack(edge_attr_list), dtype=torch.float)\n",
    "\n",
    "    data_new = Data(x=x_new, edge_index=edge_index_new, edge_attr=edge_attr_new)\n",
    "    data_new = data_new.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_pred, edge_pred, node_emb = model(data_new.x, data_new.edge_index, data_new.edge_attr)\n",
    "\n",
    "    # new node prediction is at index N\n",
    "    new_node_pred = node_pred[N].cpu().item()\n",
    "\n",
    "    # predictions for newly added edges: find their positions (they are at the tail of edges)\n",
    "    edge_preds = []\n",
    "    total_edges = edge_pred.size(0)\n",
    "    num_added = len(new_edges)\n",
    "    start_idx = total_edges - num_added\n",
    "    for i in range(start_idx, total_edges):\n",
    "        edge_preds.append(edge_pred[i].cpu().item())\n",
    "\n",
    "    return new_node_pred, edge_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d8ca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 train_loss=1026454.062500 val_loss=1025340.817885\n",
      "Epoch 050 train_loss=927151.500000 val_loss=922723.009215\n",
      "Epoch 100 train_loss=739474.437500 val_loss=735683.070139\n",
      "Epoch 150 train_loss=561965.375000 val_loss=558869.256977\n",
      "Epoch 200 train_loss=442996.437500 val_loss=441192.666034\n",
      "Epoch 250 train_loss=358903.625000 val_loss=357128.965681\n",
      "Epoch 300 train_loss=268577.437500 val_loss=266896.846090\n",
      "Training finished. Best val loss: 266896.8460903838\n",
      "Predicted churn_rate for new node: 0.004257898777723312\n",
      "Predicted edge flows for new edges: [-43.83625030517578, 99.27877044677734]\n",
      "Test MSE (node churn_rate): 0.002373273717239499\n"
     ]
    }
   ],
   "source": [
    "# Предполагается, что node_df и edge_df уже подготовлены\n",
    "# node_df.index = node_id\n",
    "# edge_df.index = MultiIndex (src_node, dst_node)\n",
    "# Имеются колонки node_df['text_embedding'], node_df['churn_rate']\n",
    "# Имеется колонка edge_df['transition_count']\n",
    "\n",
    "# --------------------------\n",
    "# 1) Build data\n",
    "# --------------------------\n",
    "# Example:\n",
    "# node_df = pd.read_parquet(\"node_features.parquet\")\n",
    "# edge_df = pd.read_parquet(\"edge_features.parquet\")\n",
    "\n",
    "# Uncomment and load your data here:\n",
    "# node_df = ...\n",
    "# edge_df = ...\n",
    "\n",
    "edge_df = pd.read_csv(\"../../data/edge_df.csv\")\n",
    "node_df = pd.read_parquet(\"../../data/node_semantic_df.parquet\")\n",
    "\n",
    "# Убедимся, что node_df имеет правильный индекс\n",
    "if 'node_id' in node_df.columns and node_df.index.name != 'node_id':\n",
    "    # Если node_id есть в столбцах, но не в индексе\n",
    "    node_df = node_df.set_index('node_id')\n",
    "\n",
    "train_data, scaler, numeric_cols = build_pyg_data_from_dfs(node_df, edge_df,\n",
    "                                                        node_target_col='churn_rate',\n",
    "                                                        edge_target_col='transition_count')\n",
    "\n",
    "# --------------------------\n",
    "# 2) Train\n",
    "# --------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, masks = train_model(train_data, lr=1e-3, epochs=300, val_ratio=0.1, test_ratio=0.1, hidden=64, device=device)\n",
    "\n",
    "# --------------------------\n",
    "# 3) Inference example: add a new node and edge(s)\n",
    "# --------------------------\n",
    "# new_node_features must be same length as data.x.shape[1]\n",
    "# Strategy to build new_node_features:\n",
    "# - If you have embedding and numeric features prepared, concatenate them\n",
    "# - If not, use mean of neighbor nodes features as initialization (quick hack)\n",
    "#\n",
    "# Quick hack: use mean of existing X rows\n",
    "new_node_features = train_data.x.cpu().mean(dim=0).numpy()  # naive init; replace with real features if available\n",
    "\n",
    "# Build edge_attr sample: must match existing edge_attr columns count\n",
    "E_attr_dim = train_data.edge_attr.size(1) if train_data.edge_attr is not None else 0\n",
    "sample_edge_attr = np.zeros(E_attr_dim, dtype=float)\n",
    "# Suppose we connect new node from an existing node with index 0\n",
    "new_edges = [\n",
    "    (0, 'NEW', sample_edge_attr),    # edge from node index 0 -> NEW\n",
    "    ('NEW', 1, sample_edge_attr)     # edge NEW -> node index 1\n",
    "]\n",
    "\n",
    "# For device_map usage (if you want to refer to nodes by node_id string),\n",
    "# pass data.node_id_map as device_map argument.\n",
    "new_node_pred, new_edge_preds = append_node_and_edges_and_predict(model, train_data,\n",
    "                                                                    new_node_features=new_node_features,\n",
    "                                                                    new_edges=new_edges,\n",
    "                                                                    device_map=None,\n",
    "                                                                    scaler=scaler)\n",
    "\n",
    "print(\"Predicted churn_rate for new node:\", new_node_pred)\n",
    "print(\"Predicted edge flows for new edges:\", new_edge_preds)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Evaluate on test set (optional)\n",
    "# --------------------------\n",
    "train_mask, val_mask, test_mask = masks\n",
    "device = next(model.parameters()).device\n",
    "data = train_data.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    node_pred_all, edge_pred_all, _ = model(data.x, data.edge_index, data.edge_attr)\n",
    "    test_mse = F.mse_loss(node_pred_all[test_mask], data.y[test_mask]).item()\n",
    "print(\"Test MSE (node churn_rate):\", test_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d389a",
   "metadata": {},
   "source": [
    "## ✅ 7. Обучение GraphSAGE на предсказание churn_rate\n",
    "\n",
    "Здесь мы обучаем регрессию (предсказание churn_rate), используем:\n",
    "\n",
    "- train/val/test split\n",
    "- HuberLoss (более устойчива к выбросам)\n",
    "- Adam\n",
    "- эпохи, вывод MSE/MAE\n",
    "\n",
    "Работает с объектом graph, который был построен на шаге 1 и 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65194ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_gnn_regression.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "\n",
    "def train_val_test_split(num_nodes, test_size=0.15, val_size=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Разбиваем индексы нод на train/val/test.\n",
    "    \"\"\"\n",
    "    all_idx = list(range(num_nodes))\n",
    "\n",
    "    train_idx, test_idx = train_test_split(all_idx, test_size=test_size, random_state=seed)\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=val_size, random_state=seed)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(train_idx, dtype=torch.long),\n",
    "        torch.tensor(val_idx, dtype=torch.long),\n",
    "        torch.tensor(test_idx, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def train_graphsage_regression_fullbatch(\n",
    "    graph,\n",
    "    model,\n",
    "    epochs=50,\n",
    "    lr=0.001,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    graph = graph.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    num_nodes = graph.num_nodes\n",
    "    train_idx, val_idx, test_idx = train_val_test_split(num_nodes)\n",
    "\n",
    "    loss_fn = nn.HuberLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    x = graph.x\n",
    "    edge_index = graph.edge_index\n",
    "    y = graph.y\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ::::::::::::::: TRAIN :::::::::::::::\n",
    "        model.train()\n",
    "        preds = model(x, edge_index)[0].squeeze()\n",
    "        loss = loss_fn(preds[train_idx], y[train_idx])\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # ::::::::::::::: VAL :::::::::::::::\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(x, edge_index)[0].squeeze()\n",
    "            val_loss = loss_fn(val_pred[val_idx], y[val_idx]).item()\n",
    "            val_mae = (val_pred[val_idx] - y[val_idx]).abs().mean().item()\n",
    "            val_mape = mean_absolute_percentage_error(y[val_idx].cpu(), val_pred[val_idx].cpu())\n",
    "            val_r2 = r2_score(y[val_idx].cpu(), val_pred[val_idx].cpu())\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train={loss.item():.4f} | Val={val_loss:.4f} | \"\n",
    "              f\"MAE={val_mae:.4f} | MAPE={val_mape:.4f} | R2={val_r2:.4f}\")\n",
    "\n",
    "    # ::::::::::::::: TEST :::::::::::::::\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(x, edge_index)[0].squeeze()\n",
    "        test_loss = loss_fn(test_pred[test_idx], y[test_idx]).item()\n",
    "        test_mae = (test_pred[test_idx] - y[test_idx]).abs().mean().item()\n",
    "        test_mape = mean_absolute_percentage_error(y[test_idx].cpu(), test_pred[test_idx].cpu())\n",
    "        test_r2 = r2_score(y[test_idx].cpu(), test_pred[test_idx].cpu())\n",
    "\n",
    "    print(\"\\n===== FINAL TEST =====\")\n",
    "    print(f\"Test Loss = {test_loss:.4f} | MAE = {test_mae:.4f} | MAPE = {test_mape:.4f} | R2 = {test_r2:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0f0de",
   "metadata": {},
   "source": [
    "###  Загрузить граф из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82885069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Загрузка всей структуры\n",
    "def load_graph_data(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "    \n",
    "    graph = loaded_data['graph']\n",
    "    node_id_map = loaded_data['node_id_map']\n",
    "    scaler = loaded_data['scaler']\n",
    "    numeric_cols = loaded_data['numeric_cols']\n",
    "    \n",
    "    return graph, node_id_map, scaler, numeric_cols\n",
    "\n",
    "# В другом ноутбуке или позже загружаем\n",
    "graph, loaded_node_id_map, loaded_scaler, loaded_numeric_cols = load_graph_data('../../data/graph_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c07e7",
   "metadata": {},
   "source": [
    "###  Как запускать обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a462c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train=0.0071 | Val=0.0014 | MAE=0.0490 | MAPE=117302443407481.5000 | R2=-18.6447\n",
      "Epoch 002 | Train=0.0026 | Val=0.0003 | MAE=0.0195 | MAPE=57271646035486.4297 | R2=-3.1399\n",
      "Epoch 003 | Train=0.0014 | Val=0.0009 | MAE=0.0329 | MAPE=118827662087830.8750 | R2=-11.6854\n",
      "Epoch 004 | Train=0.0019 | Val=0.0014 | MAE=0.0443 | MAPE=151556226748175.5312 | R2=-18.7227\n",
      "Epoch 005 | Train=0.0020 | Val=0.0012 | MAE=0.0401 | MAPE=142309308220356.1875 | R2=-15.8888\n",
      "Epoch 006 | Train=0.0015 | Val=0.0008 | MAE=0.0291 | MAPE=113796642797688.6562 | R2=-9.8905\n",
      "Epoch 007 | Train=0.0010 | Val=0.0004 | MAE=0.0194 | MAPE=80633684434462.1719 | R2=-5.1547\n",
      "Epoch 008 | Train=0.0007 | Val=0.0003 | MAE=0.0168 | MAPE=52205059047424.2188 | R2=-3.1130\n",
      "Epoch 009 | Train=0.0007 | Val=0.0003 | MAE=0.0168 | MAPE=38146209958731.6641 | R2=-2.8881\n",
      "Epoch 010 | Train=0.0008 | Val=0.0003 | MAE=0.0194 | MAPE=41976519427975.9688 | R2=-3.3281\n",
      "Epoch 011 | Train=0.0008 | Val=0.0003 | MAE=0.0205 | MAPE=44842754799736.9375 | R2=-3.5214\n",
      "Epoch 012 | Train=0.0008 | Val=0.0003 | MAE=0.0195 | MAPE=44101533449276.6641 | R2=-3.3301\n",
      "Epoch 013 | Train=0.0007 | Val=0.0003 | MAE=0.0182 | MAPE=45609143947987.1797 | R2=-3.1507\n",
      "Epoch 014 | Train=0.0006 | Val=0.0003 | MAE=0.0173 | MAPE=51276515145848.7344 | R2=-3.4363\n",
      "Epoch 015 | Train=0.0005 | Val=0.0004 | MAE=0.0174 | MAPE=62442620861500.3906 | R2=-4.4650\n",
      "Epoch 016 | Train=0.0004 | Val=0.0005 | MAE=0.0197 | MAPE=78742242814313.5156 | R2=-6.1009\n",
      "Epoch 017 | Train=0.0004 | Val=0.0006 | MAE=0.0233 | MAPE=92366335022742.7031 | R2=-7.8913\n",
      "Epoch 018 | Train=0.0004 | Val=0.0007 | MAE=0.0257 | MAPE=101001872061620.8594 | R2=-9.1129\n",
      "Epoch 019 | Train=0.0004 | Val=0.0007 | MAE=0.0260 | MAPE=101706605927966.2656 | R2=-9.2900\n",
      "Epoch 020 | Train=0.0004 | Val=0.0007 | MAE=0.0242 | MAPE=95337047053010.9688 | R2=-8.4775\n",
      "Epoch 021 | Train=0.0004 | Val=0.0006 | MAE=0.0214 | MAPE=85487094194657.9844 | R2=-7.1182\n",
      "Epoch 022 | Train=0.0003 | Val=0.0005 | MAE=0.0187 | MAPE=73732698360651.4062 | R2=-5.7587\n",
      "Epoch 023 | Train=0.0002 | Val=0.0004 | MAE=0.0169 | MAPE=62611111481103.1875 | R2=-4.7142\n",
      "Epoch 024 | Train=0.0002 | Val=0.0004 | MAE=0.0173 | MAPE=58221009475222.7812 | R2=-4.1236\n",
      "Epoch 025 | Train=0.0002 | Val=0.0003 | MAE=0.0181 | MAPE=57733430023830.8125 | R2=-3.8443\n",
      "Epoch 026 | Train=0.0002 | Val=0.0003 | MAE=0.0185 | MAPE=57442921672945.1875 | R2=-3.7116\n",
      "Epoch 027 | Train=0.0002 | Val=0.0003 | MAE=0.0185 | MAPE=57313626577498.6016 | R2=-3.6960\n",
      "Epoch 028 | Train=0.0002 | Val=0.0003 | MAE=0.0182 | MAPE=57556177749895.7578 | R2=-3.7936\n",
      "Epoch 029 | Train=0.0002 | Val=0.0004 | MAE=0.0179 | MAPE=59121229488610.0703 | R2=-4.0211\n",
      "Epoch 030 | Train=0.0001 | Val=0.0004 | MAE=0.0174 | MAPE=61515594804525.3203 | R2=-4.3984\n",
      "Epoch 031 | Train=0.0001 | Val=0.0004 | MAE=0.0173 | MAPE=65392158071386.4609 | R2=-4.9108\n",
      "Epoch 032 | Train=0.0001 | Val=0.0005 | MAE=0.0180 | MAPE=70108724962725.7500 | R2=-5.5066\n",
      "Epoch 033 | Train=0.0001 | Val=0.0005 | MAE=0.0189 | MAPE=74547022975698.9219 | R2=-5.9662\n",
      "Epoch 034 | Train=0.0001 | Val=0.0005 | MAE=0.0193 | MAPE=76292151482006.7031 | R2=-6.1154\n",
      "Epoch 035 | Train=0.0001 | Val=0.0005 | MAE=0.0190 | MAPE=75096411664625.0469 | R2=-5.9110\n",
      "Epoch 036 | Train=0.0001 | Val=0.0005 | MAE=0.0182 | MAPE=71398877082804.8125 | R2=-5.4511\n",
      "Epoch 037 | Train=0.0001 | Val=0.0004 | MAE=0.0176 | MAPE=67499822477553.0625 | R2=-4.8973\n",
      "Epoch 038 | Train=0.0001 | Val=0.0004 | MAE=0.0171 | MAPE=63889975495499.4062 | R2=-4.3643\n",
      "Epoch 039 | Train=0.0001 | Val=0.0003 | MAE=0.0170 | MAPE=60623982489841.0703 | R2=-3.9348\n",
      "Epoch 040 | Train=0.0001 | Val=0.0003 | MAE=0.0169 | MAPE=58254585618914.0391 | R2=-3.6476\n",
      "\n",
      "===== FINAL TEST =====\n",
      "Test Loss = 0.0002 | MAE = 0.0139 | MAPE = 25071288382415.4336 | R2 = 0.0621\n"
     ]
    }
   ],
   "source": [
    "# from gnn_models import GraphSAGENet\n",
    "# from train_gnn_regression import train_graphsage_regression\n",
    "\n",
    "# graph = Data(...) — то, что мы собрали ранее\n",
    "\n",
    "model = GraphSAGENet(\n",
    "    in_channels=graph.x.size(1),\n",
    "    hidden_channels=128,\n",
    "    # out_channels=1,\n",
    "    num_layers=2,\n",
    "    edge_dim=graph.edge_attr.size(1)\n",
    ")\n",
    "\n",
    "# trained_model = train_graphsage_regression(\n",
    "#     graph=graph,\n",
    "#     model=model,\n",
    "#     epochs=40,\n",
    "#     batch_size=64,\n",
    "#     lr=0.001,\n",
    "#     device='cpu'  # если GPU нет → 'cpu'\n",
    "# )\n",
    "\n",
    "trained_model = train_graphsage_regression_fullbatch(\n",
    "    graph=graph,\n",
    "    model=model,\n",
    "    epochs=40,\n",
    "    lr=0.001,\n",
    "    device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f042e8",
   "metadata": {},
   "source": [
    "### Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bbd1c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полная модель сохранена в trained_graphsage.pth\n"
     ]
    }
   ],
   "source": [
    "# В первом ноутбуке\n",
    "def save_complete_model(model, scaler, node_id_map, filepath='complete_model.pth'):\n",
    "    \"\"\"\n",
    "    Сохраняет модель и все необходимые для работы объекты\n",
    "    \"\"\"\n",
    "    complete_data = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_params': {\n",
    "            'in_channels': model.in_channels,\n",
    "            'hidden_channels': model.hidden_channels,\n",
    "            'num_layers': model.num_layers,\n",
    "            'edge_dim': model.edge_dim\n",
    "        },\n",
    "        'scaler': scaler,\n",
    "        'node_id_map': node_id_map,\n",
    "        'model_class': model.__class__.__name__\n",
    "    }\n",
    "    \n",
    "    torch.save(complete_data, filepath)\n",
    "    print(f\"Полная модель сохранена в {filepath}\")\n",
    "\n",
    "# Использование\n",
    "node_id_map = train_data.node_id_map\n",
    "save_complete_model(trained_model, scaler, node_id_map, 'trained_graphsage.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
