{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "result_df = pd.read_csv(\"../../data/clean_data.100k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e2631",
   "metadata": {},
   "source": [
    "## 4. –ú–æ–¥–µ–ª—å GNN (GraphSAGE).\n",
    "\n",
    "–ú–æ–¥–µ–ª—å GraphSAGE (PyG) —Å –æ–±—É—á–µ–Ω–∏–µ–º –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º.\n",
    "–í–∫–ª—é—á—ë–Ω –∏ node-level head (–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç churn_rate) –∏ edge-level head (–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç transition_count / flow).\n",
    "–ö–æ–¥ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ —É –Ω–∞—Å –≤ –Ω–∞–ª–∏—á–∏–∏:\n",
    "\n",
    "- `node_df` ‚Äî DataFrame –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–¥ (–∏–Ω–¥–µ–∫—Å = node_id), –≤ –∫–æ—Ç–æ—Ä–æ–º –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ text_embedding (np.array) –∏ –∫–æ–ª–æ–Ω–∫–∞-—Ç–∞—Ä–≥–µ—Ç churn_rate.\n",
    "\n",
    "- `edge_df` ‚Äî DataFrame –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–±–µ—Ä, –∏–Ω–¥–µ–∫—Å = (src_node, dst_node), —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–æ–≤—ã–µ edge-—Ñ–∏—á–∏ –∏ –∫–æ–ª–æ–Ω–∫—É transition_count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b850519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 train_loss=324698.500000 val_loss=324241.193476\n",
      "Epoch 050 train_loss=276987.687500 val_loss=275422.702426\n",
      "Epoch 100 train_loss=200794.500000 val_loss=199357.622091\n",
      "Epoch 150 train_loss=145250.796875 val_loss=144521.742381\n",
      "Epoch 200 train_loss=111651.218750 val_loss=110930.779796\n",
      "Epoch 250 train_loss=75849.648438 val_loss=75142.252204\n",
      "Epoch 300 train_loss=46587.550781 val_loss=46190.166986\n",
      "Training finished. Best val loss: 46190.1669860743\n",
      "Predicted churn_rate for new node: -0.002679973840713501\n",
      "Predicted edge flows for new edges: [-11.576274871826172, -3.8096964359283447]\n",
      "Test MSE (node churn_rate): 0.0785408467054367\n"
     ]
    }
   ],
   "source": [
    "# gnn_training.py\n",
    "# GraphSAGE for node-level (churn_rate) and edge-level (transition_count) regression\n",
    "# Requires: torch, torch_geometric, scikit-learn, numpy, pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: build PyG Data (if not already built)\n",
    "# -----------------------------\n",
    "def build_pyg_data_from_dfs(node_df: pd.DataFrame, edge_df: pd.DataFrame,\n",
    "                            node_target_col='churn_rate', edge_target_col='transition_count'):\n",
    "    \"\"\"\n",
    "    node_df.index -> node_id\n",
    "    node_df contains text_embedding column (np.array) and numeric/categorical columns\n",
    "    edge_df.index -> (src_node, dst_node)\n",
    "    \"\"\"\n",
    "    # Map node_id -> idx\n",
    "    node_ids = list(node_df.index)\n",
    "    node_id_map = {nid: i for i, nid in enumerate(node_ids)}\n",
    "\n",
    "    # Prepare node numeric features (exclude target and keep embeddings)\n",
    "    # Extract embedding\n",
    "    if 'text_embedding' in node_df.columns:\n",
    "        emb = np.vstack(node_df['text_embedding'].values).astype(float)\n",
    "        node_df_wo_emb = node_df.drop(columns=['text_embedding'])\n",
    "    else:\n",
    "        emb = np.zeros((len(node_df), 0))\n",
    "        node_df_wo_emb = node_df.copy()\n",
    "\n",
    "    # Identify categorical/object columns and encode to codes\n",
    "    node_df_enc = node_df_wo_emb.copy()\n",
    "    cat_cols = []\n",
    "    for c in node_df_enc.columns:\n",
    "        if node_df_enc[c].dtype == object or str(node_df_enc[c].dtype).startswith('category'):\n",
    "            cat_cols.append(c)\n",
    "            node_df_enc[c] = node_df_enc[c].astype('category').cat.codes\n",
    "\n",
    "    # Numeric columns (exclude target)\n",
    "    num_cols = [c for c in node_df_enc.columns if c != node_target_col]\n",
    "    X_num = node_df_enc[num_cols].astype(float).fillna(0).values\n",
    "\n",
    "    # Scale numeric\n",
    "    scaler = StandardScaler()\n",
    "    if X_num.size > 0:\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "    X = np.concatenate([X_num, emb], axis=1) if emb.size else X_num\n",
    "\n",
    "    x = torch.tensor(X, dtype=torch.float)\n",
    "\n",
    "    # node target y\n",
    "    y_node = torch.tensor(node_df[node_target_col].astype(float).values, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    # Edge index and attributes\n",
    "    # edge_df index should be MultiIndex (src, dst) OR columns src_node/dst_node\n",
    "    if isinstance(edge_df.index, pd.MultiIndex) and edge_df.index.nlevels == 2:\n",
    "        srcs = [node_id_map[s] for s, d in edge_df.index]\n",
    "        dsts = [node_id_map[d] for s, d in edge_df.index]\n",
    "    else:\n",
    "        # try columns\n",
    "        if {'src_node','dst_node'}.issubset(edge_df.columns):\n",
    "            srcs = [node_id_map[s] for s in edge_df['src_node'].values]\n",
    "            dsts = [node_id_map[d] for d in edge_df['dst_node'].values]\n",
    "        else:\n",
    "            raise ValueError(\"edge_df index must be MultiIndex (src,dst) or contain src_node/dst_node columns\")\n",
    "\n",
    "    edge_index = torch.tensor([srcs, dsts], dtype=torch.long)\n",
    "\n",
    "    # Edge attributes (drop src/dst columns if present and target if present)\n",
    "    edge_df_local = edge_df.copy()\n",
    "    for c in ['src_node','dst_node', edge_target_col]:\n",
    "        if c in edge_df_local.columns:\n",
    "            edge_df_local = edge_df_local.drop(columns=[c])\n",
    "    # encode categorical edge cols\n",
    "    for c in edge_df_local.columns:\n",
    "        if edge_df_local[c].dtype == object or str(edge_df_local[c].dtype).startswith('category'):\n",
    "            edge_df_local[c] = edge_df_local[c].astype('category').cat.codes\n",
    "    edge_attr = torch.tensor(edge_df_local.fillna(0).astype(float).values, dtype=torch.float)\n",
    "\n",
    "    # Edge target (for training edge head)\n",
    "    if edge_target_col in edge_df.columns:\n",
    "        edge_y = torch.tensor(edge_df[edge_target_col].astype(float).values, dtype=torch.float).unsqueeze(1)\n",
    "    else:\n",
    "        edge_y = None\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y_node)\n",
    "    data.edge_y = edge_y  # custom attribute for edge targets\n",
    "    data.node_id_map = node_id_map\n",
    "\n",
    "    return data, scaler, num_cols\n",
    "\n",
    "# -----------------------------\n",
    "# Model: GraphSAGE encoder + node regression head + edge regression head\n",
    "# -----------------------------\n",
    "class GraphSAGENet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, num_layers=2, edge_dim=0):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers-1):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "\n",
    "        # Node regression head\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels//2, 1)  # churn_rate scalar\n",
    "        )\n",
    "\n",
    "        # Edge regression head: we'll use concatenation of src_emb || dst_emb || edge_attr\n",
    "        self.edge_dim = edge_dim\n",
    "        edge_input_dim = hidden_channels * 2 + edge_dim\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_input_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, 1)  # transition_count (or normalized)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        # x: [N, in_channels], edge_index: [2, E], edge_attr: [E, edge_dim]\n",
    "        h = x\n",
    "        for conv in self.convs:\n",
    "            h = conv(h, edge_index)\n",
    "            h = F.relu(h)\n",
    "\n",
    "        # node preds\n",
    "        node_pred = self.node_mlp(h)  # [N, 1]\n",
    "\n",
    "        # edge preds\n",
    "        if edge_attr is not None:\n",
    "            src_idx = edge_index[0]\n",
    "            dst_idx = edge_index[1]\n",
    "            src_h = h[src_idx]\n",
    "            dst_h = h[dst_idx]\n",
    "            edge_input = torch.cat([src_h, dst_h, edge_attr], dim=1)\n",
    "            edge_pred = self.edge_mlp(edge_input)  # [E, 1]\n",
    "        else:\n",
    "            edge_pred = None\n",
    "\n",
    "        return node_pred, edge_pred, h  # also return node embeddings\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "def train_model(data: Data,\n",
    "                lr=1e-3,\n",
    "                epochs=200,\n",
    "                val_ratio=0.1,\n",
    "                test_ratio=0.1,\n",
    "                hidden=64,\n",
    "                device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Train GraphSAGENet on Data.\n",
    "    data.edge_y can be None (skip edge head training).\n",
    "    Returns trained model and scalers/maps for later inference.\n",
    "    \"\"\"\n",
    "    # Move data to device\n",
    "    data = data.clone()\n",
    "    data = data.to(device)\n",
    "    N = data.num_nodes\n",
    "    E = data.edge_index.shape[1]\n",
    "\n",
    "    # Train/val/test split on nodes (node-level supervised)\n",
    "    idx = np.arange(N)\n",
    "    idx_train, idx_tmp = train_test_split(idx, test_size=(val_ratio+test_ratio), random_state=42)\n",
    "    relative_val = val_ratio / (val_ratio + test_ratio)\n",
    "    idx_val, idx_test = train_test_split(idx_tmp, test_size=(1-relative_val), random_state=42)\n",
    "\n",
    "    train_mask = torch.zeros(N, dtype=torch.bool, device=device)\n",
    "    val_mask = torch.zeros(N, dtype=torch.bool, device=device)\n",
    "    test_mask = torch.zeros(N, dtype=torch.bool, device=device)\n",
    "    train_mask[idx_train] = True\n",
    "    val_mask[idx_val] = True\n",
    "    test_mask[idx_test] = True\n",
    "\n",
    "    # If edge target exists, train on all edges (no edge split here for simplicity)\n",
    "    edge_has_target = getattr(data, 'edge_y', None) is not None\n",
    "    if edge_has_target:\n",
    "        edge_y = data.edge_y.to(device)\n",
    "    else:\n",
    "        edge_y = None\n",
    "\n",
    "    model = GraphSAGENet(in_channels=data.x.size(1),\n",
    "                         hidden_channels=hidden,\n",
    "                         num_layers=2,\n",
    "                         edge_dim=(data.edge_attr.size(1) if data.edge_attr is not None else 0)).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    best = {'val_loss': float('inf'), 'model_state': None}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        node_pred, edge_pred, _ = model(data.x, data.edge_index, data.edge_attr)\n",
    "        # compute node loss only on train_mask\n",
    "        loss_node = F.mse_loss(node_pred[train_mask], data.y[train_mask])\n",
    "\n",
    "        if edge_has_target and edge_pred is not None:\n",
    "            loss_edge = F.mse_loss(edge_pred, edge_y)  # all edges\n",
    "            loss = loss_node + loss_edge\n",
    "        else:\n",
    "            loss = loss_node\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            node_pred_val, edge_pred_val, _ = model(data.x, data.edge_index, data.edge_attr)\n",
    "            val_loss_node = F.mse_loss(node_pred_val[val_mask], data.y[val_mask]).item()\n",
    "            if edge_has_target and edge_pred_val is not None:\n",
    "                val_loss_edge = F.mse_loss(edge_pred_val, edge_y).item()\n",
    "                val_loss = val_loss_node + val_loss_edge\n",
    "            else:\n",
    "                val_loss = val_loss_node\n",
    "\n",
    "        if val_loss < best['val_loss']:\n",
    "            best['val_loss'] = val_loss\n",
    "            best['model_state'] = model.state_dict()\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:03d} train_loss={loss.item():.6f} val_loss={val_loss:.6f}\")\n",
    "\n",
    "    # load best\n",
    "    model.load_state_dict(best['model_state'])\n",
    "    print(\"Training finished. Best val loss:\", best['val_loss'])\n",
    "    return model, (train_mask, val_mask, test_mask)\n",
    "\n",
    "# -----------------------------\n",
    "# Inference: add new node & new edges, get predictions\n",
    "# -----------------------------\n",
    "def append_node_and_edges_and_predict(model: GraphSAGENet, data: Data,\n",
    "                                      new_node_features: np.ndarray,\n",
    "                                      new_edges: list,\n",
    "                                      device_map: dict = None,\n",
    "                                      scaler=None):\n",
    "    \"\"\"\n",
    "    new_node_features: 1D numpy array matching data.x columns\n",
    "    new_edges: list of tuples (src_node_id, dst_node_id, edge_attr_array)\n",
    "      - src/dst are node indices or node_ids recognized by device_map: if device_map provided,\n",
    "        keys are node_id strings and values are indices in data.x\n",
    "      - for edges involving the new node, you can use 'NEW' as src or dst to indicate the new node.\n",
    "    Returns:\n",
    "      node_pred_for_new_node (float), edge_preds_for_new_edges (list)\n",
    "    Note: This function constructs a new Data object with appended node and edges for inference.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # current counts\n",
    "    N = data.x.size(0)\n",
    "    E = data.edge_index.size(1)\n",
    "\n",
    "    # map node identifiers\n",
    "    def resolve(idx_or_id):\n",
    "        if isinstance(idx_or_id, int):\n",
    "            return idx_or_id\n",
    "        elif device_map is not None and idx_or_id in device_map:\n",
    "            return device_map[idx_or_id]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown node identifier and no device_map provided\")\n",
    "\n",
    "    # Build new x\n",
    "    x_new = torch.cat([data.x.cpu(), torch.tensor(new_node_features, dtype=torch.float).unsqueeze(0)], dim=0)\n",
    "    # Build new edge_index and edge_attr\n",
    "    edge_idx_list = [data.edge_index.cpu().numpy()[0].tolist(), data.edge_index.cpu().numpy()[1].tolist()]\n",
    "    edge_attr_list = data.edge_attr.cpu().numpy().tolist() if data.edge_attr is not None else []\n",
    "\n",
    "    new_edge_attr_tensors = []\n",
    "    new_edges_pairs = []\n",
    "    for src, dst, edge_attr in new_edges:\n",
    "        # allow 'NEW' to denote new node\n",
    "        if src == 'NEW':\n",
    "            src_idx = N\n",
    "        else:\n",
    "            src_idx = resolve(src)\n",
    "        if dst == 'NEW':\n",
    "            dst_idx = N\n",
    "        else:\n",
    "            dst_idx = resolve(dst)\n",
    "        edge_idx_list[0].append(src_idx)\n",
    "        edge_idx_list[1].append(dst_idx)\n",
    "        edge_attr_list.append(np.array(edge_attr).astype(float))\n",
    "        new_edges_pairs.append((src_idx, dst_idx))\n",
    "\n",
    "    edge_index_new = torch.tensor(edge_idx_list, dtype=torch.long)\n",
    "    edge_attr_new = torch.tensor(np.vstack(edge_attr_list), dtype=torch.float)\n",
    "\n",
    "    data_new = Data(x=x_new, edge_index=edge_index_new, edge_attr=edge_attr_new)\n",
    "    data_new = data_new.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_pred, edge_pred, node_emb = model(data_new.x, data_new.edge_index, data_new.edge_attr)\n",
    "\n",
    "    # new node prediction is at index N\n",
    "    new_node_pred = node_pred[N].cpu().item()\n",
    "\n",
    "    # predictions for newly added edges: find their positions (they are at the tail of edges)\n",
    "    edge_preds = []\n",
    "    total_edges = edge_pred.size(0)\n",
    "    num_added = len(new_edges)\n",
    "    start_idx = total_edges - num_added\n",
    "    for i in range(start_idx, total_edges):\n",
    "        edge_preds.append(edge_pred[i].cpu().item())\n",
    "\n",
    "    return new_node_pred, edge_preds\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage (put in main cell)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ node_df –∏ edge_df —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã\n",
    "    # node_df.index = node_id\n",
    "    # edge_df.index = MultiIndex (src_node, dst_node)\n",
    "    # –ò–º–µ—é—Ç—Å—è –∫–æ–ª–æ–Ω–∫–∏ node_df['text_embedding'], node_df['churn_rate']\n",
    "    # –ò–º–µ–µ—Ç—Å—è –∫–æ–ª–æ–Ω–∫–∞ edge_df['transition_count']\n",
    "\n",
    "    # --------------------------\n",
    "    # 1) Build data\n",
    "    # --------------------------\n",
    "    # Example:\n",
    "    # node_df = pd.read_parquet(\"node_features.parquet\")\n",
    "    # edge_df = pd.read_parquet(\"edge_features.parquet\")\n",
    "\n",
    "    # Uncomment and load your data here:\n",
    "    # node_df = ...\n",
    "    # edge_df = ...\n",
    "\n",
    "    # For safety, here's a quick guard:\n",
    "    try:\n",
    "        node_df  # noqa\n",
    "        edge_df  # noqa\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"You must provide node_df and edge_df variables before running this script.\")\n",
    "\n",
    "    data, scaler, numeric_cols = build_pyg_data_from_dfs(node_df, edge_df,\n",
    "                                                         node_target_col='churn_rate',\n",
    "                                                         edge_target_col='transition_count')\n",
    "\n",
    "    # --------------------------\n",
    "    # 2) Train\n",
    "    # --------------------------\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model, masks = train_model(data, lr=1e-3, epochs=300, val_ratio=0.1, test_ratio=0.1, hidden=64, device=device)\n",
    "\n",
    "    # --------------------------\n",
    "    # 3) Inference example: add a new node and edge(s)\n",
    "    # --------------------------\n",
    "    # new_node_features must be same length as data.x.shape[1]\n",
    "    # Strategy to build new_node_features:\n",
    "    # - If you have embedding and numeric features prepared, concatenate them\n",
    "    # - If not, use mean of neighbor nodes features as initialization (quick hack)\n",
    "    #\n",
    "    # Quick hack: use mean of existing X rows\n",
    "    new_node_features = data.x.cpu().mean(dim=0).numpy()  # naive init; replace with real features if available\n",
    "\n",
    "    # Build edge_attr sample: must match existing edge_attr columns count\n",
    "    E_attr_dim = data.edge_attr.size(1) if data.edge_attr is not None else 0\n",
    "    sample_edge_attr = np.zeros(E_attr_dim, dtype=float)\n",
    "    # Suppose we connect new node from an existing node with index 0\n",
    "    new_edges = [\n",
    "        (0, 'NEW', sample_edge_attr),    # edge from node index 0 -> NEW\n",
    "        ('NEW', 1, sample_edge_attr)     # edge NEW -> node index 1\n",
    "    ]\n",
    "\n",
    "    # For device_map usage (if you want to refer to nodes by node_id string),\n",
    "    # pass data.node_id_map as device_map argument.\n",
    "    new_node_pred, new_edge_preds = append_node_and_edges_and_predict(model, data,\n",
    "                                                                      new_node_features=new_node_features,\n",
    "                                                                      new_edges=new_edges,\n",
    "                                                                      device_map=None,\n",
    "                                                                      scaler=scaler)\n",
    "\n",
    "    print(\"Predicted churn_rate for new node:\", new_node_pred)\n",
    "    print(\"Predicted edge flows for new edges:\", new_edge_preds)\n",
    "\n",
    "    # --------------------------\n",
    "    # 4) Evaluate on test set (optional)\n",
    "    # --------------------------\n",
    "    train_mask, val_mask, test_mask = masks\n",
    "    device = next(model.parameters()).device\n",
    "    data = data.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_pred_all, edge_pred_all, _ = model(data.x, data.edge_index, data.edge_attr)\n",
    "        test_mse = F.mse_loss(node_pred_all[test_mask], data.y[test_mask]).item()\n",
    "    print(\"Test MSE (node churn_rate):\", test_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d389a",
   "metadata": {},
   "source": [
    "## ‚úÖ 7. –û–±—É—á–µ–Ω–∏–µ GraphSAGE –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ churn_rate\n",
    "\n",
    "–ó–¥–µ—Å—å –º—ã –æ–±—É—á–∞–µ–º —Ä–µ–≥—Ä–µ—Å—Å–∏—é (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ churn_rate), –∏—Å–ø–æ–ª—å–∑—É–µ–º:\n",
    "\n",
    "- train/val/test split\n",
    "- HuberLoss (–±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º)\n",
    "- Adam\n",
    "- —ç–ø–æ—Ö–∏, –≤—ã–≤–æ–¥ MSE/MAE\n",
    "\n",
    "–†–∞–±–æ—Ç–∞–µ—Ç —Å –æ–±—ä–µ–∫—Ç–æ–º graph, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª –ø–æ—Å—Ç—Ä–æ–µ–Ω –Ω–∞ —à–∞–≥–µ 1 –∏ 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65194ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_gnn_regression.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_val_test_split(num_nodes, test_size=0.15, val_size=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    –†–∞–∑–±–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –Ω–æ–¥ –Ω–∞ train/val/test.\n",
    "    \"\"\"\n",
    "    all_idx = list(range(num_nodes))\n",
    "\n",
    "    train_idx, test_idx = train_test_split(all_idx, test_size=test_size, random_state=seed)\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=val_size, random_state=seed)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(train_idx, dtype=torch.long),\n",
    "        torch.tensor(val_idx, dtype=torch.long),\n",
    "        torch.tensor(test_idx, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "\n",
    "def train_graphsage_regression_fullbatch(\n",
    "    graph,\n",
    "    model,\n",
    "    epochs=50,\n",
    "    lr=0.001,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    graph = graph.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    num_nodes = graph.num_nodes\n",
    "    train_idx, val_idx, test_idx = train_val_test_split(num_nodes)\n",
    "\n",
    "    loss_fn = nn.HuberLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    x = graph.x\n",
    "    edge_index = graph.edge_index\n",
    "    y = graph.y\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ::::::::::::::: TRAIN :::::::::::::::\n",
    "        model.train()\n",
    "        # preds = model(x, edge_index).squeeze()\n",
    "        preds = model(x, edge_index)\n",
    "        # print('l[0]', l[0])\n",
    "        # print(type(l[0]))\n",
    "        preds = preds[0].squeeze()\n",
    "\n",
    "        loss = loss_fn(preds[train_idx], y[train_idx])\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # ::::::::::::::: VAL :::::::::::::::\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(x, edge_index)[0].squeeze()\n",
    "\n",
    "            val_loss = loss_fn(val_pred[val_idx], y[val_idx]).item()\n",
    "            val_mae = (val_pred[val_idx] - y[val_idx]).abs().mean().item()\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train={loss.item():.4f} | Val={val_loss:.4f} | MAE={val_mae:.4f}\")\n",
    "\n",
    "    # ::::::::::::::: TEST :::::::::::::::\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(x, edge_index)[0].squeeze()\n",
    "\n",
    "        test_loss = loss_fn(test_pred[test_idx], y[test_idx]).item()\n",
    "        test_mae = (test_pred[test_idx] - y[test_idx]).abs().mean().item()\n",
    "\n",
    "    print(\"\\n===== FINAL TEST =====\")\n",
    "    print(f\"Test Loss = {test_loss:.4f}\")\n",
    "    print(f\"Test MAE  = {test_mae:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c07e7",
   "metadata": {},
   "source": [
    "### üìò –ö–∞–∫ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a462c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train=0.0277 | Val=0.0065 | MAE=0.0668\n",
      "Epoch 002 | Train=0.0160 | Val=0.0095 | MAE=0.1178\n",
      "Epoch 003 | Train=0.0126 | Val=0.0151 | MAE=0.1576\n",
      "Epoch 004 | Train=0.0138 | Val=0.0161 | MAE=0.1636\n",
      "Epoch 005 | Train=0.0136 | Val=0.0135 | MAE=0.1497\n",
      "Epoch 006 | Train=0.0117 | Val=0.0100 | MAE=0.1277\n",
      "Epoch 007 | Train=0.0098 | Val=0.0073 | MAE=0.1064\n",
      "Epoch 008 | Train=0.0086 | Val=0.0057 | MAE=0.0903\n",
      "Epoch 009 | Train=0.0083 | Val=0.0049 | MAE=0.0793\n",
      "Epoch 010 | Train=0.0082 | Val=0.0045 | MAE=0.0745\n",
      "Epoch 011 | Train=0.0079 | Val=0.0044 | MAE=0.0747\n",
      "Epoch 012 | Train=0.0072 | Val=0.0045 | MAE=0.0790\n",
      "Epoch 013 | Train=0.0063 | Val=0.0050 | MAE=0.0857\n",
      "Epoch 014 | Train=0.0055 | Val=0.0058 | MAE=0.0930\n",
      "Epoch 015 | Train=0.0051 | Val=0.0067 | MAE=0.0980\n",
      "Epoch 016 | Train=0.0049 | Val=0.0071 | MAE=0.0986\n",
      "Epoch 017 | Train=0.0048 | Val=0.0068 | MAE=0.0936\n",
      "Epoch 018 | Train=0.0044 | Val=0.0058 | MAE=0.0843\n",
      "Epoch 019 | Train=0.0038 | Val=0.0046 | MAE=0.0735\n",
      "Epoch 020 | Train=0.0033 | Val=0.0037 | MAE=0.0640\n",
      "Epoch 021 | Train=0.0030 | Val=0.0032 | MAE=0.0603\n",
      "Epoch 022 | Train=0.0029 | Val=0.0031 | MAE=0.0589\n",
      "Epoch 023 | Train=0.0026 | Val=0.0034 | MAE=0.0584\n",
      "Epoch 024 | Train=0.0023 | Val=0.0038 | MAE=0.0595\n",
      "Epoch 025 | Train=0.0019 | Val=0.0046 | MAE=0.0645\n",
      "Epoch 026 | Train=0.0018 | Val=0.0052 | MAE=0.0704\n",
      "Epoch 027 | Train=0.0017 | Val=0.0054 | MAE=0.0716\n",
      "Epoch 028 | Train=0.0016 | Val=0.0050 | MAE=0.0680\n",
      "Epoch 029 | Train=0.0014 | Val=0.0044 | MAE=0.0632\n",
      "Epoch 030 | Train=0.0012 | Val=0.0040 | MAE=0.0613\n",
      "Epoch 031 | Train=0.0011 | Val=0.0037 | MAE=0.0606\n",
      "Epoch 032 | Train=0.0011 | Val=0.0037 | MAE=0.0609\n",
      "Epoch 033 | Train=0.0009 | Val=0.0040 | MAE=0.0621\n",
      "Epoch 034 | Train=0.0008 | Val=0.0043 | MAE=0.0649\n",
      "Epoch 035 | Train=0.0007 | Val=0.0046 | MAE=0.0667\n",
      "Epoch 036 | Train=0.0007 | Val=0.0045 | MAE=0.0660\n",
      "Epoch 037 | Train=0.0006 | Val=0.0041 | MAE=0.0630\n",
      "Epoch 038 | Train=0.0005 | Val=0.0036 | MAE=0.0591\n",
      "Epoch 039 | Train=0.0005 | Val=0.0032 | MAE=0.0564\n",
      "Epoch 040 | Train=0.0004 | Val=0.0031 | MAE=0.0555\n",
      "\n",
      "===== FINAL TEST =====\n",
      "Test Loss = 0.0114\n",
      "Test MAE  = 0.1010\n"
     ]
    }
   ],
   "source": [
    "# from gnn_models import GraphSAGENet\n",
    "# from train_gnn_regression import train_graphsage_regression\n",
    "\n",
    "# graph = Data(...) ‚Äî —Ç–æ, —á—Ç–æ –º—ã —Å–æ–±—Ä–∞–ª–∏ —Ä–∞–Ω–µ–µ\n",
    "\n",
    "model = GraphSAGENet(\n",
    "    in_channels=graph.x.size(1),\n",
    "    hidden_channels=128,\n",
    "    # out_channels=1,\n",
    "    num_layers=2,\n",
    "    edge_dim=graph.edge_attr.size(1)\n",
    ")\n",
    "\n",
    "# trained_model = train_graphsage_regression(\n",
    "#     graph=graph,\n",
    "#     model=model,\n",
    "#     epochs=40,\n",
    "#     batch_size=64,\n",
    "#     lr=0.001,\n",
    "#     device='cpu'  # –µ—Å–ª–∏ GPU –Ω–µ—Ç ‚Üí 'cpu'\n",
    "# )\n",
    "\n",
    "trained_model = train_graphsage_regression_fullbatch(\n",
    "    graph=graph,\n",
    "    model=model,\n",
    "    epochs=40,\n",
    "    lr=0.001,\n",
    "    device=\"cpu\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
